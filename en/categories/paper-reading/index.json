[{"content":"Author \u0026amp; Institution\nABHIMITRA MEKA*, Max Planck Institute for Informatics, Saarland Informatics Campus and Google\nMOHAMMAD SHAFIEI*, Max Planck Institute for Informatics, Saarland Informatics Campus\nMICHAEL ZOLLH√ñFER, Stanford University\nCHRISTIAN RICHARDT, University of Bath\nCHRISTIAN THEOBALT, Max Planck Institute for Informatics, Saarland Informatics Campus, Germany\nLink: http://gvv.mpi-inf.mpg.de/projects/LiveIlluminationDecomposition/\nAbstract  Propose the first approach for the decomposition of a monocular color video into direct and indirect illumination components in real time  In separate layers, the contribution made to the scene appearance by the scene reflectance, the light sources and the reflections from various coherent scene regions to one another Works for regular videos and produces temporally coherent decomposition layers at real-time frame rates Core: several sparsity priors that enable the estimation of the per-pixel direct and indirect illumination layers based on a small set of jointly estimated base reflectance colors The resulting variational decomposition problem uses a new formulation based on sparse and dense sets of non-linear equations that we solve efficiently using a novel alternating data-parallel optimization strategy   Existing techniques that invert global light transport require image capture under multiplexed controlled lighting, or only enable the decomposition of a single image at slow off-line frame rates  1. Introduction  Contributions  Joint illumination decomposition of direct and indirect illumination layers, and estimation and refinement of base colors that constitute the scene reflectance A sparsity-based automatic estimation of the underlying reflectance when a user identifies regions of strong interreflections A novel parallelized sparse‚Äìdense optimizer to solve a mixture of high-dimensional sparse problems jointly with lowdimensional dense problems at real-time frame rates    2. Related Work  Inverse Rendering  The colors in an image depend on scene geometry, material appearance and illumination Reconstructing these components from a single image or video is a challenging and illposed problem called inverse rendering Many complex image editing tasks can be achieved using a purely image-based decomposition without full inverse rendering   Global Illumination Decomposition  To decompose the captured radiance of a scene into direct and indirect components, some methods actively illuminate the scene to investigate the effect of light transport   Intrinsic Images  Many approaches have been introduced for the task of intrinsic image decomposition that explains a photograph using physically interpretable images such as reflectance and shading   Intrinsic Video Layer-based Image Editing  A physically accurate decomposition is not required to achieve complex image editing tasks such as recoloring of objects Instead, a decomposition into multiple semitransparent layers is often sufficient    3. Overview  The first real-time method for temporally coherent illumination decomposition of a video into a reflectance layer, direct illumination layer and multiple indirect illumination layers  Propose a novel sparsity-driven formulation for the estimation and refinement of a base color palette, which is used for decomposing the video frames Algorithm starts by automatically estimating a set of base colors that represent scene reflectances Automatic and only occasionally requires a minimal set of user clicks on the first video frame to identify regions of strong inter-reflections and refine the base colors  Propagate the user input automatically to the rest of the video by a spatiotemporal region-growing method   Perform the illumination decomposition Formulation results in a mixture of dense and sparse non-convex high-dimensional optimization problems, which they solve efficiently using a custom-tailored parallel iterative non-linear solver that they implement on the GPU   Evaluate on a variety of synthetic and real-world scenes, and provide comparisons that show that our method outperforms state-of-the-art illumination decomposition, intrinsic decomposition and layer-based image editing techniques, both qualitatively and quantitatively Demonstrate that real-time illumination decomposition of videos enables a range of advanced, illumination-aware video editing applications that are suitable for photo-real augmented reality applications, such as inter-reflection-aware recoloring and retexturing  4. Problem Formulation Algorithm: video frame -\u0026gt; a reflectance layer + a direct illumination layer + multiple indirect illumination layers\n  Simplifying assumption\n Assume that the scene is Lambertian, i.e., surfaces exhibit no view-dependent effects and hence their reflectance can be parameterized as a diffuse albedo with RGB components Assume that all light sources in the scene produce only white colored light. Hence, the direct illumination in the scene can be expressed by a grayscale or single channel image Assume that the second reflection bounce (or the first inter-reflection) of light is the primary source of indirect illumination in the scene, while the contribution of subsequent bounces of light is negligible Assume that the motion of the camera in the video is smooth with significant overlap between adjacent frames Assume that no new objects or materials come into view after the first frame    The algorithm factors each video frame $\\pmb I$ into a per-pixel product of the reflectance $\\pmb R$ and the illumination $\\pmb S$\n$$\n\\pmb I(\\pmb x)=\\pmb R(\\pmb x)\\odot \\pmb S(\\pmb x)\n$$\n $\\pmb x$: pixel location $\\odot$: element-wise product For diffuse objects, the reflectance layer captures the surface albedo, and the illumination layer $\\pmb S$‚Äã‚Äã jointly captures the direct and indirect illumination effects Represent the illumination layer as a colored RGB image to allow indirect illumination effects to be expressed in the illumination layer    Further decompose the illumination layer into a grayscale direct illumination layer resulting from the white illuminant, and multiple indirect colored illumination layers resulting from inter-reflections from colored objects in the scene\n  We start by estimating a set of base colors that consists of $K$ unique reflectance colors ${\\pmb b_k}$‚Äã that represent the scene\n $K$ is specified by the user, as superfluous clusters will be removed automatically This set of base colors serves as the basis for the illumination decomposition The base colors help constrain the values of pixels in the reflectance layer $\\pmb R$    For every surface point in the scene, assume that a single indirect bounce of light may occur from every base reflectance color, in addition to the direct illumination\n  The global illumination in the scene is modeled using a linear decomposition of the illumination layer $\\pmb S$‚Äã‚Äã into a direct illumination layer $T_0$‚Äã‚Äã and the sum of the $K$‚Äã‚Äã‚Äã indirect illumination layers $\\{T_k\\}_{0\u0026lt;k\\leq K}$‚Äã‚Äã\n$$\n\\pmb I(\\pmb x)=\\pmb R(\\pmb x)\\odot \\sum_{k=0}^K\\pmb b_k T_k(\\pmb x)\n$$\n $\\pmb b_0$: represents the color of the illuminant: white in paper\u0026rsquo;s case, i.e. $\\pmb b_0=(1,1,1)$ $T_0(\\pmb x)$: indicates the light transport contribution from the direct illumination The contribution from each base color $\\pmb b_k$ at a given pixel location $\\pmb x$ is measured by the map $T_k(\\pmb x)$‚Äã  Provides the net contribution by the base reflectance color to the global scene illumination   Obtain the set of base colors automatically using a real-time clustering technique  Once the base colors are obtained, the scene clustering can be further refined using a few simple user-clicks This refines only the regions of clustering but not the base colors themselves        5. Base Color Estimation  Initialize the set of base colors by clustering the dominant colors in the first video frame  This clustering step not only provides an initial base color estimate, but also a segmentation of the video into regions of approximately uniform reflectance If needed, the clustering in a video frame undergoes a user-guided correction   The base colors are used for the illumination decomposition  Further refined Used to compute the direct and indirect illumination layers    5.1. Chromaticity Clustering   Cluster the first video frame by color to approximate the regions of uniform reflectance that are observed in scenes with sparsely colored objects\n Based on a much faster histogram-based k-means clustering approach Perform the clustering of each RGB video frame in a discretized chromaticity space    Workflow\n Obtain chromaticity image by dividing the input image by its intensity $\\pmb C(\\pmb x)=\\pmb I(\\pmb x)/|\\pmb I(\\pmb x)|$ Compute a histogram of the chromaticity image with 10 partitions along each axis Perform weighted k-means clustering to obtain cluster center chromaticity values, using the population of the bins as the weight and the mid-point of the bin as sample values  The user provides an upper limit of the number of clusters visible in the scene $K$   Collapse adjacent similar clusters by measuring the pairwise chromaticity distance between estimated cluster centers  If this distance is below a threshold of 0.2, merge the smaller cluster into the larger cluster The average RGB colors of all pixels assigned to each cluster then yield the set of initial base colors      The histogram-based clustering approach significantly reduces the segmentation complexity, independent of the image size\n Produces a segmentation of the input frame, by assigning each pixel to its closest cluster Provides a coarse approximation of the reflectance layer $\\pmb R_{\\mathrm{cluster}}$  Use $\\pmb R_{\\mathrm{cluster}}$ as an initialization for the reflectance layer $\\pmb R$ in the energy optimization      5.2. Misclustering Correction  Since the clustering directly depends on the color of a pixel, regions of strong inter-reflections may be erroneously assigned to the base color of an indirect illuminant instead of the base color representing the reflectance of the region  Such a misclustering is difficult to correct automatically because of the inherent ambiguity of the illumination decomposition problem Rely on minimal manual interaction to identify misclustered regions and then automatically correct the underlying reflectance base color in all subsequent frames    5.2.1. Region Identification and Tracking   Identifying the true reflectance of a pixel in the presence of strong inter-reflections from other objects is an ambiguous task\n In case of direct illumination, the observed color value of a pixel is obtained by modulating the reflectance solely by the color of the illuminant In the case of inter-reflections, there is further modulation by light reflected from other objects, which then depends on their reflectance properties  Such regions are easy to identify by a user Ask the user to simply click on such a region only in the first frame it occurs   Automatically identify the full region by flood filling it using connected-components analysis based on the cluster identifier    Real-time tracking of non-rigidly deforming, non-convex marked regions in subsequent frames\n Given the marked pixel region in the previous frame Probe the same pixel locations in the current frame to identify pixels with the same cluster ID as in the previous frame Flood fill starting from these valid pixels to obtain the tracked marked region in the new frame  Do not flood fill for pixels inside the regions to keep this operation efficient Observe that one or two valid pixels are sufficient to correctly identify the entire misclustered region      5.2.2. Reflectance Correction  Once all pixels in a misclustered region are identified in a video frame (either marked or tracked), exploit the sparsity constraint of the indirect illumination layers to solve for the correct reflectance base color  Perform multiple full illumination decompositions for each identified region, evaluating each base color‚Äôs suitability as the region‚Äôs reflectance For each base color, measure the sparsity obtained over the region using the illumination sparsity term The base color that provides the sparsest solution of the decomposition is then used as the corrected reflectance   The intuition behind such a sparsity prior is that using the correct underlying reflectance should lead to an illumination layer which is explained by the color spill from only a sparse number of nearby objects  6. Illumination Decomposition   Decomposition each input video frame $\\pmb I$ into its reflectance layer $\\pmb R$, its direct illumination layer $T_0$ and a set of indirect illumination layers ${T_k}$ corresponding to the base colors ${\\pmb b_k}$\n The direct illumination layer $T_0$‚Äã‚Äã represents the direct contribution to the scene by the external light sources The indirect illumination layers $\\{T_k\\}$‚Äã‚Äã‚Äã‚Äã capture the inter-reflections that occur within the scene    Formulate the illumination decomposition as an energy minimization problem with the following energy:\n$$\nE_{\\mathrm{decomp}}(\\pmb \\chi)=E_{\\mathrm{data}}(\\pmb \\chi)+E_{\\mathrm{reflectance}}(\\pmb \\chi)+E_{\\mathrm{illumination}}(\\pmb \\chi)\n$$\n $\\pmb \\chi=\\{\\pmb R,T_0,\\{T_k\\}\\}$‚Äã is the set of variables to be optimized The base colors ${\\pmb b_k}$ stay fixed Optimize this energy using a novel fast GPU solver to obtain real-time performance    Data Fidelity Term\nThis constraint enforces that the decomposition result reproduces the input image:\n$$\nE_{\\mathrm{data}}(\\pmb \\chi)=\\lambda_{\\mathrm{data}}\\cdot \\sum_{\\pmb x}\\left\\Vert\\pmb I(\\pmb x)-\\pmb R(\\pmb x)\\odot \\sum_{k=0}^K\\pmb b_kT_k(\\pmb x) \\right\\Vert^2\n$$\n $\\lambda_{\\mathrm {data}}$: the weight for this energy term $T_k$: the $(K+1)$‚Äã‚Äã‚Äã illumination layers of the decomposition(one direct layer $T_0$‚Äã and $K$‚Äã indirect layers $\\{T_k\\}$‚Äã‚Äã)  6.1. Reflectance Priors  Constrain the estimated reflectance layer $\\pmb R$ using three priors:\n$$\nE_{\\mathrm{reflectance}}(\\pmb \\chi)=E_{\\mathrm{clustering}}(\\pmb \\chi)+E_{\\mathrm{r-sparsity}}(\\pmb \\chi)+E_{\\mathrm{r-consistency}}(\\pmb \\chi)\n$$  Reflectance Clustering Prior\n  Use the cluster to guide the decomposition, as the chromaticity-clustered image $\\pmb R_{\\mathrm{cluster}}$ is an approximation of the reflectance layer $\\pmb R$‚Äã\n  Constrain the reflectance map to remain close to the clustered image using the following energy term:\n$$\nE_{\\mathrm{clustering}}(\\pmb \\chi)=\\lambda_{\\mathrm{clustering}}\\cdot \\sum_{\\pmb x} \\parallel\\pmb r(\\pmb x)-\\pmb r_{\\mathrm{cluster}}(\\pmb x) \\parallel_2^2\n$$\n $\\pmb r$: represent the quantity $\\pmb R$‚Äã in the log-domain, i.e. $\\pmb r=\\ln\\pmb R$ $\\pmb r_{\\mathrm{cluster}}$: the clustered reflectance map    Reflectance Sparsity Prior\n Encourages a piecewise constant reflectance map using gradient sparsity Natural scenes generally consist of a small set of objects and materials, hence the reflectance layer is expected to have sparse gradients Such a spatially sparse solution for the reflectance image can be obtained by minimizing the $\\mathcal ‚Ñì_p-\\mathrm{norm}$ ($p\\in[0,1]$) of the gradient magnitude $\\parallel\\nabla \\pmb r\\parallel_2$  $$\nE_{\\mathrm{r-sparsity}}=\\lambda_{\\mathrm{r-sparsity}}\\cdot\\sum_{\\pmb x}\\parallel\\nabla \\pmb r(\\pmb x)\\parallel_2^p\n$$\nSpatiotemporal Reflectance Consistency Prior\n Enforces that the reflectance stays temporally consistent by connecting every pixel with a set of randomly sampled pixels in a small spatiotemporal window by constraining the reflectance of the pixels to be close under a defined chromaticity-closeness condition For each pixel $\\pmb x$‚Äã‚Äã in the reflectance image, connect it to $N_s$ randomly sampled pixels $\\pmb y_i$ Samples are chosen from reflectance images of the current and previous frames $t_i$  $$\n\\begin{align}\nE_{\\mathrm{r-consistency}}(\\pmb \\chi)\u0026amp;=\\lambda_{\\mathrm{r-consistency}}\\cdot \\sum_{i=1}^{N_s}g_i(\\pmb x)\\cdot \\parallel\\pmb r(\\pmb x)-\\pmb r_{t_i}(\\pmb y_i)\\parallel_2^2\\\\\ng_i(\\pmb x)\u0026amp;=\\begin{cases}\nw_{iw}(\\pmb x)\u0026amp;\\mathrm{if}\\ \\parallel\\pmb c(\\pmb x)-\\pmb c_{t_i}(\\pmb y_i)\\parallel_2\u0026lt;\\tau_{cc}\\\\\n0\u0026amp;\\mathrm{otherwise}\n\\end{cases}\n\\end{align}\n$$\n $\\tau_{cc}$: a chromaticity consistency threshold  6.2. Illumination Priors  Constrain the illumination $\\pmb S$ to be close to monochrome and the indirect illumination layers ${T_k}$ to have a sparse decomposition, spatial smoothness and non-negativity  $$\nE_{\\mathrm{illumination}}(\\pmb \\chi)=E_{\\mathrm{monochrome}}(\\pmb \\chi)+E_{\\mathrm{i-sparsity}}(\\pmb \\chi)+E_{\\mathrm{smoothness}}(\\pmb \\chi)+E_{\\mathrm{non-neg}}(\\pmb \\chi)\n$$\nSoft-Retinex Weighted Monochromaticity Prior\n The illumination layer is a combination of direct and indirect illumination effects Indirect effects such as inter-reflections tend to be spatially local with smooth color gradients whereas under the white-illumination assumption, the direct bounce does not contribute any color to the illumination layer Expect the illumination $\\pmb S$ to be mostly monochromatic except at small spatial pockets where smooth color gradients occur due to inter-reflections  $$\nE_{\\mathrm{monochrome}}(\\pmb \\chi)=\\lambda_{\\mathrm{monochrome}}\\cdot w_{\\mathrm{SR}}\\cdot \\sum_{\\pmb x}\\sum_{\\pmb c}(\\pmb S_c(\\pmb x)-|\\pmb S(\\pmb x)|)^2\n$$\n  $c\\in{R,G,B}$\n  $|\\pmb S|$: the intensity of the illumination layer $\\pmb S$‚Äã\n  This constraint pulls the color channels of each pixel close to the grayscale intensity of the pixel, hence encouraging monochromaticity\n  $w_{\\mathrm{SR}}$: the soft-color-Retinex weight\n$$\nw_{\\mathrm{SR}}=1-\\exp(-50\\cdot\\Delta\\pmb C)\n$$\n $\\Delta\\pmb C$: ‚Äãthe maximum of the chromaticity gradient of the input image in any of the four spatial directions at the pixel location    The soft-color-Retinex weight is high only for large chromaticity gradients, which represent reflectance edges\n  Hence, monochromaticity of the illumination layer is enforced only close to the reflectance edges and not at locations of slowly varying chromaticity, which represent inter-reflections\n  Relying on local chromaticity gradients may be problematic when there are regions of uniform colored reflectance, but in such regions the reflectance sparsity priors tend to be stronger and overrule the monochromaticity prior.\n  Illumination Decomposition Sparsity\n  Enforce that the illumination decomposition is sparse in terms of the layers that are activated per-pixel, i.e., those that influence the pixel with their corresponding base color\n  Assume during image formation in the real world, a large part of the observed radiance for a scene point comes from a small subpart of the scene\n  Apply the sparsity-inducing $\\mathcal ‚Ñì_1-\\mathrm{norm}$ to the indirect illumination layers\n$$\nE_{\\mathrm{i-sparsity}}(\\pmb \\chi)=\\lambda_{\\mathrm{i-sparsity}}\\cdot\\sum_{\\pmb x}\\left\\Vert\\{T_k(\\pmb x)\\}^K_{k=1} \\right\\Vert_1\n$$\n  Spatial Smoothness\n  Encourage the decomposition to be spatially piecewise smooth using an $‚Ñì1-\\mathrm{sparsity}$‚Äã prior in the gradient domain\n  Enforces piecewise constancy of each direct or indirect illumination layer\n$$\nE_{\\mathrm{smoothness}}(\\pmb \\chi)=\\lambda_{\\mathrm{smoothness}}\\cdot \\sum_{\\pmb x}\\sum_{k=0}^K\\parallel\\nabla T_k(\\pmb x)\\parallel_1\n$$\n  This allows to have sharp edges in the decomposition layers\n  Non-Negativity of Light Transport\n  Light transport is an inherently additive process\n Light bouncing around in the scene adds radiance to scene points But never subtracts from them    The quantity of transported light is always positive\n$$\nE_{\\mathrm{non-neg}}(\\pmb \\chi)=\\lambda_{\\mathrm{non-neg}}\\cdot \\sum_{\\pmb x}\\sum_{k=0}^K\\max(-T_k(\\pmb x),0)\n$$\n If the decomposition layer $T_k(\\pmb x)$‚Äã is non-negative, there is no penalty If $T_k(\\pmb x)$‚Äã becomes negative, a linear penalty is enforced    6.3. Base Color Refinement   Refine the base colors further on the first video frame to approach the ground-truth reflectance of the materials in the scene\n  The refinement of base colors is formulated as an incremental update $\\Delta \\pmb b_k$ of the base colors $\\pmb b_k$‚Äã in the original data fidelity term, along with intensity and chromaticity regularizers\n$$\n\\begin{align}\nE_{\\mathrm{refine}}(\\pmb \\chi)\u0026amp;=\\lambda_{\\mathrm{data}}\\sum_{\\pmb x}\\left\\Vert \\pmb I(\\pmb x)-\\pmb R(\\pmb x)\\odot \\sum_{k=0}^K(\\pmb b_k+\\Delta\\pmb b_k)T_k(\\pmb x) \\right\\Vert^2\\\\\n\u0026amp;+\\lambda_{\\mathrm{IR}}\\sum_{k=1}^K\\parallel\\Delta\\pmb b_k\\parallel_2^2+\\lambda_{\\mathrm{CR}}\\sum_{k=1}^K\\parallel(\\pmb C(\\pmb b_k)+\\Delta\\pmb b_k)-\\pmb C(\\pmb b_k)\\parallel_2^2\n\\end{align}\n$$\n $\\pmb \\chi={\\Delta\\pmb b_k}$: the vector of unknowns to be optimized $\\lambda_{\\mathrm{IR}}$: the weight for the intensity regularizer that ensures small base color updates $\\lambda_{\\mathrm{CR}}$: the weight of the chromaticity regularizer  Constrains base color updates $\\Delta\\pmb b_k$‚Äã to remain close in chromaticity $\\pmb C(\\cdot)$‚Äã to the initially estimated base color $\\pmb b_k$      These regularizers ensure that base color updates do not lead to oscillations in the optimization process\n  The refinement energy is solved in combination with the illumination decomposition energy\n Resulting in an estimation of the unknown variables that together promotes decomposition sparsity    This refinement of the base colors leads to a dense Jacobian matrix, because the unknown variables ${\\Delta \\pmb b_ùëò}$‚Äã in the energy are influenced by all pixels in the image\n  6.4. Handling the Sparsity-Inducing Norms   Some energy terms contain sparsity-inducing $‚Ñìùëù-\\mathrm{norms}$ ($p \\in [0, 1]$‚Äã)\n  Handle these objectives in a unified manner using Iteratively Re-weighted Least Squares\n  Approximate the $‚Ñìùëù-\\mathrm{norms}$‚Äã by a nonlinear least-squares objective based on re-weighting, i.e., replace the corresponding residuals $\\pmb r$‚Äã as follows:\n$$\n\\begin{align}\n\\parallel\\pmb r\\parallel_p\u0026amp;=\\parallel\\pmb r\\parallel_2^2\\cdot \\parallel\\pmb r\\parallel_2^{p-2}\\\\\n\u0026amp;\\approx \\parallel\\pmb r\\parallel_2^2\\cdot \\parallel\\pmb r_{\\mathrm {old}}\\parallel_2^{p-2}\n\\end{align}\n$$\nin each step of the applied iterative solver\n $\\pmb r_{\\mathrm{old}}$: the corresponding residual after the previous iteration step    6.4.1. Handling Non-negativity Constraints   The non-negativity objective $E_{\\mathrm{non-neg}}(\\pmb \\chi)$‚Äã‚Äã contains a maximum function that is non-differentiable at zero\n  Handle this objective by replacing the maximum with a re-weighted least-squares term, $\\max(-T_k(\\pmb x),0)=w_kT_k^2(\\pmb x)$, using\n$$\nw_k=\\begin{cases}\n0\u0026amp;\\mathrm{if}\\ T_k(\\pmb x)\u0026gt;0\\\\\n(|T_k(\\pmb x)+\\epsilon|)^{-1}\u0026amp;\\mathrm{otherwise}\n\\end{cases}\n$$\n $\\epsilon = 0.002$: a small constant that prevents division by zero    Transforms the non-convex energy into a non-linear least-squares optimization problem\n  7. Data-Parallel GPU Optimization   The decomposition problems are all non-convex optimizations based on an objective $E$ with unknowns $\\pmb \\chi$\n  The best decomposition $\\pmb \\chi^\\ast$‚Äã by solving the following minimization problem:\n$$\n\\pmb \\chi^\\ast=\\arg\\min_\\pmb \\chi E(\\pmb \\chi)\n$$\n  The optimization problems are in general non-linear least-squares form and can be tackled by the iterative Gauss‚ÄìNewton algorithm that approximates the optimum $\\pmb \\chi^\\ast\\approx \\pmb \\chi_k$ by a sequence of solutions $\\pmb \\chi_k=\\pmb \\chi_{k-1}+\\pmb \\delta_k^\\ast$\n  The optimal linear update $\\pmb \\delta_k^\\ast$‚Äã‚Äã is given by the solution of the associated normal equations:\n$$\n\\pmb\\delta_k^\\ast=\\arg\\min_{\\pmb\\delta_k}\\parallel\\pmb F(\\pmb \\chi_{k-1})+\\pmb\\delta_k\\pmb J(\\pmb \\chi_{k-1})\\parallel_2^2\n$$\n $\\pmb F$: a vector field that stacks all residuals, i.e., $E(\\pmb\\chi)=\\parallel\\pmb F(\\pmb\\chi)\\parallel_2^2$ $\\pmb J$: its Jacobian matrix      Obtaining real-time performance is challenging even with recent state-of-the-art data-parallel iterative non-linear least-squares solution strategies\n To avoid cluttered notation, we will omit the parameters and simply write $\\pmb J$ instead of $\\pmb J(\\pmb\\chi)$‚Äã For our decomposition energies, the Jacobian $\\pmb J$‚Äã is a large matrix with usually more than 70 million rows and 4 million columns  Previous approaches assume $\\pmb J$‚Äã‚Äã to be a sparse matrix, meaning that only a few residuals are influenced by each variable While this holds for the columns of $\\pmb J$‚Äã‚Äã‚Äã‚Äã that corresponds to the variables that are associated with the decomposition layers, it does not hold for the columns that store the derivatives with respect to the base color updates ${\\Delta \\pmb b_k}$‚Äã‚Äã, since the base colors influence each residual of $E_{\\mathrm{data}}$‚Äã   $\\pmb J=\\begin{bmatrix}\\pmb S_{\\pmb J}\u0026amp;\\pmb D_{\\pmb J}\\end{bmatrix}$ has two sub-blocks:  $\\pmb S_{\\pmb J}$: a large sparse matrix with only a few non-zero entries per row $\\pmb D_{\\pmb J}$: a dense matrix with the same number of rows, but only a few columns   The evaluation of the Jacobian $\\pmb J$ requires a different specialized parallelization for the dense and sparse parts    7.1. Sparse-Dense Splitting   Tackle the described problem using a sparse‚Äìdense splitting approach that splits the variables $\\chi$‚Äã into a sparse set $\\mathcal T$‚Äã (decomposition layers) and a dense set $\\mathcal B$‚Äã (base color updates)\n Afterwards, we optimize for $\\mathcal B$ and $\\mathcal T$‚Äã independently in an iterative flip-flop manner    Algorithm:\n  Optimize for $\\mathcal T$, while keeping $\\mathcal B$‚Äã fixed\n The resulting optimization problem is a sparse non-linear least-squares problem Improve upon the previous solution by performing a nonlinear Gauss‚ÄìNewton step The corresponding normal equations are solved using 16 steps of data-parallel preconditioned conjugate gradient Parallelize over the rows of the system matrix using one thread per row (variable)    After updating the ‚Äòsparse‚Äô variables $\\mathcal T$‚Äã, keep them fixed and solve for the ‚Äòdense‚Äô variables $\\mathcal B$‚Äã\n The resulting optimization problem is a dense least-squares problem with a small $3ùêæ \\times 3ùêæ$‚Äã system matrix (normally $ùêæ$‚Äã‚Äã is between 4 and 7 due to merged clusters) Materialize the normal equations in device memory based on a sequence of outer products, using one thread per entry of $\\pmb J^T\\pmb J$    The system is mapped to the CPU and robustly solved using singular value decomposition. After updating ‚Äòdense‚Äô variables $\\mathcal B$,\nwe again solve for ‚Äòsparse‚Äô variables $\\mathcal T$‚Äã and iterate this process until convergence\n    8. Results and Evaluation Parameters\n $\\lambda_{\\mathrm{clustering}}=200$ $\\lambda_{\\mathrm{r-sparsity}}=20$ $p=1$ $\\lambda_{\\mathrm{i-sparsity}}=3$ $\\lambda_{\\mathrm{smoothness}}=3$ $\\lambda_{\\mathrm{non-neg}}=1000$ $\\lambda_{\\mathrm{data}}=5000$ $\\lambda_{\\mathrm{IR}}=10$ $\\lambda_{\\mathrm{CR}}=10$ $\\lambda_{\\mathrm{r-consistency}}=\\lambda_{\\mathrm{monochrome}}=10$  Runtime performance\n Platform: Intel Core i7 with 2.7 GHz, 32 GB RAM and an NVIDIA GeForce GTX 980 Resolution: 640x512 pixels Performance:  Illumination decomposition: 14ms Base color refinement: 2s Misclustering correction: 1s   Perform the last two steps, base color refinement and misclustering correction, only once at the beginning of the video  Runs at real-time frame rates (‚©æ30 Hz) Enables real-time video editing applications    Demo\n\r9. Limitations  Breaking the assumptions can lead to inaccurate estimations The method can face challenges if objects enter or leave the scene during the course of the video The inter-reflections caused by out-of-view objects cannot be properly modeled, since the corresponding base color might not be available If an object with an unseen color enters the scene for the first time, and the base colors are already exceeded, its inter-reflections cannot be modeled Complex, textured scenes with many different colors are challenging to decompose, since this requires many base colors, leading to a large number of variables and an even more under-constrained optimization problem More general indoor and outdoor scenes are not the ideal use cases for the method  ","description":"Siggraph 2021 Paper Reading","id":0,"section":"posts","tags":["Rendering"],"title":"Real-Time Global Illumination Decomposition of Videos","uri":"https://chaphlagical.github.io/en/posts/paperreading/real_time_global_illumination_decomposition_of_videos/"},{"content":"Author \u0026amp; Institution\nXIUMING ZHANG, Massachusetts Institute of Technology\nSEAN FANELLO and YUN-TA TSAI, Google\nTIANCHENG SUN, University of California, San Diego\nTIANFAN XUE, ROHIT PANDEY, SERGIO ORTS-ESCOLANO, PHILIP DAVIDSON, CHRISTOPH RHEMANN, PAUL DEBEVEC, and JONATHAN T. BARRON, Google\nRAVI RAMAMOORTHI, University of California, San Diego\nWILLIAM T. FREEMAN, Massachusetts Institute of Technology \u0026amp; Google\nLink: http://nlt.csail.mit.edu/\nAbstract   Light transport (LT)\n The light transport (LT) of a scene describes how it appears under different lighting conditions from different viewing directions Complete knowledge of a scene‚Äôs LT enables the synthesis of novel views under arbitrary lighting    In this paper\n Focus on image-based LT acquisition, primarily for human bodies within a light stage setup Propose a semi-parametric approach for learning a neural representation of the LT that is embedded in a texture atlas of known but possibly rough geometry Model all non-diffuse and global LT as residuals added to a physically-based diffuse base rendering  Show how to fuse previously seen observations of illuminants and views to synthesize a new image of the same scene under a desired lighting condition from a chosen viewpoint Allows the network to learn complex material effects (such as subsurface scattering) and global illumination (such as diffuse interreflection), while guaranteeing the physical correctness of the diffuse LT (such as hard shadows)   With this learned LT, one can relight the scene photorealistically with a directional light or an HDRI map, synthesize novel views with view-dependent effects, or do both simultaneously, all in a unified framework using a set of sparse observations    1. Introduction   Light Transport\n Models how light interacts with objects in the scene to produce an observed image    Inferring light transport\n Acquiring the LT of a scene from images of that scene requires untangling the myriad interconnected effects of occlusion, shading, shadowing, interreflections, scattering, etc    Application\n Phototourism Telepresence Storytelling Special effects Generating ground truth data for machine learning task  Many works rely on high-quality renderings of relit subjects under arbitrary lighting conditions and from multiple viewpoints  Relighting View synthesis Re-enacting Alpha-matting        Previous work has shown that it is possible to construct a light stage , plenoptic camera , or gantry that directly captures a subset of the LT function and thereby enables the image-based rendering thereof\n Widely used in film productions and within the research community Can only provide sparse sampling of the LT limited to the number of LEDs(~ 300 on a spherical dome) and the number of cameras (~50-100 around the subject), resulting in the inability to produce photorealistic renderings outside the supported camera/light locations Traditional image-based rendering approaches are usually designed for fixed viewpoints and are unable to synthesize unseen (novel) views under a desired illumination    In this paper\n Learn to interpolate the dense LT function of a given scene from sparse multi-view, One-Light-at-A-Time (OLAT) images acquired in a light stage, through a semi-parametric technique that called Neural Light Transport (NLT) Many prior works have addressed similar tasks with classic works tending to rely on physics to recover analytical and interpretable models Recent works using neural networks to infer a more direct mapping from input images to an output image    Rendering method\n Traditional rendering methods  Make simplifying assumptions when modeling geometry, BRDFs, or complex inter-object interactions Make the problem tractable   Deep learning approaches  Can tolerate geometric and reflectance imperfections Require many aspects of image formation (even those guaranteed by physics) be learned ‚Äúfrom scratch,‚Äù which may necessitate a prohibitively large training set   NLT  Straddle this divide between traditional methods and deep learning approaches  Construct a classical model of the subject being imaged (a mesh and a diffuse texture atlas per Lambertian reflectance) Embed a neural network within the parameterization provided by that classical model Construct the inputs and outputs of the model in ways that leverage domain knowledge of classical graphics techniques Train that network to model all aspects of LT, including those not captured by a classical model   Able to learn an accurate model of the complicated LT function for a subject from a small training dataset of sparse observations A key novelty: the learned model is embedded within the texture atlas space of an existing geometric model of the subject, which provides a novel framework for simultaneous relighting and view interpolation Express the 6D LT function at each location on the surface of the geometric model as simply the output of a deep neural network  Works well (as neural networks are smooth and universal function approximators) and obviates the need for a complicated parameterization of spatially-varying reflectance         Main contribution  An end-to-end, semi-parametric method for learning to interpolate the 6D light transport function per-subject from real data using convolutional neural networks A unified framework for simultaneous relighting and view synthesis by embedding networks into a parameterized texture atlas and leveraging as input a set of One-Light-at-A-Time (OLAT) images A set of augmented texture-space inputs and a residual learning scheme on top of a physically accurate diffuse base, which together allow the network to easily learn non-diffuse, higher-order light transport effects including specular highlights, subsurface scattering, and global illumination    2. Related Work  Problem  Recovering a model of light transport from a sparse set of images of some subject Predicting novel images of that subject from unseen views and/or under unobserved illuminations    Single observation\n The most sparse sampling is just a single image, from which one could attempt to infer a model (geometry, reflectance, and illumination) of the physical world that resulted in that image  Via hand-crafted or learned priors   Though practical, the quality gap between what can be accomplished by single-image techniques and what has been demonstrated by multi-image techniques is significant  Can\u0026rsquo;t show complex light transport effects such as specular highlights or subsurface scattering Limited to a single task, such as relighting, and some support only a limited range of viewpoint change    Multiple views\n Multiview geometry techniques recover a textured 3D model that can be rendered using conventional graphics or photogrammetry techniques  Have material and shading variation baked in Do not enable relighting   Image-based rendering techniques such as light fields or lumigraphs  Can be used to directly sample and render the plenoptic function The accuracy of these techniques is limited by the density of sampled input images Do not enable relighting   Reprojection-based methods  For unstructured inputs Assume the availability of a geometry proxy (so does our work), reproject nearby views to the query view, and perform image blending in that view Rely heavily on the quality of the geometry proxy  A class-specific geometry prior (such as that of a human body) can be used to increase the accuracy of a geometry proxy   Cannot synthesize pixels that are not visible in the input views Do not enable relighting   Deep learning  Been used to synthesize new images from sparse sets of input images Usually by training neural networks to synthesize some intermediate geometric representation that is then projected into the desired image Some techniques even entirely replace the rendering process with a learned ‚Äúneural‚Äù renderer Generally do not attempt to explicitly model light transport  Do not enable relighting Capable of preserving view-dependent effects for the fixed illumination condition under which the input images were acquired   Often breaks ‚Äúbackwards compatibility‚Äù with existing graphics systems    Multiple illuminants\n Repeatedly imaging a subject with a fixed camera but under different illuminations and then recovering the surface normals Most photometric stereo solutions assume Lambertian reflectance and do not support relighting with non-diffuse light transport Neural networks can be applied to relight a scene captured under multiple lighting conditions from a fixed viewpoint  Multiple views and illuminant\n  Utilize the symmetry of illuminations and view directions to collect sparse samples of an 8D reflectance field, and reconstruct a complete field using a low-rank assumption\n  Lack an explicit geometric model\n Rendering is limited to a fixed set of viewpoints    Supports relighting and view synthesis\n But assume pre-defined BRDFs Cannot synthesize more complex light transport effects present in real images    In this paper\n Follows the convention of the nascent field of ‚Äúneural rendering‚Äù, in which a separate neural network is trained for each subject to be rendered, and all images of that subject are treated as ‚Äútraining data.‚Äù  These approaches have shown great promise in terms of their rendering fidelity But require per-subject training and are unable to generalize across subjects yet   Paper\u0026rsquo;s approach  Unlike prior work that focuses on a specific task The texture-space formulation allows for simultaneous light and view interpolation The model is a valuable training data generator for many works that rely on high-quality renderings of subjects under arbitrary lighting conditions and from multiple viewpoints      3. Method   The framework is a semi-parametric model with a residual learning scheme that aims to close the gap between the diffuse rendering of the geometry proxy and the real input image\n The semi-parametric approach is used to fuse previously recorded observations to synthesize a novel, photorealistic image under any desired illumination and viewpoint    The method relies on recent advances in computer vision that have enabled accurate 3D reconstructions of human subjects\n Such as technique that takes as input several images of a subject and produces as output a mesh of that subject and a UV texture map describing its albedo At first glance, this appears to address the entirety of the problem: given a textured mesh, we can perform simultaneous view synthesis and relighting by simply re-rendering that mesh from some arbitrary camera location and under some arbitrary illumination Simplistic model of reflectance and illumination only permits equally simplistic relighting and view synthesis    Assuming Lambertian reflectance:\n$$\n\\tilde L_o(\\pmb x,\\pmb \\omega_o)=\\rho(\\pmb x)L_i(\\pmb x,\\pmb \\omega_i)(\\pmb \\omega_i\\cdot \\pmb n(\\pmb x))\n$$\n  $\\tilde L_0(\\pmb x,\\pmb\\omega_0)$ is the diffuse rendering of a point $\\pmb x$ with a surface normal $\\pmb n(\\pmb x)$ and albedo $\\rho(\\pmb x)$, lit by a directional light $\\pmb \\omega_i$ with an incoming intensity $L_i(\\pmb x,\\pmb \\omega_i)$ and view from $\\pmb \\omega_0$\n  This reflectance model is only sufficient for describing matte surfaces and direct illumination\n  More recent methods also make strong assumptions about materials by modeling reflectance with a cosine lobe model\n The shortcomings of these methods are obvious when compared to a more expressive rendering approach, such as the rendering equation, which makes far fewer simplifying assumptions:  $$\nL_o(\\pmb x,\\pmb \\omega_o)=L_e(\\pmb x,\\pmb \\omega_o)+\\int_\\Omega f_s(\\pmb x,\\pmb \\omega_i,\\pmb \\omega_o)L_i(\\pmb x,\\pmb \\omega_i)(\\pmb \\omega_i\\cdot \\pmb n(\\pmb x))\\mathrm d\\pmb \\omega_i\n$$\n  Limitations in computing $\\tilde L_o(\\pmb x,\\pmb \\omega_o)$\n It assumes a single directional light instead of integrating over the hemisphere of all incident directions $\\Omega$ It approximates an object‚Äôs BRDF $f_s(\\cdot)$‚Äã as a single scalar It ignores emitted radiance $L_e(\\cdot)$‚Äã (in addition to scattering and transmittance, which this rendering equation does not model either)    The goal of the learning-based model is to close the gap between $L_o(\\pmb x,\\pmb \\omega_o)$ and $\\tilde L_o(\\pmb x,\\pmb \\omega_o)$, and furthermore between $L_o(\\pmb x,\\pmb \\omega_o)$ and the observed image\n    Motivation\n the geometry and texture atlas offers us a mapping from each image of a subject onto a canonical texture atlas that is shared across all views of that subject    Approach\n Use geometry and texture atlas to map the input images of the subject from ‚Äúcamera space‚Äù (XY pixel coordinates) to ‚Äútexture space‚Äù (UV texture atlas coordinates) Use a semi-parametric neural network embedded in this texture space to fuse multiple observations and synthesize an RGB texture atlas for the desired relit and/or novel-view image Warped back into the camera space of the desired viewpoint, thereby giving us an output rendering of the subject under the desired illumination and viewpoint    The Demo can explain everything:\n\r4. Limitations  The method must be trained individually per scene, and generalizing to unseen scenes is an important future step for the field The fixed 1024√ó1024 resolution of the texture-space model limits the model‚Äôs ability to synthesize higher-frequency contents  Especially when the camera zooms very close to the subject, or when an image patch is allocated too few texels This could be solved by training on higherresolution images, but this would increase memory requirements and likely require significant engineering effort   Has occasional failure modes, where complex light transport effects, such as the ones on the glittery chain, are hard to synthesize, and the final renderings lack high-frequency details  ","description":"Siggraph 2021 Paper Reading","id":1,"section":"posts","tags":["Rendering"],"title":"Neural Light Transport for Relighting and View Synthesis","uri":"https://chaphlagical.github.io/en/posts/paperreading/neural_light_transport_for_relighting_and_view_synthesis/"},{"content":"Author: Carl Schissler, Gregor M√ºckl, Paul Calamia\nInstitution: Facebook Reality Labs Research, USA\nLink: https://dl.acm.org/doi/10.1145/3450626.3459751\nAbstract  Diffraction is one the the most perceptually important yet difficult to simulate acoustic effects  A phenomenon that allows sound to propagate around obstructions and corners   A significant bottleneck in real-time simulation of diffraction:  The enumeration of high-order diffraction propagation paths in scenes with complex geometry   The paper present a dynamic geometric diffraction approach that consists of an extensive mesh preprocessing pipeline and complementary runtime algorithm  Preprocessing module identifies a small subset of edges that are important for diffraction using a novel silhouette edge detection heuristic  It also extends these edges with planar diffraction geometry and precomputes a graph data structure encoding the visibility between the edges   The runtime module uses bidirectional path tracing against the diffraction geometry to probabilistically explore potential paths between sources and listeners, then evaluates the intensities for these paths using the Uniform Theory of Diffraction  It uses the edge visibility graph and the A* pathfinding algorithm to robustly and efficiently find additional high-order diffraction paths     The paper demonstrate how this technique can simulate 10th-order diffraction up to 568 times faster than the previous state of the art, and can efficiently handle large scenes with both high geometric complexity and high numbers of sources  1. Introduction   In order to generate a convincing simulation of reality, the senses must be provided with plausible recreations of the real world\n For virtual reality (VR) and augmented reality (AR) applications, high quality audio is especially important to create immersion and a sense of presence Audio sources must be rendered in a way that seems plausible to the user, i.e. where the percept matches the user‚Äôs expectation Involving simulating how the sounds emitted by sources interact with the virtual environment through reverberation, reflections, and diffraction, among other acoustic phenomena    One of the most difficult to simulate yet perceptually important acoustic effects in a geometric acoustics (GA) framework is diffraction\n Diffraction is a wave scattering phenomenon that occurs when sound interacts with a feature in the environment whose size is similar to the wavelength With proper simulation of diffraction, sources that become occluded from view are still audible but become low-pass filtered In a GA simulation that does not handle diffraction, occluded sources will be abruptly silenced, resulting in a jarring unnatural transition that has the potential to break the perceived plausibility of the auralization    The paper present a new efficient approach for simulating diffraction within a real-time GA framework that allows for dynamic motion of rigid geometry and for high-order diffraction\n Focus on a particular subset of the diffraction problem: the simulation of direct diffraction only  Direct diffraction is defined as diffraction that occurs directly between a source and listener with no reflections involved   The approach is able to simulate perceptually-important features  Particularly the smooth transition from unoccluded to occluded state Ignoring more complex paths which can be difficult to identify but contribute less to the overall sound field      Main contributions:\n A mesh preprocessing approach that extracts a reduced subset of silhouette diffraction edges and augments the mesh with diffraction flag geometry A runtime approach for efficiently finding high-order diffraction paths using stochastic bidirectional path tracing and a persistent cache of paths A complementary approach that uses a precomputed edge-to-edge visibility graph and the A* algorithm to quickly find high-order diffraction paths    2. Background 2.1. Sound Propagation   Based on solving the acoustic wave equation (wave-based methods)\n Methods:  Finite Difference Time Domain (DFTD) Finite Element Method (FEM) Boundary Element Method (BEM)   Pros  Accurate   Cons  Very computationally intensive Limited to precomputation and static scenes      Geometric acoustics algorithm\n Pros  Can simulate reflection, scattering, and reverberation efficiency   Cons  Do not handle wave phenomena like diffraction  Make the high-frequency assumption that sound travels as a ray, not a wave        2.2. Diffraction for Geometric Acoustic   Diffraction models that are applicable to GA generally consider the case of diffraction over one or more edges of the scene geometry\n Diffraction order: the number of edges in a path    Uniform Theory of Diffraction (UTD)\n Given a source, listener, and sequence of diffraction edges, the UTD can analytically compute an approximation for diffracted sound field Fast to evaluate, therefore UTD is attractive for real-time simulation Drawback: it assumes every edges to be infinitely long  May cause implausible results      Biot-Tolstoy-Medwin (BTM) diffraction method\n Doesn\u0026rsquo;t have UTD\u0026rsquo;s limitation Require significantly more compute to evaluate    Diffraction based on the uncertainty principle (UP)\n Use stochastic ray tracing in a Monte Carlo integrator to compute the diffracted sound field It can be easily integrated into existing path tracing algorithm It has slow convergence that makes it unsuitable for real-time application    Other object-based diffraction approaches\n Used a hybrid of precomputed wave simulation and ray tracing to simulate sound scattering around objects 2D rasterization-based method for approximating occlusion that compares the diffracted path length to the straight-line distance between the source and listener Uses a dense volumetric sampling of rays around existing direct and reflected propagation path segments to approximate the BTM magnitude response    Main computational challenge\n For either UTD or BTM Find sequences of edges that can form valid diffraction paths High-order diffraction involves considering the interaction of every edge with every other edge recursively  A naive approach is to recursively consider all pairs of edges, but this has complexity $O(N^d)$, where $N$ is the number of edges and $d$‚Äã is the maximum diffraction order      Optimization\n Frustum and beam tracing have been used to find diffraction paths more efficiently  Frusta or beams are emitted from the source and propagated through the environment to find paths Have difficulty scaling to complex geometries or to high diffraction order due to the large number of child beams and time spent on intersection testing   Used ray tracing from sources to detect first-order diffraction edges, followed by a traversal of a precomputed edge-to-edge visibility graph to find all diffraction paths originating at those edges  Reduce the number of edges considered and allowed computation of diffraction up to order 3 or 4 in real time But retains exponential algorithmic complexity      2.3. Mesh Simplification for Acoustics  Compared to graphics rendering, room acoustic simulation is quite tolerant to aggressive geometry simplification, provided that properties of the environment such as volume and surface area are preserved  Due to the long wavelengths of low-frequency sound, low spatial resolution of spatial audio as well as the diffuse nature of late reverberation Simplification also tends to increase the size of planar surfaces relative to the wavelength, which may improve accuracy for GA   Methods  Extract significant faces using a regular grid subdivision of the scene followed by clustering and bounding box fitting to approximate the input geometry  Drastically reduce the number of faces in a mesh but it does not preserve details, topology, or scene volume   Use a remeshing approach to first voxelize the scene and then extract an isosurface  This isosurface extraction was followed by coplanar face merging to reduce the number of faces, and post-processing to patch cracks in the mesh surface   Use a remeshing approach to simplify the mesh, except that the edge collapse algorithm was used instead of coplanar face merging  Generated different geometry for each simulation wavelength using proportionally-sized voxel grids, and applied a parallel edge merging step to reduce the number of edges considered for diffraction   Performed frequency-dependent simplification and used meshes with varying level of detail to speed up real-time simulation  Using time-dependent geometry, where lower levels of detail would be used for higher-order reflections To reduce the number of edges considered for diffraction, compared the angle between adjacent faces to a threshold as a way to select significant diffraction edges      3. Overview  Preprocessing stage  Apply a series of operations to first simplify input meshes, then identify important silhouette diffraction edges using a ray-based heuristic Augment the edges with additional diffraction geometry and also precompute a visibility graph between the edges that is used to accelerate the runtime exploration of high-order diffraction paths   Runtime stage  Use bidirectional stochastic ray tracing to explore possible diffraction paths, then validate those paths using robust visibility tests  The resulting paths are stored in a persistent cache to add temporal stability over successive simulation updates   Utilize the precomputed edge visibility graph and the A* pathfinding algorithm to efficiently and robustly find additional high-order diffraction paths The output of the runtime simulation module is a collection of frequency-dependent room impulse response parameters  These parameters are the input to the audio rendering module which uses standard signal processing techniques to auralize the impulse response parameters   The final output audio is spatialized using the listener‚Äôs head-related transfer function and then reproduced over headphones    4. Diffraction Mesh Preprocessing   Input: Raw triangle mesh with acoustic materials assigned to each triangle\n  Stages:\n  Apply standard mesh simplification algorithms to reduce the input mesh complexity to a given error tolerance\n  Identify a subset of the edges of the mesh that are relevant for diffraction using a novel silhouette edge identification method and apply additional post-processing to simplify and cluster the selected edges\n  Output: A collection of mesh boundaries\n Sequences of edges that make up the same logical diffraction edge    An important part: decouple the diffraction edges from the underlying surface geometry to reduce the number of edges considered for diffraction at runtime\n    Augment the simplified mesh with additional diffraction geometry (flags) that bisect the outside angle of each boundary\n These flags are used at runtime to detect when a ray passes nearby a diffraction edge, similar to the Uncertainty Principle method    Precomputing a graph of edge-to-edge visibility that can be used to accelerate pathfinding during the runtime algorithm\n    In the case of moving geometry, apply this pipeline separately to each rigid part of the scene\n  4.1. Mesh Simplification   Apply standard mesh simplification approaches to reduce the overall geometric complexity\n  Benefits\n By reducing the number of triangles, the number of edges that must be considered in the other preprocessing stages is also reduced After simplification some geometric connectivity problems may be corrected Improves the consistency of the local mesh curvature and this helps with correct identification of diffraction edges    Make use of vertex welding and edge collapse operations\n Forgo the use of voxelization and marching cubes due to the artifacts that can be introduced when using large voxels, as well as their tendency to actually increase the number of diffraction edges by beveling sharp corners after surface reconstruction    Vertex welding is applied by greedily clustering each vertex with its neighbors within a certain tolerance distance $\\epsilon_{weld}=0.001\\mathrm m$\n Use a spatial hashing approach to implement this with $O(N)$‚Äã time complexity    Edge collapse algorithm is similar to the standard approach, but with a few extensions\n Limit the maximum amount of error that can be introduced in triangles normals to $\\epsilon_m=10¬∞$‚Äã, while the original algorithm only prevents flipping of triangles ($\\epsilon_n=90¬∞$‚Äã‚Äã‚Äã)  Help to preserve the overall shape of the mesh better, particularly at the silhouette edges and mesh corners which are important for diffraction   Prevent simplification across acoustic material boundaries    4.2. Diffraction Edge Extraction 4.2.1. Initial Edge Selection   Determine which edges in a mesh are relevant for diffraction\n Too few edges are selected -\u0026gt; Cause some diffraction paths to be missed -\u0026gt; Abrupt occlusion Use only the edges that can produce significant diffraction in order to get the best performance Identify those edges is a non-trivial task    Edges that are between two faces with similar normals are unlikely to produce any significant diffraction\n Previous approach: Dihedral angle to classify edges as diffracting  The angle between adjacent face normals $\\theta_s$, is compared to a threshold angle $\\epsilon_\\theta$, to determine if the shared edge is a diffraction edge If $\\theta_s\u0026gt;\\epsilon_\\theta$, that edge is classified as a diffraction edge This can greatly overestimate the number of diffraction edges for highly-tessellated curved surfaces because it uses only local information  This is especially problematic for highly tessellated meshes or curved surfaces that have $\\theta_s$ close to 0 In such meshes, the value of $\\epsilon_\\theta$ must be close to 0 to find all relevant edges, which cause many extraneous edges to also be selected -\u0026gt; poor runtime performance   It\u0026rsquo;s difficult to find a value $\\epsilon_\\theta$ that works robustly for all inputs   A novel diffraction edge identification method based on face normal clustering  Adapt Felzenszwalb graph segmentation algorithm  Face adjacency graph   Cluster adjacent faces that have similar surface normals The boundaries between the face clusters are then used as the initial set of diffraction edges Compared to existing approaches based on local curvature, this approach works well on meshes with any level of tessellation      The novel diffraction edge identification method\n  Computing the weight for each edge between adjacent faces in the face adjacency graph\n Propose using the cosine of the angle between each pair of adjacent face normals, $w(f_i,f_j)=\\cos(\\theta_s)=\\vec n_i\\cdot\\vec n_j$    These weights are sorted in decreasing order, such that face pairs that have more similar normals come first\n  Each face in the mesh is initially assigned to its own unique cluster, where each cluster maintains information about the distribution of surface normals for faces that belong to the cluster\n  Represent the normal distribution using a cone where the axial direction $\\vec a$ approximates the average normal and the opening angle $\\theta_{\\vec n}$ approximates the spread of normals within the cluster\n  The algorithm proceeds by inspecting each face pair in order of decreasing weight and evaluating whether or not the clusters that the faces belong to can be merged\n    To determine if merging two clusters is possible:\n   Compute the merged normal cone for the two clusters, i.e. the smallest cone that contains both merged cones   Given two cones $i$‚Äã and $j$‚Äã, this can be efficiently approximated by first computing the vector $\\vec x_i$‚Äã, on the boundary of cone $i$‚Äã that has the greatest angle with the axis of cone $j$‚Äã, and vice versa to yield $\\vec x_j$‚Äã\n  The average of the two extreme\nvectors is then used as the merged cone axis and the angle between them is used as the opening angle of the cone $\\theta_{\\vec n}$\n  The clusters are then merged if $\\theta_\\vec n$‚Äã is less than merging threshold for either cluster\n  The merging threshold is maintained separately for each cluster and is initially set to $\\tau=\\epsilon_\\theta$ at the beginning of the algorithm, where $\\epsilon_\\theta$‚Äã‚Äã‚Äã ‚Äãis the minimum dihedral angle to consider for diffraction\n  After two clusters are merged, the resulting cluster\u0026rsquo;s threshold is increased according to the following relation:\n$$\n\\tau(F_i\\cup F_j)=\\theta_{\\vec n}+\\frac{k_{\\epsilon_\\theta}}{|F_i|+|F_j|}\n$$\nwhere $|F_i|$ and $|F_j|$ represent the number of faces in the constituent clusters\nwhere $k=4$ is a parameter that controls the scale of the clusters\n  This has the effect of requiring stronger evidence for a boundary between small clusters\n       Merge clusters smaller than a certain threshold with adjacent larger clusters\n This is necessary in the case of noisy mesh data such as that from 3D reconstructions where there may be occasional small clusters that are not included in the cluster for a large flat wall For each cluster that is considered too small, we merge it into the neighboring cluster that has the most similar average surface normal    Once the face clusters are computed, extract the boundaries between the clusters as the initial set of diffraction edges\n Each boundary is a set of edges that have adjacent faces belonging to the same 2 clusters These boundaries are then provided to the next stage of the preprocessing pipeline      4.2.2. Curved Boundary Splitting   The face clusters in the previous step can have any shape\n There is no restriction on the collinearity of the edges that make up a cluster boundary In conflict with the final goal of turning each boundary into a single straight diffraction edge Propose a simple approach for splitting mesh boundaries into collinear segments    Curved Boundary Splitting\n Calculate a bounding cylinder of the vertices  Axis  The dominant direction of the boundary Defined by the two vertices in the boundary that are farthest apart   Radius  A measure of how collinear the vertices are Given by the maximum distance of a boundary vertex from the axis line segment   The cylinder is used to determine whether or not a boundary should be split into more than one boundary   A boundary should be split if the aspect ratio of its bounding cylinder $\\Big(\\frac{2r}{h}\\Big)$ is more than a certain threshold, e.g. 0.025  The splitting point is chosen to be the boundary vertex that is farthest from the cylinder‚Äôs axis The edges that are on either side of the split are placed into one of the two resulting boundaries  These new boundaries are then recursively split until their aspect ratio falls below the threshold     The output of this stage is a collection of mesh boundaries that are known to be approximately straight.    4.2.3. Silhouette Edges   Only consider the diffraction that occurs in the shadow region of an edge\n  The only edges that can produce diffraction are silhouette edges\n  i.e. Those edges that can cast a ‚Äúshadow\u0026quot; when illuminated from a point in the conservative shadow region\n The edge axis is perpendicular to the image The edge is shared by faces $f_0$ and $f_1$ that have face normals $\\vec n_0$ and $\\vec n_1$ The exterior angle between $f_0$ and $f_1$ is bisected by edge normal $\\vec n_e$ and a planar diffraction flag that extend a distance $d_{flag}$ from the edge Conservative shadow regions: defined by the planes of $f_0$ and $f_1$‚Äã Shadow regions: bounded by the horizon plane and plane of face $f_{shadow}$  The horizon plane, with normal vector $\\vec h$: defined by the edge vertices and the reference point $\\vec p_{ref}$‚Äã, which corresponds to a source, listener, or point on a previous diffraction edge, calculated as $\\vec h=(\\vec v_1-\\vec v_0)\\times (\\vec p_{ref}-\\vec v_0)$   $\\epsilon_h$ and $\\epsilon_f$ extend the shadow region on the horizon and face sides Enable smoother transitions between direct and diffracted state      A novel approach to reliably identify these silhouette edges\n Utilize global information about the structure of the mesh acquired through stochastic ray tracing from points on an edge to determine whether or not a given edge is a silhouette Based on the observation that in order for an edge to contribute to diffraction, it must be able to cast a shadow, and that a source or listener with non-zero size must be able to go into the conservative shadow region on both sides of the edge Main idea: if there are other parts of the mesh that completely obstruct one or both sides of a given edge such that no sound source or listener can form a shadowed path over the edge, that edge cannot produce any diffraction paths    Algorithm\n  The information can be determined approximately by stochastic ray tracing in the conservative shadow regions (CSR) of each edge\n  Emit rays that randomly sample the CSR from uniformly-sampled random points on the edge\n  The number of rays traced for an edge, $N_{samp}$, is determined by its length and the angular size of the CSR:\n$$\nN_{samp}=\\min\\Bigg(N_{samp}^{max}, \\dfrac{|\\vec v_1-\\vec v_0|_2}{h_d}\\dfrac{\\theta_s}{h_\\theta} \\Bigg)\n$$\n $\\theta_s=\\cos^{-1}(\\vec n_0\\cdot \\vec n_1)$ is the angular size of the CSR $h_d=0.1\\mathrm m$ is the distance sampling resolution $h_\\theta=5¬∞$ is the angular sampling resolution In practice, $N_{samp}$ is limited to a reasonable maximum value, e.g. $10^4$  To prevent spending too much time on very long edges        To sample the outgoing ray direction, use a uniform spherical distribution that has been modified to generate rays in a wedge\nshape\n  The outgoing ray direction in the local tangent space is given by:\n$$\n\\vec r_d=\\Big(u_0, \\sqrt{1-u_0^2}\\sin u_1, \\sqrt{1-u_0^2}\\cos u_1 \\Big)\n$$\n $u_0$: a uniform random variable in the range $[0,\\theta_s)$ $u_1$: a uniform random variable in the range $[-\\sin\\theta_r, \\sin \\theta_r]$  $\\theta_r=30¬∞$ controls the amount of spreading of the rays in the direction of the edge axis $\\theta_r=0$‚Äã‚Äã‚Äã would generate rays that are always perpendicular to the edge      Once the local ray direction is generated, it is rotated to mesh space by applying the orthonormal rotation matrix $\\pmb R_i=\\begin{bmatrix}\\frac{\\vec v_1-\\vec v_0}{|\\vec v_1-\\vec v_0|_2},\\vec n_i,\\vec t_i\\end{bmatrix}$, where $i$ is the face index\n      Classify an edge as silhouette if both sides of the CSR have at least $N_{valid}=1$ rays that don\u0026rsquo;t hit anything within a certain distance, $\\tilde d_s$\n  Use this information as a proxy for whether or not a source or listener can be occluded by the edge\n  The distance $\\tilde d_s$‚Äã is proportional to the diameter of a source or listener, $d_s$\n As a source or listener grows bigger, an edge must protrude further from the nearby geometry to produce any diffraction $d_s$‚Äã is a parameter of our algorithm that controls how aggressive the silhouette edge detection is  Here use $d_s=0.25\\mathrm m$, which roughly corresponds to the size of a human head     Yellow circle: a sound source or listener with diameter $d_s$‚Äã Red rays: represent random rays generated by $\\vec r_d=\\Big(u_0, \\sqrt{1-u_0^2}\\sin u_1, \\sqrt{1-u_0^2}\\cos u_1 \\Big)$ In this figure:  Edge (a) is not silhouette edge because the circle cannot be placed where it is completely occluded by the edge and all of the rays hit\nanother face before traveling distance $\\tilde d_s$ Edge (b) is classified as a silhouette edge because at least $N_{valid}$ rays were able to travel for a least distance $\\tilde d_s$‚Äã‚Äã and because the circle can be occluded by the edge      In order for the approach to work correctly, the value of $\\tilde d_s$‚Äã must increase for rays that are closer to the CSR boundary\n  If $\\tilde d_s$‚Äã did not increase for rays near the CSR boundary, Edge (a) would be erroneously classified as a silhouette edge\n  The value of $\\tilde d_s$ for each ray using the following relation:\n$$\n\\tilde d_s=\\dfrac{d_s}{\\max(\\cos(u_0),\\epsilon)}\n$$\nThis causes the threshold distance to increase substantially for rays that have large $u_0$\n        Summary of silhouette detection algorithm\n Inspects every input edge and traces rays to determine if that edge is a silhouette If at least $N_{valid}$ rays on both sides of an edge are able to travel a distance of at least $\\tilde d_s$‚Äã before hitting other geometry, then that edge is classified as a silhouette    4.3. Diffraction Geometry Construction   Inspired by the Uncertainty Principle (UP) diffraction approach\n Augmenting the main geometry with diffraction flags - quadrilaterals that bisect the outside angle of diffraction edges and protrude a distance proportional to the wavelength of the lowest frequency band, e.g. $d_{flag}=6\\lambda$    Construct a similar set of diffraction flags but do not require any particular flag length, $d_{flag}$\n The accuracy of the diffraction approach does not depend on the flag length due to the use of the analytical UTD diffraction model Changing the flag length changes the diffracted sound intensity for UP because in that model the flag is an integral domain In this paper\u0026rsquo;s approach, the length of the diffraction flags controls how likely it is for a ray to intersect a flag and find diffraction paths over the associated edge  Use $d_{flag}=1.0\\mathrm m$‚Äã‚Äã‚Äã as a reasonable tradeoff between finding enough diffraction paths and spending too much time on ray-versus-flag intersection tests for rays as they traverse the scene      Algorithm\n  Convert the input mesh boundaries, each made up of one or more edges, into singular diffraction edges that act as proxies for the underlying surface geometry\n Decouple the surface geometry representation from the edges used to compute diffraction effects Each roughly collinear mesh boundary is approximated with a single straight edge  Apply the approach discussed in previous section Curved Boundary Splitting a second time to compute the best-fitting proxy edge for a mesh boundary   Calculate the local geometric information needed for diffraction, namely the adjacent face normals of the proxy edge  Compute the area-weighted average of the face normals on each side of the boundary after assigning each adjacent face to one side or another based on the similarity of its normal vector to the faces processed so far      Determine where to place the two far vertices for each flag\n  The simplest approach\n  Place the far vertices at distance $d_{flag}$‚Äã form each edge vertex in the direction of the edge normal, e.g. $\\vec v_i+\\vec n_e d_{flag}$\n Work well in many cases, but can fail with certain meshes Consider the diffraction edges at the top ring of a tessellated cylinder, like follow figure:   Placing the far vertices along the edge normal produces many gaps in the ring of flags that may reduce the effectiveness of the runtime diffraction algorithm    Using the vertex normals rather than the edge normal to determine the far vertex locations, e.g. $\\vec v_i+\\vec n_{v_i}d_{flag}$\n Exception: if both vertex normals point toward the center of the edge, we use the edge normal instead Use vertex normals on the convex parts of the mesh and edge normals on the concave parts        Support intersecting rays against either the surface mesh or the flags\n Put the additional flag geometry in a separate mesh and acceleration structure with the same transformation as the surface mesh A bitmask is then used by the ray tracer to select what type(s) of geometry each ray should intersect with  Flags should not interfere with next event estimation in the path tracer or line-of-sight checks in the runtime diffraction algorithm        4.4. Diffraction Graph  Build a separate directed edge-to-edge visibility graph between the final set of diffraction edges for each rigid mesh in the scene  Used in the runtime graph traversal algorithm to speed up the search for diffraction paths The data structure is a flat array of edge neighbor indices Generate the graph in a different way that scales better to complex scenes with many edges  In practice most edges can only diffract with a few neighbors   Handles approximate partial visibility and has $O(N)$ time complexity Utilize the diffraction flag geometry along with stochastic ray tracing to determine whether or not edges are mutually visible For each edge $e_i$ in the mesh, the paper emit random rays in the conservative shadow regions according to the same distribution used to generate silhouette rays (See Section 4.2.3), but with $\\theta_r=60¬∞$  These rays are then intersected with the surface mesh to find the ray endpoint at distance $d_{max}$ The same ray is intersected with the diffraction flags to find all hits along the ray up to distance $d_{max}$  For each flag intersection, we check to see if the associated edge, $e_j$, is in the CSR of the edge $e_i$ that emitted the rays This condition is met when the signed distance of an edge endpoint to the face planes of the other edge is more than $\\epsilon_f$ for one plane and less than $\\epsilon_f$ for the other  Additional tolerance $\\epsilon_f$ that prevents edge pairs that share a face plane from being discarded     It this succeeds, a directed link is added to the graph from edge $e_i$ to edge $e_j$     Another improvement  Make to the graph data structure is to partition the links originating from a given edge into two sets corresponding to the two sides (i.e. CSR) of the edge that generated the connections  By partitioning the outgoing links in this way, the graph search algorithm can be sped up by about a factor of 2   Example: if the current position of the diffraction graph search is on one side of the edge, it only has to explore neighboring edges that were visible to the far side of the edge because the other edges would not be able to form valid diffraction paths    5. Diffraction Runtime   Problem\n Finding direct diffraction paths between every source and listener in the scene each time the simulation is updated    Approach\n Based on the idea of intersecting rays with additional diffraction flag geometry, originally proposed for the UP diffraction method In contrast to the UP approach, the paper don‚Äôt rely on stochastic ray tracing to directly calculate the diffraction path intensity  Use a UP-like approach to find sequences of diffraction edges in a random ray traversal, but instead use the UTD diffraction model to analytically compute the path intensity   Maintain a persistent cache of these paths over the course of the simulation to improve the temporal coherence Propose a graph traversal algorithm to find high-order paths more quickly    Advantages\n Like UP, its time complexity does not scale exponentially with the maximum diffraction order and it can be easily integrated into existing acoustic path tracers  Enables very high-order diffraction to be calculated with good performance, even in scenes with high geometric detail   Unlike UP, the degree of convergence of the results does not depend on the number of rays traced Efficiently handle diffraction between multiple dynamic objects    5.1. Ray Tracing   Core of runtime system: Bidirectional path tracer (BDPT) with multiple importance sampling\n Use this to compute early reflections and to build an energy-decay histogram for the late reverb from which frequency-dependent reverberation times can be determined Diffraction approach is integrated within the path tracer and is similarly bidirectional  It can find diffraction paths starting from either the listener or a source This bidirectionality improves the likelihood of finding paths in certain geometric configurations where either source or listener is highly occluded      In the path tracer, consider diffraction only for subpaths originating at a source or listener that have not yet intersected any surfaces\n This is consistent with our restriction to only direct diffraction For these subpaths, intersect the constituent rays with both surface geometry and diffraction flag geometry to find the nearest intersection  If the intersection is with a surface mesh, we reflect or transmit the ray according to the surface material and disable intersections with further diffraction flags Otherwise, the ray hit a diffraction flag and remains a candidate for more diffraction events      Since flags can stick through geometry, we trace an additional ray from the ray vs. flag intersection point toward its projection on the edge to verify that the edge is visible\n If so, try to find paths to sources or listeners in the scene that are in the shadow region of the edge For each of these possible paths, we evaluate whether or not diffraction over the edge is valid, given the sequence of previous edges in the subpath If a precomputed diffraction graph is available for the intersected mesh, we can also perform a deterministic graph search to find additional high-order diffraction paths (Diffraction graph traversal)  Analogous to deterministic next event estimation in a path tracer If the edge is not in a valid configuration to produce diffraction, the ray continues past the flag in its current direction without modification      After any paths have been found for the current edge, we modify the outgoing ray direction to explore the scene further\n One possible ray distribution is the diffraction probability density function (DAPDF) proposed for the UP diffraction model  However, found in practice, a simple lambertian distribution on the opposite side of the flag empirically finds more diffraction paths and is faster to sample   Once the ray is redirected, we modify the ray‚Äôs frequency-dependent energy according to the DAPDF  Ensure that the outgoing ray has the correct diffracted energy for its direction, and also that further reflections of that subpath are influenced by the diffraction that occurred earlier in the path      Ray tracing repeats until a surface mesh is intersected or a maximum number of diffractions occur, at which point the further rays for the subpath are handled using standard BDPT\n  5.2. Path Validation 5.2.1. Shadow Test  Each diffraction edge in a subpath must intersect the shadow region of the previous edge, if one exists  The shadow region is defined as the intersection of the two half-spaces corresponding to the shadow face plane and the shadow horizon plane The shadow horizon plane is defined by the diffraction edge vertices $\\vec v_0$, $\\vec v_1$ and the reference point $\\vec p_{ref}$‚Äã, which for the first edge is the source or listener position For diffraction beyond 1, $\\vec p_{ref}$‚Äã‚Äã is the point on the previous edge that creates the largest (i.e. closest to conservative) shadow region  This can be determined by clipping the previous edge‚Äôs line segment with the face planes of the current edge, so as to limit the previous edge segment to only the part in the current edge‚Äôs CSR on the non-shadowed side  If the previous edge is completely outside of this region, diffraction cannot occur between the edges     The clipped endpoint that creates the shadow region with greatest angle is chosen as $\\vec p_{ref}$ and the horizon plane normal $\\vec h$ is calculated as $\\vec h=(\\vec v_1-\\vec v_0)\\times(\\vec p_{ref}-\\vec v_0)$ Use a few dot products to check if the current edge intersects the shadow region for the previous edge Once a potentially valid subpath is found, we can then check for connections to sources or listeners that are in the shadow region of the last edge using a similar shadow test    5.2.2. Shadow Test Tolerances   Allow a tolerance of $\\epsilon_h=1.0\\mathrm m$ for the horizon plane and $\\epsilon_f=0.1\\mathrm m$ for the shadow face plane\n  The tolerance $\\epsilon_f$‚Äã allows our approach to find diffraction paths between coplanar edges without numerical issues\n It also avoids problems where the diffraction wedge geometry doesn‚Äôt correspond exactly to the surface mesh  For instance, if the averaged face normals for a proxy edge are slightly wrong, the shadow test might reject otherwise valid diffraction paths. Introducing a face plane tolerance helps to avoid these geometric issues.      The large horizon plane tolerance $\\epsilon_h$‚Äã‚Äã‚Äã‚Äã is to anticipate diffraction paths before they are needed and enable smooth transitions between direct and diffracted sound\n This is important when a source or listener moves from the region where direct sound is valid into the shadow region of an edge It‚Äôs possible that a ray may not immediately hit the flag for the edge  Due to the random nature of the rays Resulting in a temporary gap in the audio until the diffraction path is found This phenomenon is more problematic with high-order diffraction because those paths are much less likely to be explored by random ray traversal   By anticipating diffraction paths that may soon become valid, those paths are more likely to be in the path cache when they are actually needed (i.e. when the direct sound becomes occluded) In the case where direct sound is unoccluded, these anticipated paths are not used for auralization    5.2.3. Visibility Test   For each source or listener in the shadow region, check to see if that source or listener can form a valid path back to the listener or source that emitted the subpath\n  Compute the apex points (points where diffraction occurs) on each edge using the Newton‚Äôs method approach\n An important detail: clamp the points to be on the edge\u0026rsquo;s line segment  This is needed for robust diffraction around curved surfaces with many small edges In such cases, the apex point often is not between the edges‚Äô endpoints, and rejecting these paths would make the diffraction significantly less robust      Once the apex points are determined, then trace a series of rays between the source, apex point(s), and listener to determine if the path is blocked by other geometry\n  Bias each apex point a variable distance $\\beta$ out from the edge along the edge normal $\\vec n_e$‚Äã‚Äã‚Äã to prevent self-intersection of rays with neighboring faces, and also to implement a robust soft visibility test\n  Main idea of soft visibility test\n If all rays in the path are unoccluded for some $\\beta \\in [\\beta_\\min,\\beta_\\max]$, then that path is considered valid   Set $\\beta=\\beta_\\min$ and then trace rays between all of the points along the path If any rays are blocked, we then geometrically increase $\\beta$ by a factor of 2 and trace more rays between the new biased apex points Repeat this until $\\beta\\geq \\beta_\\max$. If no $\\beta$ passed the visibility test, then the diffraction path is discarded   Use $\\beta_\\min=0.01\\mathrm m$ and $\\beta_\\max=1.0\\mathrm m$        5.3. Diffraction Path Cache   Purpose: reduce unnatural variation in the sound\n The diffraction paths that are found on each simulation update may be different because different random rays are traced  Lead to audible artifacts in real-time applications where the number of rays is small   Leverage the idea of a persistent cache of paths and adapt it to diffraction    The cache contains diffraction edge sequences from previous time steps that are known to be valid\n The cache entries are stored in a hash map data structure accessed by an integer key that is generated from a hash of the source index, listener index, and edge indices for a path At the beginning of each time step, the cache entries are revalidated using the approach from visibility test and newly invalid entries are discarded During ray tracing, as new valid paths are found, they are inserted into the cache Insert a special invalid entry in the cache to indicate that that edge sequence shouldn‚Äôt be checked again this frame  For explored paths that are known to be invalid   Use the cache to avoid checking the same edge sequences for diffraction more than once on each frame At the end of each time step, inspect the contents of the cache and pick the loudest single diffraction path for each source/listener pair  The intensity and direction for this path is then used for the final audio rendering whenever the direct sound is occluded i.e. the same interpolated delay line tap is used for direct sound and diffraction to ensure smoothness      In scenes with many edges, the number of paths that are in the cache can increase substantially\n If the cache is too large, it can slow down revalidation of the cached paths on the next simulation update  The cache also prioritizes the valid paths in the cache based on the intensity of the loudest frequency band   Use an additional min-heap data structure to dynamically rank the paths as they are found and discard all except the top $N$  $N$ is chosen to be proportional to the number of sources in the scene. e.g. $N=20$   If a new path is found and it is quieter than the Nth quietest path, we don‚Äôt add that path to the valid set (though we still mark that path as explored on this frame) This effectively enforces a maximum size for the set of valid paths in the cache globally for all sources/listeners While not directly perceptually motivated, this scheme produces an approximate kind of perceptual prioritization, where if the scene is complex, the quietest paths will be masked by the louder ones    Main reason to restrict the rendered output to just one path is because it helps to overcome deficiencies with the UTD diffraction model\n UTD assumes every edge is infinitely long and that the adjacent faces are also infinite  Cause UTD to produce a total diffracted sound field that is much too loud in some geometric configurations   Since UTD considers each path to be over an infinite edge, the sum of diffraction contributions is not physically correct or plausible By picking the single loudest (usually shortest) path, we get a diffracted sound field that is much closer to correct in these situations.    5.4. Diffraction Graph Traversal   Use precomputed edge-to-edge visibility graph\n Improve robustness and performance Increase the likelihood that we find valid high-order diffraction paths between sources and listeners that are separated by complex geometry, rather than relying on the random traversal of diffracted rays to find high-order paths Apply the A* algorithm from the agent navigation field to find the shortest diffraction path through the graph starting from the intersected flag While this only finds one path through the graph for each query, it tends to be a prominent path because of distance attenuation    The graph traversal begins whenever a diffraction flag with the correct orientation is intersected by a BDPT subpath\n Do a separate traversal for each source or listener in the scene that was not able to form a valid diffraction path directly over the edge, i.e. we only perform the graph traversal when a lower-order path was not found using visibility test At this point, we transform $\\vec p_{ref}$ and the goal source or listener into the mesh\u0026rsquo;s local space so that the search can operate locally to avoid transforming vertices and normals The search starts at the graph node corresponding to the intersected flag From there, we investigate only the neighboring nodes that are on the opposite side of the flag from $\\vec p_{ref}$    For each neighbor, compute the estimated distance from the neighboring edge to the goal source or listener\n This is the A* heuristic that is used to rank potential paths through the graph The choice of heuristic influences which paths are prioritized  Using the point on the neighboring edge that is closest to the line between the source and listener      Once the distance from the closest point on the neighbor to the goal is determined, it is added to the shortest distance through the graph from the starting edge to the current edge to yield the total estimated distance for the neighbor\n Discard any neighbors that are in the A* closed set and which have distance estimates greater than best path through the graph to that node, if the neighbor was previously visited If a neighbor is not in the A* open set or has a distance estimate lower than the best so far, we then check the edge further to see if it is in a proper geometric configuration for diffraction according to shadow test    Once all neighbors are either discarded or put into the A* heap, we check the top of the heap (i.e. the node with smallest distance estimate) to see if a valid diffraction path is formed from the starting node to the top node\n First check to make sure the goal point is inside the shadow region of the final edge If this succeeds, the edge sequence for the shortest path is reconstructed and transformed into world space for the final path validation and visibility testing  If the path is valid, that path is inserted into the cache and the graph search terminates Otherwise, the neighbors of that node are investigated recursively      This process repeats until a path is found or until a maximum number of nodes has been visited\n If no valid path through the graph exists, which is sometimes the case with diffraction through complex environments, A* degenerates to Djikstra‚Äôs algorithm and explores the entire graph By limiting the number of nodes that are visited, we can avoid spending a lot of time searching the extraneous parts of the graph when no path actually exists  Suggest using a limit that is 2 - 3 times larger than the maximum diffraction order      6. Implementation 7. Result 7.1. Scene 7.2. Preprocessing  The time taken by each section of our preprocessing pipeline  7.3. Runtime  The runtime performance with respect to the maximum diffraction order    The runtime performance with respect to the varying diffraction flag lengths\n  The number of paths found for varying diffraction flag lengths\n  7.4. Validation  Paper\u0026rsquo;s approach vs. Offline FDTD simulation   Two different configuration of paper\u0026rsquo;s approach vs. Offline FDTD simulation  8. Conclusions   Presented a complete approach for simulating approximate acoustic diffraction for real-time AR and VR applications\n Uses a novel mesh preprocessing pipeline to identify a reduced set of diffraction edges as well as construct diffraction flag geometry and edge visibility graphs Traces rays against the diffraction flags to probabilistically explore possible diffraction paths, then computes the path intensities using the UTD diffraction model Utilize a precomputed edge visibility graph and the A* algorithm to greatly speed up the exploration of high-order diffraction paths    This diffraction technique is between 2.7 and 586 times faster than the previous state of the art in real-time high order diffraction, depending on the\nscene, and is able to scale efficiently and robustly to large scenes with high geometric detail\n  Evaluated its objective accuracy by comparing to an offline FDTD wave simulation\n  Limitations\n Only consider direct diffraction between a source or listener, i.e. diffraction paths consisting of only edge diffraction with no reflections  In theory, combinations of reflection and diffraction are compatible with paper\u0026rsquo;s approach, but would require changes to how paths are stored and accessed in the diffraction path cache, and also changes to how the path intensity is evaluated The generation of unique cache identifiers for diffuse reflections may prove more difficult than for diffraction edges or specular reflections   Has all of the limitations of UTD such as inaccuracy with small edges  Other more-accurate diffraction models like BTM could be used in place of UTD to ameliorate some of these issues   The diffraction graph traversal algorithm only finds a single path per edge, though this nevertheless produces plausible results Since the graphs for each mesh in the scene are disjoint, the graph search can only find diffraction paths around individual objects    Future work\n Apply this diffraction method to mobile class devices where compute is extremely limited Explore possibilities for leveraging more precomputation to further reduce the runtime overhead of diffraction    ","description":"Siggraph 2021 Paper Reading","id":2,"section":"posts","tags":["Rendering"],"title":"Fast Diffraction Pathfinding for Dynamic Sound Propagation","uri":"https://chaphlagical.github.io/en/posts/paperreading/fast_diffraction_pathfinding_for_dynamic_sound_propagation/"},{"content":"Author: JOERG H. MUELLER, THOMAS NEFF, PHILIP VOGLREITER, MARKUS STEINBERGER, DIETER SCHMALSTIEG\nInstitution: Graz University of Technology, Austria\nLink: https://dl.acm.org/doi/10.1145/3446790\nAbstract Motivation\n Temporal coherence has the potential to enable a huge reduction of shading costs in rendering  Current Work\n Existing techniques focus either only on spatial shading reuse or cannot adaptively choose temporal shading frequencies  Idea\n Temporal shading reuse is possible for extended periods of time for a majority of samples Approximate shading gradients to efficiently determine when and how long shading can be reused  1. Introduction   An important strategy to reduce shading load is to exploit spatial and temporal coherence\n Spatial coherence  Checkerboard rendering, Foveated rendering Variable rate shading Motion-adaptive shading \u0026hellip;   Temporal coherence  Temporal anti-aliasing      Four unresolved research questions for shading reuse over longer periods of time poses:\n How do temporal artifacts affect the perceived image quality when reusing shading samples over time?  Perception of shading differences Conducting a controlled user study to determine the perceived effect of shading artifacts due to temporal shading reuse for scenes with advanced shading and animations   What are the limits of shading reuse in scenes with advanced shading and animation?  Temporal coherence Analyzing the potential amount of coherence in shading and visibility over time   How can we determine ahead of time when shading samples become invalid without actually reshading the samples?  Gradients Analyzing analytical and numerical first-order approximations of temporal shading gradients and their ability to predict the magnitude of future shading changes Analyzing the spatial variation of the temporal shading gradient and show how to incorporate spatial information to better predict future shading changes   Given sufficient temporally coherent shading samples, how can they efficiently be reused in practice?  Framework A general-purpose framework for predicting shading changes and temporally reusing shading over time      2. Related Work   Reduce shader invocations by exploiting shading coherence\n Foveated rendering Variable rate shading    Temporal coherence is commonly exploited by using information from previous frames for spatio-temporal filtering\n Temporal anti-aliasing  Use exponential-decay history buffers for filtering Use the temporal variation of sampling position to achieve spatial anti-aliasing      Shading gradients can be used to estimate the variation of shading and are thus often used in spatio-temporal filtering\n Guiding spatio-temporal upsampling filters Denoising filters Reconstruction in adaptive frameless rendering Spatial sampling    Temporal upsampling methods reuse previous shading result without filtering or accumulation\n Warps the image plane of the previous, fully rendered keyframe based on the latest headtracking update Advanced warping and reprojection techniques  Render cache Reverse reprojection caching  Use scene depth or motion vectors for dense 3D warping while reusing the shading from the last keyframe     Temporal methods are based on the assumption that the temporal variation in shading is slow and as spatial reprojection errors accumulate over time, a frequent refresh of the cache is required A fraction of samples typically violate this assumption and thus lead to perceivable artifacts, when not shaded more often    To avoid spatial reprojection errors while reusing shading samples multiple times, shading can be generated in alternative spaces and resampled for display\n Example: Generation of depth of field and motion blur  For efficiency reason, shading in alternative spaces requires GPU extensions   Similarly, the shading cache has been designed to allow for spatio-temporal shading reuse in path tracing Texture-space shading methods have been popularized and have even been used for temporal upsampling on a VR client Cons: All these methods only allow for fixed temporal upsampling rate    Numerous image quality metrics try to model the human perception of images\n peak signal-to-noise ratio (PSNR)  Provides an objective and easy to compute metric Failed to capture human perception   structural similarity index measure(SSIM)  Designed to more closely resemble perception   IW-SSIM  Made to SSIM to enhance the predictions of the metric   HDR-VDP-2  Try to model the visual system to some extent Adapt the metric for the evaluation of foveated rendering   VMAF  Combination of different metrics Especially useful for video game content   FILP  Derived from the manual method of comparing images by alternating between them and provides an error map showing where differences would be perceived between the two images in comparison    Major disadvantage above all: only compare images, disregarding any temporal artifacts, such as flickering\n  3. Perception of Shading Differences   Conducted a controlled user experiment\n  To determine the limits of keeping shading over multiple frames in scenes with advanced shading and animation\n  34 participants were shown two video clips, one generated with forward rendering as ground truth reference, and the other by reusing shading from previous frames and the participants were asked to rate the relative quality of the two video clips, following a pairwise comparison design\n As the order of clips was randomized, they did not know which clip was the reference.    From the rating, we compute an average relative quality score ($Q$‚Äã‚Äã), ranging from -2 to +2, where +2 means the reference is significantly better and +1 slightly better. 0 indicates that they have been rated equal\n  Compute the probability $p_{ref}$‚Äã of choosing the reference over the reuse approach\n A $p_{ref}$ of 50% indicates that there is no difference between the approaches $p_{ref}$ of 75% is referred to as 1 just-noticeable-difference (JND) unit Staying under 1 JND is considered high quality    For statistical analysis, use repeated-measures ANOVA and Bonferroni adjustment for post hoc tests\n  Test scenes:\n Physically-based materials Animated models Animated light sources Dynamic shadows      Temporal forward rendering (TFR)\n To determine the perceived quality reduction caused by reusing shading samples TFR decouples the temporal changes in shading from other major effects that influence the shading reuse ability  Temporal changes in visibility and spatial sampling   Use a modified fragment shader to compute shading as if a fragment was shaded at a specific time in the past Recreate all input parameters to the shader, including view, light and model matrices, textures and shadow maps for up to 120 frames (2 s) in the past and compare the shading results to the new shading  If the difference of a shading sample$(r,g,b)$ is above a certain threshold $T$, i.e., $T\u0026lt;\\max(|\\Delta r|,|\\Delta g|,|\\Delta b|)$‚Äã, we consider the shading to be changed Shading, including gamma-correction and possibly tone-mapping, is computed and compared in floating point This threshold is an approximation of Weber‚Äôs law, which states that the just noticeable luminance difference is constant in relation to the base luminance      Result\n Reusing shading samples that are slightly different does not reduce perceived quality For $T=2$ and $T=4$, $Q$ cannot be separated from 0.0 with confidence, and $p_{ref}$ is nearly 50% At a threshold of $T=8$, the mean quality is above 0.0, indicating that some participants see a minor quality deterioration. The distribution is still nearly balanced, with $p_{ref}$=55% At $T=16$, the distribution is just shy of 1 JND($p_{ref}$=75%). $Q$ is closed to 0.5‚Äã    4. Temporal Coherence for Shading Reuse 4.1. Temporal coherence of visibility  Project every sample of a current frame back to the previous frame and determine whether the sample was visible before Over 90% of samples stay visible between frames The most significant visibility disruption is caused by large camera movement or fast moving objects  4.2. Temporal coherence of shading  Use temporal forward rendering to determine how shading behaves independently of changes in visibility and spatial sampling More than 75% of all samples change less than the color difference $T=8$ for 120 frames in the test scenes  4.3. Limits of applying temporal coherence  Both the temporal coherence of visibility and the temporal coherence of shading demonstrate a very high potential for reusing shading over many frames Practical implementations need to also consider the spatial sampling of shading  The drift of shading samples, their reprojection error and the required filtering    Evaluate two practical rendering approaches for shading reuse:\n reverse reprojection caching (RRC)  RRC reprojects samples from the previous frame to the current frame, potentially accumulating spatial sampling errors The implementation runs in two passes: a depth pre-pass and a forward rendering pass that either uses the cache or reshades In order to avoid the accumulation of these errors, shading samples can be gathered in a temporally invariant space such as object space or texture space   shading atlas (SA)  Combine the shading atlas with the rendering pipeline of texel shading This method shades pairs of triangles in rectangular blocks that are dynamically allocated in a single texture, the shading atlas The location of the shading samples remains unchanged in the atlas, until the visibility of the triangles changes, or their resolution changes due to a level of detail change, in which case shading is recomputed    Evaluate result:\n Reverse reprojection caching accumulates spatial sampling errors over time, especially when the camera is moving The shading atlas reuse is independent of spatial sampling, and thus the reuse correlates better with the dynamics of the shading  The shading atlas considers samples reusable only when an entire block is reusable, leading to a slightly worse overall reuse    5. Predicting Shading Changes  Existing methods enable us to map shading samples from one frame to the next either through image space reprojection or shading in a temporally unaffected space, such as object space or texture space We require efficient prediction of the point in the future when shading samples will become invalid in order to know how long shading can be reused  5.1. Prediction with fixed upsampling rates  Previous strategies rely on uniform temporal upsampling  i.e., shading samples every $N^{\\mathrm{th}}$ frame   RRC  Updates 16 √ó 16 pixel tiles with a constant refresh rate A constant fraction of all tiles is updated in each frame, leading to a fixed livespan for each tile   SAU  The livespan of cache entries is also constant, but every cache entry has an individual remaining time to live depending on when it became visible   Evaluate RRC and SAU with the same user study design as described in Section 3 Result  Uniform upsampling is not able to leverage the potential for shading reuse well, even when reusing shading only once (2√ó upsampling) RRC at 2x temporal upsampling leads to noticeable differences in 82% of the cases For higher upsampling rates (4 and 8), all participants always noticed differences and reported image quality to be close to ‚Äúsignificantly worse‚Äù A uniform upsampling frequency is not sufficient for longer shading reuse    5.2. Prediction with shading gradients   A first-order gradient analysis is often sufficient in the spatial domain\n  Consider a Taylor approximation of a shading function $s$‚Äã‚Äã as an obvious choice for prediction\n A simple linear predictor can be formulated as a first-order Taylor expansion from time $t_0$ to $t$:  $$\ns(t)=s(t_0)+s'(t_0)\\cdot(t-t_0)+e(t)\n$$\nwith a residual error $e(t)$.\n Based on a color threshold $T$, we can predict a reshading deadlien  $$\nd=t-t_0=\\dfrac{T}{s'(t_0)}\n$$\n  Analytic derivatives\n Handle scalar, vector-valued parameters, texture lookup, Poisson-sampled shadow maps Shader inputs such as camera, object or light transformations, are extended with their temporal derivatives All time-varying parameters where a future state can be calculated deterministically (such as prerecorded animations or physical simulations): Obtain their derivatives directly by augmenting the animation code User-driven inputs (Camera movement): compute gradient based on an extrapolation of the input Even though the derivatives can be computed alongside the shading, the overhead is non-negligible  Finite differences\n  An alternative to costly analytic derivatives\n  The simplest case: take the backward difference between two shading results\n It better approximates the limit of finite differences But always requires shading twice in a row    Computed between shading in consecutive frames or between frames that are further apart in time\n  More economical and has a potentially beneficial low-pass filtering effect on spurious shading changes\n  When a shading sample is first considered, shading must always be done twice\n  A first deadline for reshading is extrapolated from the initial gradient\n    Comparison of gradient methods\n  Evaluate these options for temporal forward rendering (TFR), using the previously found threshold $T = 8$‚Äã from the first user experiment\n  Render multiple frames of increasing shading age, resulting in frames with the same sample position, but increasingly outdated shading\n Use this data to retrospectively obtain the ideal deadline Starting from the current frame containing the correct shading result Determine the exact frame in the past where the shading difference exceeds the threshold $T$  Analytical derivatives \u0026amp; finite differences: used to directly predict a future deadline Long-range differences: repeat the process to find the next frame in the past that exceeds the threshold  Limit the search process to 119 frames into the past, effectively clamping the deadline in the range of 1 to 118 frames        Result\n Late shading: cause artifacts in the final image output and thus should be avoided Early shading: harm performance, but does not lower quality Ideally technique:  avoid late shading completely keeping early shadings as low as possible   Simple long-range differences (between two shading points) show the least amount of late shadings, while having only slightly increased early shading    5.3. Spatial filtering of temporal gradients   Propose a simple maximum filter in image space, inspired by the render cache and shading cache\n Make shading decision based on the estimated temporal gradients of neighboring samples    Evaluation\n Using TFR to isolate the effect of the spatial filtering, while avoiding other sources of misprediction, such as reprojection errors Using a downsampling factor of 8 x 8, followed by a convolution with a rectangular kernel size 9 √ó 9    Result\n The gradient filtering distributes the highly localized gradients of the shadow boundaries to the surroundings   Applying an image-space filter on top of the gradients strongly reduces late shading but increases early shading, leading to less reuse and less performance improvement Long-range differences are most attractive    6. Temporal adaptive shading framework   Temporally adaptive shading (TAS) framework\n Reliably avoids repeating redundant shading computations, while responding instantly to areas where rapid changes of shading occur.    Reuse unit (RU)\n To make the framework largely independent of the rendering algorithm to which it is applied RU is a group of samples for which a uniform decision is made on whether the samples will be shaded anew or shading will be reused The samples of these units are shaded together, and, consequently, must be stored together in the cache data structure The renderer determines visibility independently for each unit An RU can be a single pixel as in the case of reverse reprojection caching or a whole block within the shading atlas    Workflow:\n  Spatially-filtered shading gradients from the last frame are multiplied with the time elapsed since the last shading of each RU and compared to the threshold ($T$‚Äã‚Äã‚Äã) to decide whether reshading is necessary. Newly visible units are always shaded for two consecutive frames to determine a gradient from finite differences\n  The shading is either reused, or the unit is reshaded. In the latter case, a new shading difference to the previous shading result is computed for each sample\n  The shading gradient is estimated based on the shading difference, scaled by the time difference between them, and a spatial filter is applied to distribute the shading gradient information\n    6.1. Temporally adaptive reprojection caching (TARC)  Image-space pixels serve as reuse units Replace the periodic refresh of the reverse reprojection caching shader with the first two steps of the framework Store per-unit and per-sample variables in a double-buffered G-buffer The input buffers are reprojected, and the potentially altered values are stored in the output buffers Store the estimated shading gradient in the G-buffer during the depth pre-pass Implement spatial maximum filtering by downsampling the gradient buffer using a maximum filter with overlapping square kernels In comparison to standard reverse reprojection caching, TARC needs additional memory for screen size buffers to store the shading difference and the time since the last shading  6.2. Temporally adaptive shading atlas (TASA)   Using a texture-space representation for storing shading samples avoids the accumulation of reprojection errors faced by TARC\n  Convenient to define reuse units by proximity of shading samples on object surfaces\n  The reuse units in TASA correspond to two triangles packed into a rectangle of $2^N √ó 2^M$‚Äã texels, where each unit‚Äôs size in the atlas is determined based on its image-space projection\n Other granularities (e.g., 8x8 texels, per-object texture charts, micro-polygons) could be chosen    By retaining the maximum of all shading gradients across an entire reuse unit, a conservative object-space filter is applied to the unit at almost no additional cost\n The samples of a reuse unit are processed together    The resulting object-space filtering is particularly relevant when some of the shading samples are currently occluded in image space\n  Limiting the filter to the boundaries of a reuse unit fails to capture spatial gradients that cross the boundaries of adjacent reuse units\n Example: a shadow boundary may be creeping slowly across an entire surface consisting of multiple neighboring reuse units A solution: extend the object-space filter to support a convolution-style kernel larger than a single reuse unit  But it\u0026rsquo;s a costly operation   Another solution: concatenate the per-reuse-unit filter to an image-space filter that determines the maximum over direct image-space neighbors  Very inexpensive Captures spatial coherence of perspectively close shading samples, which may not be apparent in object space      Resulting Pipeline:\n Exact visibility is computed per frame in a geometry pre-pass and stored in a G-buffer as primitive ID with corresponding shading gradients Reading the primitive ID, the atlas is updated such that it has room for the visible reuse units. The shading gradients are maximum filtered using a 2 √ó 2 window in image-space for each reuse unit to propagate the maximum gradient in an image-space neighborhood Shading decisions are made on all reuse units. Reuse units for which samples are newly allocated and reallocated in the atlas are always shaded, i.e., they are considered newly visible The shading workload is executed including the computation of the shading differences and shading gradients are directly maximum filtered per reuse unit The G-buffer is revisited for the final deferred rendering pass    The additional memory requirements include a copy of shading atlas to compute the shading differences, per-patch shading differences and times, and a screen space buffer for the spatial filter\n  7. Evaluation and Results  Test TASA with a 16 MPx atlas (TASA16) and with an 8 MPx atlas (TASA8) to evaluate actual use  To avoid sampling artifacts from the atlas when displaying the final image   Use a threshold of $T=8$ in all experiments  Aiming to stay below 1 JND   Present detailed timing results in comparison to Forward+ rendering Experimental setup  Three test scenes An image resolution of 1920 x 1080 Extended the tested sequences to 15 seconds   Run on Intel Core i7-4820K GPU and NVIDIA GTX 1080Ti using a custom rendering framework based on Vulkan  7.1. Reuse   In Section 4.3, the theoretically possible reuse with a perfect prediction of when to shade, resulting in a reuse of 80‚Äì90% for both TARC and TASA. About 1‚Äì5% of shading is due to changes in visibility\n  Actual reuse for the TAS implementations with a color difference threshold $T=8$:\n TARC shows low reuse for dynamic camera movements  The reprojection error for camera movements also effects shading gradient predictions, which are slightly too high and, in combination with the spatial filter, invalidate shading often While a smaller filter size would increase the reuse potential, it leads to clearly visible artifacts due to missing shading in some scenes A better reprojection filter for gradients and an adaptively sized image-space filter may increase reuse potential for TARC  More advanced filtering and filter size adjustments would also increase overheads     TASA is able to retain a high amount of reuse in comparison to its ideal version  The reuse reduction is similar for both stationary and moving cameras, underlining that shading in texture space enables consistent addressing of shading samples View-dependent shading effects do not heavily influence shading reuse Only in Space with its many highly metallic materials, a moving camera significantly reduces shading reuse      7.2. Quality  For $T=4$Ôºå $p_{ref}$ is close to 50%, and $Q$ is at about 0.1 For $T=8$, $p_{ref}$ is about 60%, still significantly below 1 JND, and $Q$ is 0.2, indicating a very high quality A setting of $T = 16$‚Äã is about twice as bad in $Q$‚Äã and very close to 1 JND, thus, we would suggest to use $T = 8$ For $T = 32$‚Äã, TARC is already above 1 JND, and $Q$‚Äã is close to ‚Äúslightly worse‚Äù Unknown reason for the slight drop in $Q$ for TASA16 from $T = 2$ to $T = 4$  as the confidence intervals overlap, this may just be a statistical outlier    7.3. Runtime   The overheads of TAS include computing shading differences, spatial filtering, and dynamically deciding whether to shade or not\n May lead to thread divergence during shading and thus reduce the efficiency    Measure the overhead of TARC and TASA in addition to the full shading\n TARC: between 14.5% to 16.9% TASA: between 2.3% to 5%    The actual speedups:\n Among the tested scenes, Space is especially difficult to speed up using temporal coherence, since most of the scene‚Äôs surface points are either very dynamic or belong to the sky box TARC does not improve over Forward+ for moving cameras, but it does have some considerable speedups between 1.38√ó and 2.4√ó when the camera is stationary The main focus is on the performance gains of TASA  Able to reuse shading across the left and right eye buffers in VR stereo rendering TASA outperforms the other methods for all scenes, both in mono and stereo rendering The speedup in stereo mode over Forward+ is in the range 2 - 5√ó (1.1 - 3√ó in mono mode)  TASA must compensate the overhead of its SA foundation SA alone is only around half the speed of Forward+ for monoscopic rendering, most likely due to its 8 MPx atlas size that is 4√ó the resolution of the final output image        Overall\n Adaptivity is key for temporal shading reuse  Uniform temporal reuse strategies reduce shader invocations, quality drops quickly Using simple shading differences with spatial filtering for gradient estimates works well and is efficient Especially placing shading samples in texture space appears to be an efficient strategy for reusing them over longer periods of time   Using an atlas that matches the screen resolution introduces spatial sampling artifacts and reduces sharpness  TAS can easily compensate for these additional shading samples, leading to overall performance gains    7.4. Free-moving virtual reality experiment   Integrated TASA into Unreal Engine 4 and conducted a small user experiment in VR\n Adapted the Showdown VR Demo scene , a slow motion fly-through of a combat scenario involving several soldiers fighting a giant animated robot The comparison to the threshold $T$‚Äã‚Äã‚Äã is evaluated after tone mapping with the Academy Color Encoding System (ACES) Filmic Tonemapper used in Unreal Engine 4    For VR user experiment\n Slightly modified the scene by subdividing large primitives that exceed the maximum block size in the shading atlas Modified the scene to include fully dynamic directional lighting with cascaded shadow mapping, which was only approximated in the original scene Eight participants (6 male, 2 female, age 24 to 33, with VR experience) tried the SA baseline, followed by TASA configurations using the thresholds [4, 8, 16, 32, 64] and SAU with 4√ó and 8√ó upsampling in a randomized order Used an Intel Core i7-8700K with an NVIDIA RTX 2080Ti and displayed on an HTC Vive at a resolution of 1512 √ó 1680 per eye, using a fixed frame rate of 90 Hz    Result\n For TASA with $T = 64$ and $T = 32$‚Äã, all participants detected artifacts  $T=64$ was ‚Äúvery bad‚Äù $T=32$‚Äã‚Äã‚Äã was ‚Äúadequate with some annoying artifacts‚Äù   For $T=16$‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã, four participants reported an identical experience compared to the baseline, while the remaining four detected minor artifacts on shadows, reflective surfaces and the soldiers in the scene For $T=8$‚Äã‚Äã‚Äã‚Äã, six participants reported an identical experience compared to the baseline, while two participants were still able to identify minor artifacts on the soldiers For $T=4$‚Äã‚Äã, no participant was able to detect any visual artifacts, and all participants reported an identical experience compared to the baseline     For SAU with 4√ó upsampling, four participants reported artifacts related to ‚Äújittery‚Äù and ‚Äúflickery‚Äù motion, and they reported ‚Äúlow frame rate‚Äù for the reflections For SAU with 8√ó upsampling, all but one participant reported major artifacts of reflections and shadows, as well as major discomfort, particularly describing the experience as ‚Äúvery uncomfortable when moving around‚Äù  One participant even reported a mild case of motion sickness   Constant temporal upsampling is more likely to be perceived as jittery, which according to the participants is more discomforting and distracting than the artifacts of TASA, even for large thresholds TASA with $T = 8$‚Äã resulted in a mostly identical experience to the baseline   TASA provides a more optimal performance-quality tradeoff compared to SAU  8. Discussion and Conclusion  Investigate how shading reuse is perceived and could benefit rendering, considering visibility, spatial sampling and temporal behavior of shading, separated and combined Evaluated the perception of outdated shading using a rendering approach that separates shading from visibility and spatial sampling effects, finding that a shading difference of 3% ($T = 8$‚Äã‚Äã after 8-bit quantization) is not noticed by study participants Even in highly dynamic scenes, many shading samples stay valid for extended periods of time, when considered independently of visibility and spatial sampling There is a potential of typically more than 80% shading reuse from frame to frame even in highly dynamic scenes at 60 Hz  The higher frame rates of VR increase this potential further   Accumulating spatial resampling errors limits the temporal reuse  Using texture-space caching of shading samples instead   Fixed upsampling techniques lead to noticable artifacts even at low upsampling rates  Extrapolating shading differences works very well when combined with a simple image-space filter for capturing spatio-temporal effects    8.1. Limitations   The simple box filter with a rather big kernel size used in TARC leads to considerable amounts of unnecessary shading\n A more advanced spatial filter could consider spatial gradients such as optical flow to resolve these issues at the cost of increased runtime and complexity    While the existing measures capture most changes, some less frequent ones can still cause artifacts\n Example: discontinuous rendering or changes that propagate from outside the image or from an occluded area Depending on the use case, specialized cases such as discontinuous changes, e.g. to light sources, can be caught on the scene object level A more general solution to capture artifacts from discontinuous shading could speculatively update samples that are not due yet    Evaluation is based on a single threshold applied to the per-channel maximum RGB color difference after tone mapping\n The threshold might be too conservative in certain areas of the HDR spectrum and overlook additional gains A more advanced method may be necessary    A method for deriving the threshold for noticeable differences from the perception of the human visual system has the potential to lead to further temporal savings\n This can possibly be done in a different color space or in the high dynamic range space before tone mapping Example: a higher threshold could be used for dark pixels that are close to bright ones, or a lower threshold needs to be used in dark areas where the visual system is more sensitive    The shading atlas shades pairs of triangles within rectangular blocks with power-of-two side lengths\n When the level of detail changes, a block of a different size is allocated and shading cannot be reused    8.2. Future work   Temporal reuse and TAS can be applied to other rendering techniques, including global illumination algorithms and ray-tracing\n  For the overall speedup, it is important to not only consider the non-shading workload, such as the geometry stage, but also pre- and post-processing, which do not necessarily lend themselves to shading reuse\n Motion blur and depth of field benefit greatly from spatial shading reuse, especially in object or texture space as shown by stochastic rasterization literature Many of the currently used preand post-processing techniques approximate global shading effects, such as shadows and reflections  If, by virtue of shading reuse, more time can be spent on the samples that actually require shading, this benefits the trend for moving global effect computation from post-processing to ray-tracing      In the worst case (discontinuous view change), the whole scene has to be shaded, this can lead to a higher variability in frame rate: Only some frames can be accelerated; others remain at the baseline speed\n The absolute frame time variability was unchanged in comparison to the baseline, while the mean frame time was reduced The frame time variability depends mostly on the complexity of the current view Lower frame rate variability could be obtained by using TAS as an oracle for a scheduling technique, which uses the predicted shading differences as priority, instead of making decision based on a fixed threshold    TAS can be easily combined with spatial reuse of sampling, such as variable rate shading, foveation and checkerboard rendering\n Bring more physically correct shading and fewer approximations that require pre-processing steps, like rendering shadow maps, or post-processing, like screen-space effects in deferred rendering    ","description":"Siggraph 2021 Paper Reading","id":4,"section":"posts","tags":["Rendering"],"title":"Temporally Adaptive Shading Reuse for Real-time Rendering and Virtual Reality","uri":"https://chaphlagical.github.io/en/posts/paperreading/temporally_adaptive_shading_reuse_for_real_time_rendering_and_virtual_reality/"},{"content":"Author: MILAN JARO≈†, LUBOM√çR ≈ò√çHA, PETR STRAKO≈†, and MATƒöJ ≈†PE≈§KO\nInstitution: IT4Innovations, VSB‚ÄìTechnical University of Ostrava, Czech Republic\nLink: https://dl.acm.org/doi/10.1145/3447807\n1. Introduction GPUs Rendering vs CPUs Rendering\n Limited memory size Example: Pixar\u0026rsquo;s Coco movie were using up to 120GB, do not fit into the memory of a single GPU  Main contribution\n  A solution to GPUs rendering memory size limitation\n Based on replication of a small amount of scene data, between 1% ~ 5%, and well-chosen distribution of the rest of the data into the memory of several GPUs Both replication and distribution of data is based on a memory access pattern analysis of a path tracer during a 1spp prepass The data with the highest number of memory accesses are replicated and the rest is stored only in the memory of the GPU that had the highest number of accesses to it Minimizes the penalty associated with reading data from the remote memory and is effective at providing near-linear scalability    Demonstration that our proposed approach works on a memory management level\n Any path tracing code that supports GPU acceleration using CUDA can adopt our approach without redesigning its internal data structures    Two key technologies\n NVLink GPU Interconnect  Enables multiple GPUs to efficiently share the content of their memories due to its high bandwidth and low latency   CUDA Unified Memory(UM)  Provides programmers with control over data placement across the memories of interconnected GPUs    2. Related Work 2.1. Out-of-core Rendering  Out of core rendering is the capability to render (with GPUs) scenes requiring more memory than the one directly connected to the device It will use the system\u0026rsquo;s memory instead  2.2. Distributed Rendering  sort first: Image based partitioning sort middle: Related to rasterization only sort last: Use scene data distribution  2.2.1. Image-Parallel Rendering  Distribute among processors or machines per blocks of pixels of a rendered image Most common and efficient way the scene data is fully replicated in all local memories and ray tracing is embarrassingly parallel In the case of ray tracing complex scenes that do not fit into local memory, this approach results in on-demand scene data movement while rays remains fixed (Moving scene data instead of ray data) Our proposed solution is based on scene data communication while rays never leave the GPU they are created on  2.2.2. Data-Parallel Rendering  Distribute the workload by subdividing the scene data In the case of distributed ray tracing, these approaches transfer ray data among processors or machines, while scene data do not move after the initial distribution (Moving ray data instead of scene data)  2.3. Distributed Shared Memory Systems  Shared Memory Processors (SMP) \u0026amp; Distributed Shared Memory (DSM)  Local memory with caches Hardware or software layer transparently creates an illusion of global shared memory for applications   The latency to access remote data is considerably larger than the latency to access local data  good data locality is therefore critical for high performance    2.4. CUDA Unified Memory For Multi-GPU Systems  UM Manages communication between multiple GPUs and CPUs transparently by adopting DSM techniques UM simplifies both out-of-core processing between GPUs and CPUs as well as multi-GPU processing and combinations of both NVLink interconnect is the key enabler of DSM multi-GPU system  3. Blender Cycles Path Tracer  An unbiased renderer based on unidirectional path tracing that supports CPU and GPU rendering For acceleration it uses a Bounding Volume Hierarchy (BVH) Supports CUDA, Optix, and OpenCL  3.1. Extensions for Multi-GPU Support Workflow:\n distribute the data structures evenly among all GPUs run the kernel with memory access counters and get the memory access statistics redistribute the data structures among GPUs based on memory access statistics run the original path-tracing kernel with redistributed data  3.2. Multi-GPU Benchmark Systems   BullSequana X410-E5 NVLink-V blade server\n with 4 Tesla V100 GPUs, each with 16 GB of memory and direct NVLink interconnect    NVidia DGX-2\n able to process massive scenes of sizes up to 512 GB in the shared memory of its 16 Tesla V100 GPUs, each with 32 GB of memory The uniqueness of this platform is the enhancement of the NVLink interconnect by using NVSwitches, which enable the connection of all 16 GPUs and higher bandwidth    3.3. Benchmark Scenes 4. Data Distributed Multi-GPU Path Tracing 4.1. Basic Distribution of Entire Data Structures 4.1.1. Memory Access Analysis  Define the order in which data structures are replicated as a ratio of the total memory accesses to a particular data structure over its size The analysis was done on the first sample when rendering the scenes with a resolution of 5,120x2,560 pixels   small_structures: a set of data structures smaller than 16MB (the most important one is svm_nodes, which stores Shader Virtual Machine (SVM) data and codes) bvh_node: stores the BVH tree without its leaves (leaves are stored in a separate structure) prim_tri_verts: holds coordinates of all vertices in the scene prim_tri_index: a set of all triangles in the scene and it contains indices to the prim_tri_verts  We can see that:\n The most important data structure is bvh_nodes, because it is responsible for 79.6% of all memory accesses  If it is replicated in the memory of all GPUs, the 79.6% of all memory accesses will be to the local memory The size of this structure is 7.2 GB, which represents 26.5% of the entire scene size   If in addition to small_structures and bvh_nodes, prim_tri_index and prim_tri_verts are also replicated, then the relative rendering time is only 109% while 40.7% of the scene is replicated and the rest is distributed  4.1.2. Performance and Scalability Evaluation The scalability of the proposed approach is evaluated for four different cases:\n all data structures are replicated‚Äîthis case serves as a baseline as it achieves the best performance and scalability all data structures are evenly distributed  continuous distribution: the structures are divided into large chunks of a size equal to the structure size over a number of GPUs, and each GPU owns one chunk round robin distribution: the distributed structure is divided into chunks of 2 MB, which are distributed in a round robin fashion   small structures and bvh_nodes are replicated while all other data structures are distributed small structures, bvh_nodes, prim_tri_index, and prim_tri_verts are replicated while all other data structures are distributed  Result:\n round robin distribution of small chunks performs better than continuous distribution of large chunks, therefore it is always used to distribute non-replicated data structures path tracing with fully distributed data structures does not scale on both platforms (there is reasonable scalability for two GPUs on DGX-2, but not beyond that) if small structures and bvh_nodes are replicated, the scalability is significantly improved if small structures, bvh_nodes, prim_tri_index, and prim_tri_verts are replicated, the scalability is further improved  4.2. Advanced Distribution Based on Memory Access Pattern and Statistics   The data placement is done with chunks, and hints are set for each chunk individually\n  The optimal chunk size was identified experimentally by benchmarking the path tracer performance for chunks of sizes from 64 kB to 128 MB:\n for scenes smaller than 30 GB the optimal chunk size is 2 MB (smaller chunks are not recommended) for scenes of sizes around 40 GB the optimal chunk size is 16 MB for scenes of sizes above 120 GB the optimal chunk size is 64 MB    The workflow of this data placement strategy:\n copy/distribute every data structure across all GPUs in a round robin fashion using chunks of an optimal size run the path tracing kernel with memory access counters for 1spp to measure the statistics gather the statistics on the CPU and run the proposed algorithm to get the optimal data chunks distribution use cudaMemAdvise to migrate or replicate all chunks run the original unmodified path tracing kernel    4.2.1. Memory Access Pattern Analysis   To identify the memory access pattern, per chunk access counters have been implemented in the GPU path tracing kernel of Cycles\n Independent counters for all data structures and all their chunks A total number of memory accesses per chunk can be recorded for each GPU    The memory analysis starts with all data structures being evenly distributed using a round robin distribution\n The modified path tracing kernel with memory access counters is executed on all GPUs for one sample    When the kernel finishes, then for every chunk of every structure, a number of accesses from all GPUs is recorded\n The workload is distributed among GPUs by horizontal stripes so that each GPU works on one stripe     1% of scene data covers between 56.7% and 74.4% of memory accesses, depending on the scene size This analysis shows that there are clear candidates among the chunks that should be replicated on all GPUs, while a major portion of the data is accessed infrequently and can be distributed with an acceptable impact on performance.  4.2.2. Data Placement Algorithm Based on Memory Access Pattern   The per GPU counters are summed to get the total number of accesses $a_{sum}$ for each chunk $c\\in C$ of each data structure $s\\in S$ and $a(a_{sum},s,c)$ tuple is created\n  All tuples are put into a single 1D array $H_{comb}$. The array is sorted by $a_{sum}$ from largest value to the smallest one and stored in $H_{\u0026lt;}$ array\n  The last input of the algorithm is the number of chunks that can be replicated $N_{dup}$. This value can either be set manually or automatically using formula:\n$$\nN_{dup}=\\dfrac{1}{C_s}\\Big(G_f-\\frac{S_s}{N_g}\\Big)\n$$\n $G_f$: the amount of free memory per GPU in MB available to store scene data $S_s$: scene size in MB $N_g$: total number of GPUs $C_s$: the chunk size in MB    Define a threshold $t$ as the $N_{dup}$-th element in the sorted array $H_\u0026lt;$ and evaluate all tuples in the array $H_\u0026lt;$. If the counter value $a_{sum}$ is larger than $t$, then the corresponding chunk will be set as *SetReadMostly*, and therefore replicated\n In the opposite case, the chunk is set as SetPreferredLocation and is assigned to the GPU with the highest number of accesses to this chunk  If the memory of this GPU is full, then the GPU with second, third, fourth, and so on, highest number of accesses is selected until a GPU with free memory is found.   It the counter value is equal to zero (without any accesses), then the corresponding chunk will be distributed in a round robin fashion across GPUs with free memory    4.2.3. Performance Evaluation  The performance of the proposed algorithm was evaluated for different ratios between replicated and distributed data at a 2 MB chunk level of granularity for the Moana 12 and 27 GB scenes   Figure shows the path tracing performance for one sample per pixel and 5,120x2,560 pixel resolution for both scenes and platforms, and for all available GPUs Once at least 1% of chunks are replicated, the performance is almost identical  4.2.4. Maximum Scene Size Analysis The following equation describes the maximum ratio of replicated data that fits into the memory of GPU memory for a scene of a given size:\n$$\nN_{max_dup}=\\frac{\\Big(G_f-\\frac{S_s}{N_g}\\Big)}{\\Big(S_s-\\frac{S_s}{N_g}\\Big)}\n$$\n $G_f$: the amount of free memory per GPU in MB available to store the scene data $S_s$: the scene size in MB $N_g$: total number of GPUs  5. Performance for massive scenes Group 1: Moana 38 GB, Museum 41 GB, Agent 37 GB, and Spring 41 GB are designed to stress the Barbora GPU server with 64 GB of total GPU memory\n the performance of the Barbora server is almost identical to the performance of DGX-2 for the same amount of scene replication (up to 10%) DGX-2 is able to further replicate scene data up to 60%, which improves performance by 2.8% only in the case of the Moana 38 GB scene (for the other scenes the performance is higher by only less than 1%) This means that for scenes of sizes approximately up to 45 GB distributed over 4 GPUs, the significantly less complex and cheaper GPU interconnect in the Barbora server is sufficient For the Museum, Agent, and Spring scenes 2% of scene replication attains optimal performance. This holds for 4, 8, and 16 GPUs Only the Moana scene needs higher amounts of replicated data, up to 25% for 16 GPUs Scalability can be evaluated on DGX-2 for 4, 8, and 16 GPUs only. For the Moana, Museum, Agent, and Spring scenes, for 5% scene replication, the parallel efficiencies, going from 4 to 16 GPUs, are 82.7%, 97.1%, 97.9%, and 98.1%, respectively. In the case of the Moana scene, a higher replication ratio is needed to improve scalability, e.g., for 25% data replication ratio the parallel efficiency is 94.4%  Group 2: Moana 169 GB, Museum 124 GB, Agent 167 GB, and Spring 137 GB are designed to stress the DGX-2 server with 512 GB of total GPU memory\n The performance is affected by selecting the right chunk size, particularly for the Moana scene For 8 GPUs and the Moana, Agent, and Spring scenes, if the replication ratio is 10% the scene does not fit into GPU shared memory anymore and chunks are swapped between GPU and CPU memory (the default behavior of the CUDA Unified Memory), which makes the rendering several times slower depending on the number of chunks being moved back and forth. This is the point at which our approach stops working, and therefore it is crucial to correctly select the replication ratio to avoid this situation For the Agent scene 1% of scene replication gives optimal performance for both 8 and 16 GPUs. The Spring scenes needs only 0.1% for 8 GPUs and 0.25% for 16 GPUs. The Museum scene needs 1% for 8 GPUs and 2% for 16 GPUs. Finally, the Moana scene requires 2% for 8 GPUs and 5% for 16 GPUs Scalability between 8 and 16 GPUs is good for all scenes. The parallel efficiencies are 93.8%, 98.8%, 98.6%, and 99.0% for the Moana, Museum, Agent, and Spring scenes, respectively   Analyze memory access pattern only for one sample per pixel Rendering times grows linearly with the number of samples   Number of bounces influence  6. Conclusions Contribution\n Presented a solution for path tracing of massive scenes on multiple GPUs Analyzes the memory access pattern of a path tracer and defines how the scene data should be distributed across GPUs with a minimal loss of performance Those parts of the scene data that have the highest memory access rate are replicated on all GPUs, because their distribution would have a major negative impact on performance  Methods\n Uses the same memory management rules for the entire data structure Splits the data structures into chunks and we control the placement and/or replication of each chunk separately  Feature\n Only control the memory allocations, the path tracer data structures do not have to be redesigned Take full advantage of NVLink 2.0 interconnect and its high bandwidth and low latency  ","description":"Siggraph 2021 Paper Reading","id":5,"section":"posts","tags":["Rendering"],"title":"GPU Accelerated Path Tracing of Massive Scenes","uri":"https://chaphlagical.github.io/en/posts/paperreading/gpu_accelerated_path_tracing_of_massive_scenes/"},{"content":"Abstract Real-time Rendering\n Frame rate Latency  Spatial Super Sampling: DLSS\n Decreasing the rendering time of each frame by rendering at a lower resolution  Temporal Super Sampling\n Producing more frames on the fly Problems:  It\u0026rsquo;s own computational cost The latency introduced by interpolating frames from the future    ExtraNet\n  An efficient neural network that predicts accurate shading results on an extrapolated frame, to minimize both the performance overhead and the latency\n  Rendered auxiliary geometry buffers of the extrapolated frame \u0026amp; Temporally reliable motion vectors\n  Train to perform two tasks:\n Irradiance in-painting for regions that cannot find historical correspondences Accurate ghosting-free shading prediction for regions where temporal information is available    A robust hole-marking strategy to automate the classification of these tasks\n  The data generation from a series of high-quality production-ready scenes\n  1. Introduction Some techniques to increase the rendering performance\nLeveraging the temporal coherency between frames: Reduces the sampling rate at each individual frame, and rely on temporal reprojections to combine samples taken over multiple frames to reconstruct a high quality image (Still performing spatial upsampling / reconstruction, with a virtually higher sampling rate acquired temporally to speed up the generation of the current frame of interest)\n TAA (Temporal Anti-Aliasing) TAAU (Temporal Anti-Aliasing Upsample) DLSS (Deep Learning Super Sampling) Ray Tracing Denoising  Limitation of current work\n Frame Interpolation: only when the next frame has been fully rendered, their algorithms can start to work to interpolate one or more frames in between Performance: generating a new frame is already more expensive than a full rendering of it Situation: more information can be used in rendered scene, like G-buffers  Contribution\n A new temporally interleaved real-time rendering and extrapolation architecture that can nearly double the frame rate without introducing additional latency. A novel neural network architecture that leverages G-buffers and temporal motion (computed both from consecutive frames and temporally reliable motion vectors) to perform the extrapolation as a shading prediction task. A lightweight design that incurs low performance overhead (around 8 milliseconds per frame at 720P) but results in high quality and smoothly extrapolated frames.  2. Related work and background 2.1. Temporal Reconstruction  Rely on temporal motion vectors Temporal reconstruction methods reduce the samples taken at each individual frame, and reproject samples over the course of multiple frames using backward motion vectors to reconstruct a high quality image In their paper  Leverage temporal motion vectors to reproject shading of the previous rendered frames to the extrapolated frames But they do not have any samples in the extrapolated frames, unlike temporal reconstruction    Motion Vectors\n$$\n\\pmb p_0^{t}=\\pmb P^t\\pmb M^t\\pmb V^t \\pmb s_0\n$$\n$$\n\\pmb p_0^{t-1}=\\pmb P^{t-1}\\pmb M^{t-1}\\pmb V^{t-1}\\pmb s_0\n$$\nStore screen space motion vectors =\u0026gt; Velocity Buffer\nvertex shader:\n1 2 3 4 5 6 7 8 9 10 11 12  uniform mat4 uModelViewProjectionMat; uniform mat4 uPrevModelViewProjectionMat; smooth out vec4 vPosition; smooth out vec4 vPrevPosition; void main(void) { vPosition = uModelViewProjectionMat * gl_Vertex; vPrevPosition = uPrevModelViewProjectionMat * gl_Vertex; gl_Position = vPosition; }   fragment shader:\n1 2 3 4 5 6 7 8 9 10  smooth in vec4 vPosition; smooth in vec4 vPrevPosition; out vec2 oVelocity; void main(void) { vec2 a = (vPosition.xy / vPosition.w) * 0.5 + 0.5; vec2 b = (vPrevPosition.xy / vPrevPosition.w) * 0.5 + 0.5; oVelocity = a - b; }   2.2. Texture In-painting   Using backward motion vectors to reproject shading from previous frame is that disoccluded regions will not contain any valid information in the previous frame, resulting in \u0026ldquo;holes\u0026rdquo; in the reprojected image\n  Some work:\n Convolution Deep learning Resort to a two-stage prediction strategy to separately predict missing textures in a step-by-step manner    Limitation\n Not designed specifically for real-time rendering Perform the in-painting task completely on single images without temporal information and G-buffers The running time of some methods are in the order of seconds, which are not suitable in our real-time extrapolation task    2.3. Image Warping  Realized by forward scattering or backward gathering, according to their data access patterns Compared with forward mapping, backward mapping is more appealing in real-time rendering due to its high efficiency Although visually plausible contents can be recovered at hole regions after image warping, shadow and highlight movements are almost untouched in these traditional methods, especially those used in the context of frame extrapolation  2.4. Video and Rendering Interpolation  Interpolation based frame rate upsampling is widely used for temporally upsampling videos, using optical flow of the last couple of frames Optical flow is an image based algorithm that often produces inaccurate motion, which will lead to artifacts in the interpolation results Uses accurate motion vectors available from the rendering engine, usually rely on image warping techniques to reuse shading information across frames Deep learning based video interpolation methods are still time-consuming, thus are not suitable for real-time rendering  3. Motivation and Challenge 3.1. Latency  Interpolation =\u0026gt; Produce significant lantency Extrapolation =\u0026gt; doesn\u0026rsquo;t introduce any additional latency  3.2. Challenge Disocclusion\n Backward motion vectors may not always exist  Traditional motion vectors propose to use the same pixel values from previous frames, resulting in ghosting artifacts in the disoccluded regions Occlusion motion vectors alleviates this problem by looking for a nearby similar region in the current frame, then look for the corresponding region from previous frames   Leverage an in-painting network to reshade the occluded regions using occlusion motion vector  Dynamic changes in shading\n Dynamic changes in shading over frames, such as moving shadows, parallax motion from reflections and the colors on rotating objects, may change drastically and are thus unknown in the current frame Always keep the most recently rendered 3 frames for the network to learn to extrapolate the changes  Other challenges\n Extremely low tolerance towards errors and artifacts The inference of the neural network must be (ideally much) faster than the actual rendering process  4. ExtraNet for frame extrapolation Design and train a deep neural network, namely ExtraNet, to predict a new frame from some historical frames and the G-buffers of the extrapolated frame.\n Considering that disoccluded regions of the extrapolated frame lack shading information but reliable and cheap G-buffers are available, we construct an In-painting Network to reshade the disoccluded regions with the help of occlusion motion vectors and those G-buffers In contrast to the general image in-painting task, regions that are not disoccluded may also contain invalid pixels attributing to dynamic shading changes in the scene. To tackle this issue, we design a History Encoder, an additional convolutional neural network, to extract necessary information from historical frames To meet the requirement of high inference speed which is critical to real-time rendering, adopting lightweight gated convolutions in the In-painting Network.  4.1. Problem Formulation  Frame $i$: Denoting the current frame rendered by the graphics engine Frame $i+0.5$: Next frame the network will predict Two historical frames including frame $i-1$ and $i-2$ and the corresponding G-buffers  Pipeline:\n Demodulation: Dividing by the albedo to acquire texture-free irradiance Warp the demodulated frame using both traditional backward motion vectors and occlusion motion vectors The warping operation probably incurs holes in the frame. Using In-painting network to fill the frame Modulation: multiplying by the albedo to re-acquire textured shading Apply regular post-processing(tone mapping ,TAA\u0026hellip;)  4.2. Motion Vectors and Image Warping Instead of computing a zero motion vector in disoccluded regions as the traditional backward motion vector does, occlusion motion vector computes the motion vector in disoccluded regions as the motion vector of the foreground in the previous frame\n4.3. Network Architecture   Similar structure with U-NET\n  Adopt gated convolutions\n$$\n\\pmb M=\\mathrm{Conv}(\\pmb W_m, \\pmb X)\n$$\n$$\n\\pmb F=\\mathrm{Conv}(\\pmb W_f,\\pmb X)\n$$\n$$\n\\pmb O=\\sigma(\\pmb M)\\odot \\pmb F\n$$\n $\\pmb X$: Input feature map $\\pmb W_m$ and $\\pmb W_f$: Two trainable filters $\\odot$ : Element-wise multiplication $\\sigma(\\cdot)$: Sigmoid activation    Gated convolutions increase the inference time\n Resort to a light-weight variant of gated convolution by making $\\pmb M$ a single-channel mask Not use any gated convolution in the upsampling stage  Assume all holes have already been filled in the downsampling stage      Use a residual learning strategy in our pipeline by adding the predicted image of our network with the input image warped by traditional backward motion vectors\n This stabilizes the training process    4.4. History Encoder   Input: Warped images of the current frame $i$ and its two past frames including frame $i-1$ and frame $i-2$\n Warp frames $i-1$ and $i-2$: accumulate the motion vectors Invalid pixels should be marked for these frames    Structure: Nine $3\\times 3$ convolution layers\n Down sample the input Shared by different historical frames    Output: Implicitly encode the high-level representations of shadows and shadings in previous frames, are concatenated together and fed into our In-painting Network\n  Optical flow can be explicitly predicted from several historical frames\n But correct forward optical flow is difficult to acquire in the frame extrapolation pipeline, since only historical frames are available Use backward optical flow as an approximation Not using it    4.5. Loss Function Penalizes pixel-wise error between the predicted frame $\\pmb P$ and ground-truth frame $\\pmb T$\n$$\n\\mathcal L_{l_1}=\\frac{1}{n}\\sum_i |\\pmb P_i - \\pmb T_i|\n$$\nhole-augmented loss: penalize more in the hole regions marked beforehand\n$$\n\\mathcal L_{\\mathrm {hole}}=\\frac{1}{n}\\sum_i |\\pmb P_i-\\pmb T_i|\\odot(1-\\pmb m)\n$$\n $\\pmb m$‚Äã is the binary mask fed into network  The shading-augmented loss: focuses on handling potential shading changes in the predicted frames\n$$\n\\mathcal L_{\\mathrm{shade}}=\\frac{1}{k}\\sum_{i\\in \\Phi_{\\mathrm{top}-k}}|\\pmb P_i-\\pmb T_i|\n$$\n Shading changes: stem from moving shadows due to dynamic lights and specular reflections Considering that shadows and reflections tend to generate large pixel variation among neighboring frames, we select $ùëò$‚Äã pixels with top $ùëò$‚Äã largest errors from the predicted frame and mark these pixels as potential shading change regions $\\Phi_{\\mathrm{top}-k}$ includes the indices of $k$ pixels with top $k$ largest errors  Currently, $k$ set to 10% of total pixels number    Final loss function\n$$\n\\mathcal L=\\mathcal L_{l_1}+\\lambda_{\\mathrm{hole}}\\mathcal L_{\\mathrm{hole}}+\\lambda_{\\mathrm{shade}}\\mathcal L_{\\mathrm{shade}}\n$$\n $\\lambda_{\\mathrm{hole}}$ and $\\lambda_{\\mathrm{shade}}$‚Äã ‚Äãare weights to balance the influence of the losses, here set to 1  4.6. Training detail  PyTorch Mini-batch SGD and Adam optimizer mini-batch size as 8, $\\beta_1 = 0.9$ and $\\beta_2=0.999$ in Adam optimizer default initialization applied HDR image: logarithm transformation $y=\\log(1+x)$ before feeding images into network  5. Dataset 5.1. Scenes and Buffers Each dumped frame comprises 10 buffers which can be divided into three categories:\n Actual shading frame before tone mapping (PreTonemapHDRColor) and albedo (Base color) for demodulation G-buffers used in our network, including scene depth ($ùëë$‚Äã, 1channel), world normal ($\\pmb n_ùë§$‚Äã‚Äã, 3 channels), roughness (1 channel), and metallic (1 channel) Other auxiliary buffers which are necessary for marking holes (invalid pixels) in the warped frame. They are motion vector, world position ($\\pmb p_ùë§$), NoV ($\\pmb n_ùë§\\cdot \\pmb v$, the dot product of world normal $\\pmb n_ùë§$ and view vector $\\pmb v$), and customized stencil ($ùë†$)  5.2. Marking Holes When reusing previous frames to generate a new frame, some pixels in the warped frame will be invalid, due to camera sliding and object moving. These pixels should be marked as holes before feeding into the network\n  For occlusion caused by object moving, used custom stencil to indicate the sharp changes along the edge of the dynamic object. When custom stencil value is different between the current frame ($ùë†$) and the warped frame ($ùë†_ùë§$‚Äã), these pixels will be marked as invalid:\n$$\n\\Phi_{\\mathrm{stencil}}={s_i-s_{w,i}\\neq 0}\n$$\nwhere $ùëñ$ is the pixel index and $\\Phi_{\\mathrm{stencil}}$‚Äã is the set including pixels that are counted as invalid according to stencil value\n  Self occlusions for some dynamic objects when they are moving or the camera is sliding. Resort to world normal since world normal will probably change in these regions. Calculate the cosine value between current frame‚Äôs world normal ($\\pmb n_ùë§$) and warped frame‚Äôs world normal ($\\pmb n_{ùë§ùë§}$), i.e., $\\pmb n_ùë§\\cdot \\pmb n_{ùë§ùë§}$. If this value is large than a predetermined threshold $ùëá_ùëõ$, the corresponding pixel indexed by $ùëñ$ in the warped frame is counted as invalid:\n$$\n\\Phi_{wn}={\\pmb n_{w,i}\\cdot \\pmb n_{ww,i}\u0026gt;T_n}\n$$\nwhere $\\pmb \\Phi _{wn}$‚Äã‚Äã contains invalid pixels marked by differences in world normal\n  Invalid pixels due to camera movement. These pixels can be selected out by world position, considering that static objects‚Äô world positions keep unchanged in a 3D scene. For a given pixel, let $\\pmb p_ùë§$ and $\\pmb p_{ùë§ùë§}$‚Äã‚Äã‚Äã‚Äã be the world position of current frame and warped frame, respectively. We calculate their distance by $|\\pmb p_ùë§ - \\pmb p_{ùë§ùë§}|$. If this distance is larger than a threshold $ùëá_ùëë$ (which is computed by NoV and depth values), we mark this pixel as invalid. Hence, the invalid pixels in this set $\\Phi_{wp}$‚Äã are\n$$\n\\Phi_{wp}={|\\pmb p_{w,i}-\\pmb p_{ww,i}|\u0026gt;T_d}\n$$\n  Finally,\n$$\n\\Phi_{\\mathcal{comb}}=\\Phi_{\\mathrm{stencil}}\\cup\\Phi_{wn}\\cup \\Phi_{wp}\n$$\n6. Result and Comparisons Training setups and time\n6.1. Comparisons against Frame Interpolation Methods 6.2. Comparisons against Frame Extrapolation Methods 6.3. Comparisons against ASW Technology 6.4. Analysis of Runtime Performance and Latency 6.5. Ablation Study Validation of History Encoder\nValidation of occlusion motion vectors\nValidation of the shading-augmented loss\nFailure cases\n","description":"Siggraph Asia 2021 Paper Reading","id":6,"section":"posts","tags":["Rendering"],"title":"ExtraNet: Real-time Extrapolated Rendering for Low-latency Temporal Supersampling","uri":"https://chaphlagical.github.io/en/posts/paperreading/extranet/"},{"content":"I used to use jekyll to build my blog, but it is really slow. Thanks to zzossig and his awesome theme. I finally move my blog to Hugo. And it\u0026rsquo;s really nice looking and convenient. Just keep recording and sharing some interesting knowledge!\n","description":"I move my blog to Hugo!","id":8,"section":"posts","tags":null,"title":"update","uri":"https://chaphlagical.github.io/en/posts/update/"},{"content":"Here I share something interesting!\n","description":"My Blog","id":9,"section":"","tags":null,"title":"About","uri":"https://chaphlagical.github.io/en/about/"}]