<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" 
  xmlns:content="http://purl.org/rss/1.0/modules/content/" 
  xmlns:dc="http://purl.org/dc/elements/1.1/" 
  xmlns:atom="http://www.w3.org/2005/Atom" 
  xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" 
  xmlns:media="http://search.yahoo.com/mrss/">
  <channel>
    <title>paper reading on Chaf&#39;s Blog</title>
    <link>https://chaphlagical.github.io/en/categories/paper-reading/</link>
    <description>Recent content in paper reading on Chaf&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>chaf@mail.ustc.edu.cn (Wenbo Chen)</managingEditor>
    <webMaster>chaf@mail.ustc.edu.cn (Wenbo Chen)</webMaster>
    <copyright>©2022, All Rights Reserved</copyright>
    <lastBuildDate>Sun, 29 Aug 2021 22:00:00 +0100</lastBuildDate>
    
        <atom:link href="https://chaphlagical.github.io/en/categories/paper-reading/index.xml" rel="self" type="application/rss+xml" />
    

      
      <item>
        <title>Low-Cost SPAD Sensing for Non-Line-Of-Sight Tracking, Material Classification and Depth Imaging</title>
        <link>https://chaphlagical.github.io/en/posts/paperreading/low_cost_spad_sensing_for_non_line_of_sight_tracking_material/</link>
        <pubDate>Sun, 29 Aug 2021 22:00:00 +0100</pubDate>
        <author>chaf@mail.ustc.edu.cn (Wenbo Chen)</author>
        <atom:modified>Sun, 29 Aug 2021 22:00:00 +0100</atom:modified>
        <guid>https://chaphlagical.github.io/en/posts/paperreading/low_cost_spad_sensing_for_non_line_of_sight_tracking_material/</guid>
        <description>Authors:
CLARA CALLENBERG, University of Bonn, Germany
ZHENG SHI, Princeton University, USA
FELIX HEIDE, Princeton University, USA
MATTHIAS B. HULLIN, University of Bonn, Germany
Link: https://light.princeton.edu/publication/cheapspad/
1. Background  Time-correlated imaging, or the recording of the optical response of a scene to transient illumination, allows to analyze the temporal dimension of light transport, a feature that is not accessible in pure intensity imaging Time-correlated optical measurements have established themselves as a valuable source of information The approaches available for recording time-correlated measurements are rich and varied, but most require bulky and expensive hardware and are too fragile to be used outside of lab settings A notable exception is the emerging technology of single-photon avalanche diodes (SPADs)  2.</description>
        
        <dc:creator>Wenbo Chen</dc:creator>
        
        
        
        
          
            
              <category>Rendering</category>
            
          
        
        
          
            
              <category>paper reading</category>
            
          
        
        
      </item>
      
      <item>
        <title>Real-Time Global Illumination Decomposition of Videos</title>
        <link>https://chaphlagical.github.io/en/posts/paperreading/real_time_global_illumination_decomposition_of_videos/</link>
        <pubDate>Fri, 27 Aug 2021 21:02:00 +0100</pubDate>
        <author>chaf@mail.ustc.edu.cn (Wenbo Chen)</author>
        <atom:modified>Fri, 27 Aug 2021 21:02:00 +0100</atom:modified>
        <guid>https://chaphlagical.github.io/en/posts/paperreading/real_time_global_illumination_decomposition_of_videos/</guid>
        <description>Author &amp;amp; Institution
ABHIMITRA MEKA*, Max Planck Institute for Informatics, Saarland Informatics Campus and Google
MOHAMMAD SHAFIEI*, Max Planck Institute for Informatics, Saarland Informatics Campus
MICHAEL ZOLLHÖFER, Stanford University
CHRISTIAN RICHARDT, University of Bath
CHRISTIAN THEOBALT, Max Planck Institute for Informatics, Saarland Informatics Campus, Germany
Link: http://gvv.mpi-inf.mpg.de/projects/LiveIlluminationDecomposition/
Abstract  Propose the first approach for the decomposition of a monocular color video into direct and indirect illumination components in real time  In separate layers, the contribution made to the scene appearance by the scene reflectance, the light sources and the reflections from various coherent scene regions to one another Works for regular videos and produces temporally coherent decomposition layers at real-time frame rates Core: several sparsity priors that enable the estimation of the per-pixel direct and indirect illumination layers based on a small set of jointly estimated base reflectance colors The resulting variational decomposition problem uses a new formulation based on sparse and dense sets of non-linear equations that we solve efficiently using a novel alternating data-parallel optimization strategy   Existing techniques that invert global light transport require image capture under multiplexed controlled lighting, or only enable the decomposition of a single image at slow off-line frame rates  1.</description>
        
        <dc:creator>Wenbo Chen</dc:creator>
        
        
        
        
          
            
              <category>Rendering</category>
            
          
        
        
          
            
              <category>paper reading</category>
            
          
        
        
      </item>
      
      <item>
        <title>Neural Light Transport for Relighting and View Synthesis</title>
        <link>https://chaphlagical.github.io/en/posts/paperreading/neural_light_transport_for_relighting_and_view_synthesis/</link>
        <pubDate>Wed, 25 Aug 2021 16:30:00 +0100</pubDate>
        <author>chaf@mail.ustc.edu.cn (Wenbo Chen)</author>
        <atom:modified>Wed, 25 Aug 2021 16:30:00 +0100</atom:modified>
        <guid>https://chaphlagical.github.io/en/posts/paperreading/neural_light_transport_for_relighting_and_view_synthesis/</guid>
        <description>Author &amp;amp; Institution
XIUMING ZHANG, Massachusetts Institute of Technology
SEAN FANELLO and YUN-TA TSAI, Google
TIANCHENG SUN, University of California, San Diego
TIANFAN XUE, ROHIT PANDEY, SERGIO ORTS-ESCOLANO, PHILIP DAVIDSON, CHRISTOPH RHEMANN, PAUL DEBEVEC, and JONATHAN T. BARRON, Google
RAVI RAMAMOORTHI, University of California, San Diego
WILLIAM T. FREEMAN, Massachusetts Institute of Technology &amp;amp; Google
Link: http://nlt.csail.mit.edu/
Abstract   Light transport (LT)
 The light transport (LT) of a scene describes how it appears under different lighting conditions from different viewing directions Complete knowledge of a scene’s LT enables the synthesis of novel views under arbitrary lighting    In this paper</description>
        
        <dc:creator>Wenbo Chen</dc:creator>
        
        
        
        
          
            
              <category>Rendering</category>
            
          
        
        
          
            
              <category>paper reading</category>
            
          
        
        
      </item>
      
      <item>
        <title>Fast Diffraction Pathfinding for Dynamic Sound Propagation</title>
        <link>https://chaphlagical.github.io/en/posts/paperreading/fast_diffraction_pathfinding_for_dynamic_sound_propagation/</link>
        <pubDate>Mon, 23 Aug 2021 22:20:00 +0100</pubDate>
        <author>chaf@mail.ustc.edu.cn (Wenbo Chen)</author>
        <atom:modified>Mon, 23 Aug 2021 22:20:00 +0100</atom:modified>
        <guid>https://chaphlagical.github.io/en/posts/paperreading/fast_diffraction_pathfinding_for_dynamic_sound_propagation/</guid>
        <description>Author: Carl Schissler, Gregor Mückl, Paul Calamia
Institution: Facebook Reality Labs Research, USA
Link: https://dl.acm.org/doi/10.1145/3450626.3459751
Abstract  Diffraction is one the the most perceptually important yet difficult to simulate acoustic effects  A phenomenon that allows sound to propagate around obstructions and corners   A significant bottleneck in real-time simulation of diffraction:  The enumeration of high-order diffraction propagation paths in scenes with complex geometry   The paper present a dynamic geometric diffraction approach that consists of an extensive mesh preprocessing pipeline and complementary runtime algorithm  Preprocessing module identifies a small subset of edges that are important for diffraction using a novel silhouette edge detection heuristic  It also extends these edges with planar diffraction geometry and precomputes a graph data structure encoding the visibility between the edges   The runtime module uses bidirectional path tracing against the diffraction geometry to probabilistically explore potential paths between sources and listeners, then evaluates the intensities for these paths using the Uniform Theory of Diffraction  It uses the edge visibility graph and the A* pathfinding algorithm to robustly and efficiently find additional high-order diffraction paths     The paper demonstrate how this technique can simulate 10th-order diffraction up to 568 times faster than the previous state of the art, and can efficiently handle large scenes with both high geometric complexity and high numbers of sources  1.</description>
        
        <dc:creator>Wenbo Chen</dc:creator>
        
        
        
        
          
            
              <category>Rendering</category>
            
          
        
        
          
            
              <category>paper reading</category>
            
          
        
        
      </item>
      
      <item>
        <title>Temporally Adaptive Shading Reuse for Real-time Rendering and Virtual Reality</title>
        <link>https://chaphlagical.github.io/en/posts/paperreading/temporally_adaptive_shading_reuse_for_real_time_rendering_and_virtual_reality/</link>
        <pubDate>Wed, 18 Aug 2021 15:44:00 +0100</pubDate>
        <author>chaf@mail.ustc.edu.cn (Wenbo Chen)</author>
        <atom:modified>Wed, 18 Aug 2021 15:44:00 +0100</atom:modified>
        <guid>https://chaphlagical.github.io/en/posts/paperreading/temporally_adaptive_shading_reuse_for_real_time_rendering_and_virtual_reality/</guid>
        <description>Author: JOERG H. MUELLER, THOMAS NEFF, PHILIP VOGLREITER, MARKUS STEINBERGER, DIETER SCHMALSTIEG
Institution: Graz University of Technology, Austria
Link: https://dl.acm.org/doi/10.1145/3446790
Abstract Motivation
 Temporal coherence has the potential to enable a huge reduction of shading costs in rendering  Current Work
 Existing techniques focus either only on spatial shading reuse or cannot adaptively choose temporal shading frequencies  Idea
 Temporal shading reuse is possible for extended periods of time for a majority of samples Approximate shading gradients to efficiently determine when and how long shading can be reused  1.</description>
        
        <dc:creator>Wenbo Chen</dc:creator>
        
        
        
        
          
            
              <category>Rendering</category>
            
          
        
        
          
            
              <category>paper reading</category>
            
          
        
        
      </item>
      
      <item>
        <title>GPU Accelerated Path Tracing of Massive Scenes</title>
        <link>https://chaphlagical.github.io/en/posts/paperreading/gpu_accelerated_path_tracing_of_massive_scenes/</link>
        <pubDate>Sun, 15 Aug 2021 21:34:00 +0100</pubDate>
        <author>chaf@mail.ustc.edu.cn (Wenbo Chen)</author>
        <atom:modified>Sun, 15 Aug 2021 21:34:00 +0100</atom:modified>
        <guid>https://chaphlagical.github.io/en/posts/paperreading/gpu_accelerated_path_tracing_of_massive_scenes/</guid>
        <description>Author: MILAN JAROŠ, LUBOMÍR ŘÍHA, PETR STRAKOŠ, and MATĚJ ŠPEŤKO
Institution: IT4Innovations, VSB–Technical University of Ostrava, Czech Republic
Link: https://dl.acm.org/doi/10.1145/3447807
1. Introduction GPUs Rendering vs CPUs Rendering
 Limited memory size Example: Pixar&amp;rsquo;s Coco movie were using up to 120GB, do not fit into the memory of a single GPU  Main contribution
  A solution to GPUs rendering memory size limitation
 Based on replication of a small amount of scene data, between 1% ~ 5%, and well-chosen distribution of the rest of the data into the memory of several GPUs Both replication and distribution of data is based on a memory access pattern analysis of a path tracer during a 1spp prepass The data with the highest number of memory accesses are replicated and the rest is stored only in the memory of the GPU that had the highest number of accesses to it Minimizes the penalty associated with reading data from the remote memory and is effective at providing near-linear scalability    Demonstration that our proposed approach works on a memory management level</description>
        
        <dc:creator>Wenbo Chen</dc:creator>
        
        
        
        
          
            
              <category>Rendering</category>
            
          
        
        
          
            
              <category>paper reading</category>
            
          
        
        
      </item>
      
      <item>
        <title>ExtraNet: Real-time Extrapolated Rendering for Low-latency Temporal Supersampling</title>
        <link>https://chaphlagical.github.io/en/posts/paperreading/extranet/</link>
        <pubDate>Tue, 10 Aug 2021 22:13:11 +0100</pubDate>
        <author>chaf@mail.ustc.edu.cn (Wenbo Chen)</author>
        <atom:modified>Tue, 10 Aug 2021 22:13:11 +0100</atom:modified>
        <guid>https://chaphlagical.github.io/en/posts/paperreading/extranet/</guid>
        <description>Abstract Real-time Rendering
 Frame rate Latency  Spatial Super Sampling: DLSS
 Decreasing the rendering time of each frame by rendering at a lower resolution  Temporal Super Sampling
 Producing more frames on the fly Problems:  It&amp;rsquo;s own computational cost The latency introduced by interpolating frames from the future    ExtraNet
  An efficient neural network that predicts accurate shading results on an extrapolated frame, to minimize both the performance overhead and the latency</description>
        
        <dc:creator>Wenbo Chen</dc:creator>
        
        
        
        
          
            
              <category>Rendering</category>
            
          
        
        
          
            
              <category>paper reading</category>
            
          
        
        
      </item>
      

    
  </channel>
</rss>
