[{"content":"Abstract Real-time Rendering\n Frame rate Latency  Spatial Super Sampling: DLSS\n Decreasing the rendering time of each frame by rendering at a lower resolution  Temporal Super Sampling\n Producing more frames on the fly Problems:  It\u0026rsquo;s own computational cost The latency introduced by interpolating frames from the future    ExtraNet\n  An efficient neural network that predicts accurate shading results on an extrapolated frame, to minimize both the performance overhead and the latency\n  Rendered auxiliary geometry buffers of the extrapolated frame \u0026amp; Temporally reliable motion vectors\n  Train to perform two tasks:\n Irradiance in-painting for regions that cannot find historical correspondences Accurate ghosting-free shading prediction for regions where temporal information is available    A robust hole-marking strategy to automate the classification of these tasks\n  The data generation from a series of high-quality production-ready scenes\n  1. Introduction Some techniques to increase the rendering performance\nLeveraging the temporal coherency between frames: Reduces the sampling rate at each individual frame, and rely on temporal reprojections to combine samples taken over multiple frames to reconstruct a high quality image (Still performing spatial upsampling / reconstruction, with a virtually higher sampling rate acquired temporally to speed up the generation of the current frame of interest)\n TAA (Temporal Anti-Aliasing) TAAU (Temporal Anti-Aliasing Upsample) DLSS (Deep Learning Super Sampling) Ray Tracing Denoising  Limitation of current work\n Frame Interpolation: only when the next frame has been fully rendered, their algorithms can start to work to interpolate one or more frames in between Performance: generating a new frame is already more expensive than a full rendering of it Situation: more information can be used in rendered scene, like G-buffers  Contribution\n A new temporally interleaved real-time rendering and extrapolation architecture that can nearly double the frame rate without introducing additional latency. A novel neural network architecture that leverages G-buffers and temporal motion (computed both from consecutive frames and temporally reliable motion vectors) to perform the extrapolation as a shading prediction task. A lightweight design that incurs low performance overhead (around 8 milliseconds per frame at 720P) but results in high quality and smoothly extrapolated frames.  2. Related work and background 2.1. Temporal Reconstruction  Rely on temporal motion vectors Temporal reconstruction methods reduce the samples taken at each individual frame, and reproject samples over the course of multiple frames using backward motion vectors to reconstruct a high quality image In their paper  Leverage temporal motion vectors to reproject shading of the previous rendered frames to the extrapolated frames But they do not have any samples in the extrapolated frames, unlike temporal reconstruction    Motion Vectors\n$$\n\\pmb p_0^{t}=\\pmb P^t\\pmb M^t\\pmb V^t \\pmb s_0\n$$\n$$\n\\pmb p_0^{t-1}=\\pmb P^{t-1}\\pmb M^{t-1}\\pmb V^{t-1}\\pmb s_0\n$$\nStore screen space motion vectors =\u0026gt; Velocity Buffer\nvertex shader:\n1 2 3 4 5 6 7 8 9 10 11 12  uniform mat4 uModelViewProjectionMat; uniform mat4 uPrevModelViewProjectionMat; smooth out vec4 vPosition; smooth out vec4 vPrevPosition; void main(void) { vPosition = uModelViewProjectionMat * gl_Vertex; vPrevPosition = uPrevModelViewProjectionMat * gl_Vertex; gl_Position = vPosition; }   fragment shader:\n1 2 3 4 5 6 7 8 9 10  smooth in vec4 vPosition; smooth in vec4 vPrevPosition; out vec2 oVelocity; void main(void) { vec2 a = (vPosition.xy / vPosition.w) * 0.5 + 0.5; vec2 b = (vPrevPosition.xy / vPrevPosition.w) * 0.5 + 0.5; oVelocity = a - b; }   2.2. Texture In-painting   Using backward motion vectors to reproject shading from previous frame is that disoccluded regions will not contain any valid information in the previous frame, resulting in \u0026ldquo;holes\u0026rdquo; in the reprojected image\n  Some work:\n Convolution Deep learning Resort to a two-stage prediction strategy to separately predict missing textures in a step-by-step manner    Limitation\n Not designed specifically for real-time rendering Perform the in-painting task completely on single images without temporal information and G-buffers The running time of some methods are in the order of seconds, which are not suitable in our real-time extrapolation task    2.3. Image Warping  Realized by forward scattering or backward gathering, according to their data access patterns Compared with forward mapping, backward mapping is more appealing in real-time rendering due to its high efficiency Although visually plausible contents can be recovered at hole regions after image warping, shadow and highlight movements are almost untouched in these traditional methods, especially those used in the context of frame extrapolation  2.4. Video and Rendering Interpolation  Interpolation based frame rate upsampling is widely used for temporally upsampling videos, using optical flow of the last couple of frames Optical flow is an image based algorithm that often produces inaccurate motion, which will lead to artifacts in the interpolation results Uses accurate motion vectors available from the rendering engine, usually rely on image warping techniques to reuse shading information across frames Deep learning based video interpolation methods are still time-consuming, thus are not suitable for real-time rendering  3. Motivation and Challenge 3.1. Latency  Interpolation =\u0026gt; Produce significant lantency Extrapolation =\u0026gt; doesn\u0026rsquo;t introduce any additional latency  3.2. Challenge Disocclusion\n Backward motion vectors may not always exist  Traditional motion vectors propose to use the same pixel values from previous frames, resulting in ghosting artifacts in the disoccluded regions Occlusion motion vectors alleviates this problem by looking for a nearby similar region in the current frame, then look for the corresponding region from previous frames   Leverage an in-painting network to reshade the occluded regions using occlusion motion vector  Dynamic changes in shading\n Dynamic changes in shading over frames, such as moving shadows, parallax motion from reflections and the colors on rotating objects, may change drastically and are thus unknown in the current frame Always keep the most recently rendered 3 frames for the network to learn to extrapolate the changes  Other challenges\n Extremely low tolerance towards errors and artifacts The inference of the neural network must be (ideally much) faster than the actual rendering process  4. ExtraNet for frame extrapolation Design and train a deep neural network, namely ExtraNet, to predict a new frame from some historical frames and the G-buffers of the extrapolated frame.\n Considering that disoccluded regions of the extrapolated frame lack shading information but reliable and cheap G-buffers are available, we construct an In-painting Network to reshade the disoccluded regions with the help of occlusion motion vectors and those G-buffers In contrast to the general image in-painting task, regions that are not disoccluded may also contain invalid pixels attributing to dynamic shading changes in the scene. To tackle this issue, we design a History Encoder, an additional convolutional neural network, to extract necessary information from historical frames To meet the requirement of high inference speed which is critical to real-time rendering, adopting lightweight gated convolutions in the In-painting Network.  4.1. Problem Formulation  Frame $i$: Denoting the current frame rendered by the graphics engine Frame $i+0.5$: Next frame the network will predict Two historical frames including frame $i-1$ and $i-2$ and the corresponding G-buffers  Pipeline:\n Demodulation: Dividing by the albedo to acquire texture-free irradiance Warp the demodulated frame using both traditional backward motion vectors and occlusion motion vectors The warping operation probably incurs holes in the frame. Using In-painting network to fill the frame Modulation: multiplying by the albedo to re-acquire textured shading Apply regular post-processing(tone mapping ,TAA\u0026hellip;)  4.2. Motion Vectors and Image Warping Instead of computing a zero motion vector in disoccluded regions as the traditional backward motion vector does, occlusion motion vector computes the motion vector in disoccluded regions as the motion vector of the foreground in the previous frame\n4.3. Network Architecture   Similar structure with U-NET\n  Adopt gated convolutions\n$$\n\\pmb M=\\mathrm{Conv}(\\pmb W_m, \\pmb X)\n$$\n$$\n\\pmb F=\\mathrm{Conv}(\\pmb W_f,\\pmb X)\n$$\n$$\n\\pmb O=\\sigma(\\pmb M)\\odot \\pmb F\n$$\n $\\pmb X$: Input feature map $\\pmb W_m$ and $\\pmb W_f$: Two trainable filters $\\odot$ : Element-wise multiplication $\\sigma(\\cdot)$: Sigmoid activation    Gated convolutions increase the inference time\n Resort to a light-weight variant of gated convolution by making $\\pmb M$ a single-channel mask Not use any gated convolution in the upsampling stage  Assume all holes have already been filled in the downsampling stage      Use a residual learning strategy in our pipeline by adding the predicted image of our network with the input image warped by traditional backward motion vectors\n This stabilizes the training process    4.4. History Encoder   Input: Warped images of the current frame $i$ and its two past frames including frame $i-1$ and frame $i-2$\n Warp frames $i-1$ and $i-2$: accumulate the motion vectors Invalid pixels should be marked for these frames    Structure: Nine $3\\times 3$ convolution layers\n Down sample the input Shared by different historical frames    Output: Implicitly encode the high-level representations of shadows and shadings in previous frames, are concatenated together and fed into our In-painting Network\n  Optical flow can be explicitly predicted from several historical frames\n But correct forward optical flow is difficult to acquire in the frame extrapolation pipeline, since only historical frames are available Use backward optical flow as an approximation Not using it    4.5. Loss Function Penalizes pixel-wise error between the predicted frame $\\pmb P$ and ground-truth frame $\\pmb T$\n$$\n\\mathcal L_{l_1}=\\frac{1}{n}\\sum_i |\\pmb P_i - \\pmb T_i|\n$$\nhole-augmented loss: penalize more in the hole regions marked beforehand\n$$\n\\mathcal L_{\\mathrm {hole}}=\\frac{1}{n}\\sum_i |\\pmb P_i-\\pmb T_i|\\odot(1-\\pmb m)\n$$\n $\\pmb m$‚Äã is the binary mask fed into network  The shading-augmented loss: focuses on handling potential shading changes in the predicted frames\n$$\n\\mathcal L_{\\mathrm{shade}}=\\frac{1}{k}\\sum_{i\\in \\Phi_{\\mathrm{top}-k}}|\\pmb P_i-\\pmb T_i|\n$$\n Shading changes: stem from moving shadows due to dynamic lights and specular reflections Considering that shadows and reflections tend to generate large pixel variation among neighboring frames, we select $ùëò$‚Äã pixels with top $ùëò$‚Äã largest errors from the predicted frame and mark these pixels as potential shading change regions $\\Phi_{\\mathrm{top}-k}$ includes the indices of $k$ pixels with top $k$ largest errors  Currently, $k$ set to 10% of total pixels number    Final loss function\n$$\n\\mathcal L=\\mathcal L_{l_1}+\\lambda_{\\mathrm{hole}}\\mathcal L_{\\mathrm{hole}}+\\lambda_{\\mathrm{shade}}\\mathcal L_{\\mathrm{shade}}\n$$\n $\\lambda_{\\mathrm{hole}}$ and $\\lambda_{\\mathrm{shade}}$‚Äã ‚Äãare weights to balance the influence of the losses, here set to 1  4.6. Training detail  PyTorch Mini-batch SGD and Adam optimizer mini-batch size as 8, $\\beta_1 = 0.9$ and $\\beta_2=0.999$ in Adam optimizer default initialization applied HDR image: logarithm transformation $y=\\log(1+x)$ before feeding images into network  5. Dataset 5.1. Scenes and Buffers Each dumped frame comprises 10 buffers which can be divided into three categories:\n Actual shading frame before tone mapping (PreTonemapHDRColor) and albedo (Base color) for demodulation G-buffers used in our network, including scene depth ($ùëë$‚Äã, 1channel), world normal ($\\pmb n_ùë§$‚Äã‚Äã, 3 channels), roughness (1 channel), and metallic (1 channel) Other auxiliary buffers which are necessary for marking holes (invalid pixels) in the warped frame. They are motion vector, world position ($\\pmb p_ùë§$), NoV ($\\pmb n_ùë§\\cdot \\pmb v$, the dot product of world normal $\\pmb n_ùë§$ and view vector $\\pmb v$), and customized stencil ($ùë†$)  5.2. Marking Holes When reusing previous frames to generate a new frame, some pixels in the warped frame will be invalid, due to camera sliding and object moving. These pixels should be marked as holes before feeding into the network\n  For occlusion caused by object moving, used custom stencil to indicate the sharp changes along the edge of the dynamic object. When custom stencil value is different between the current frame ($ùë†$) and the warped frame ($ùë†_ùë§$‚Äã), these pixels will be marked as invalid:\n$$\n\\Phi_{\\mathrm{stencil}}={s_i-s_{w,i}\\neq 0}\n$$\nwhere $ùëñ$ is the pixel index and $\\Phi_{\\mathrm{stencil}}$‚Äã is the set including pixels that are counted as invalid according to stencil value\n  Self occlusions for some dynamic objects when they are moving or the camera is sliding. Resort to world normal since world normal will probably change in these regions. Calculate the cosine value between current frame‚Äôs world normal ($\\pmb n_ùë§$) and warped frame‚Äôs world normal ($\\pmb n_{ùë§ùë§}$), i.e., $\\pmb n_ùë§\\cdot \\pmb n_{ùë§ùë§}$. If this value is large than a predetermined threshold $ùëá_ùëõ$, the corresponding pixel indexed by $ùëñ$ in the warped frame is counted as invalid:\n$$\n\\Phi_{wn}={\\pmb n_{w,i}\\cdot \\pmb n_{ww,i}\u0026gt;T_n}\n$$\nwhere $\\pmb \\Phi _{wn}$‚Äã‚Äã contains invalid pixels marked by differences in world normal\n  Invalid pixels due to camera movement. These pixels can be selected out by world position, considering that static objects‚Äô world positions keep unchanged in a 3D scene. For a given pixel, let $\\pmb p_ùë§$ and $\\pmb p_{ùë§ùë§}$‚Äã‚Äã‚Äã‚Äã be the world position of current frame and warped frame, respectively. We calculate their distance by $|\\pmb p_ùë§ - \\pmb p_{ùë§ùë§}|$. If this distance is larger than a threshold $ùëá_ùëë$ (which is computed by NoV and depth values), we mark this pixel as invalid. Hence, the invalid pixels in this set $\\Phi_{wp}$‚Äã are\n$$\n\\Phi_{wp}={|\\pmb p_{w,i}-\\pmb p_{ww,i}|\u0026gt;T_d}\n$$\n  Finally,\n$$\n\\Phi_{\\mathcal{comb}}=\\Phi_{\\mathrm{stencil}}\\cup\\Phi_{wn}\\cup \\Phi_{wp}\n$$\n6. Result and Comparisons Training setups and time\n6.1. Comparisons against Frame Interpolation Methods 6.2. Comparisons against Frame Extrapolation Methods 6.3. Comparisons against ASW Technology 6.4. Analysis of Runtime Performance and Latency 6.5. Ablation Study Validation of History Encoder\nValidation of occlusion motion vectors\nValidation of the shading-augmented loss\nFailure cases\n","description":"Siggraph Asia 2021 Paper Reading","id":0,"section":"posts","tags":["Rendering"],"title":"ExtraNet: Real-time Extrapolated Rendering for Low-latency Temporal Supersampling","uri":"https://chaphlagical.github.io/en/posts/paperreading/extranet/"},{"content":"I used to use jekyll to build my blog, but it is really slow. Thanks to zzossig and his awesome theme. I finally move my blog to Hugo. And it\u0026rsquo;s really nice looking and convenient. Just keep recording and sharing some interesting knowledge!\n","description":"I move my blog to Hugo!","id":2,"section":"posts","tags":null,"title":"update","uri":"https://chaphlagical.github.io/en/posts/update/"},{"content":"Here I share something interesting!\n","description":"My Blog","id":3,"section":"","tags":null,"title":"About","uri":"https://chaphlagical.github.io/en/about/"}]