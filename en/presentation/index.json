[{"content":"Author: MILAN JARO≈†, LUBOM√çR ≈ò√çHA, PETR STRAKO≈†, and MATƒöJ ≈†PE≈§KO\nInstitution: IT4Innovations, VSB‚ÄìTechnical University of Ostrava, Czech Republic\n1. Introduction GPUs Rendering vs CPUs Rendering\n Limited memory size Example: Pixar\u0026rsquo;s Coco movie were using up to 120GB, do not fit into the memory of a single GPU  Main contribution\n  A solution to GPUs rendering memory size limitation\n Based on replication of a small amount of scene data, between 1% ~ 5%, and well-chosen distribution of the rest of the data into the memory of several GPUs Both replication and distribution of data is based on a memory access pattern analysis of a path tracer during a 1spp prepass The data with the highest number of memory accesses are replicated and the rest is stored only in the memory of the GPU that had the highest number of accesses to it Minimizes the penalty associated with reading data from the remote memory and is effective at providing near-linear scalability    Demonstration that our proposed approach works on a memory management level\n Any path tracing code that supports GPU acceleration using CUDA can adopt our approach without redesigning its internal data structures    Two key technologies\n NVLink GPU Interconnect  Enables multiple GPUs to efficiently share the content of their memories due to its high bandwidth and low latency   CUDA Unified Memory(UM)  Provides programmers with control over data placement across the memories of interconnected GPUs    2. Related Work 2.1. Out-of-core Rendering  Out of core rendering is the capability to render (with GPUs) scenes requiring more memory than the one directly connected to the device It will use the system\u0026rsquo;s memory instead  2.2. Distributed Rendering  sort first: Image based partitioning sort middle: Related to rasterization only sort last: Use scene data distribution  2.2.1. Image-Parallel Rendering  Distribute among processors or machines per blocks of pixels of a rendered image Most common and efficient way the scene data is fully replicated in all local memories and ray tracing is embarrassingly parallel In the case of ray tracing complex scenes that do not fit into local memory, this approach results in on-demand scene data movement while rays remains fixed (Moving scene data instead of ray data) Our proposed solution is based on scene data communication while rays never leave the GPU they are created on  2.2.2. Data-Parallel Rendering  Distribute the workload by subdividing the scene data In the case of distributed ray tracing, these approaches transfer ray data among processors or machines, while scene data do not move after the initial distribution (Moving ray data instead of scene data)  2.3. Distributed Shared Memory Systems  Shared Memory Processors (SMP) \u0026amp; Distributed Shared Memory (DSM)  Local memory with caches Hardware or software layer transparently creates an illusion of global shared memory for applications   The latency to access remote data is considerably larger than the latency to access local data  good data locality is therefore critical for high performance    2.4. CUDA Unified Memory For Multi-GPU Systems  UM Manages communication between multiple GPUs and CPUs transparently by adopting DSM techniques UM simplifies both out-of-core processing between GPUs and CPUs as well as multi-GPU processing and combinations of both NVLink interconnect is the key enabler of DSM multi-GPU system  3. Blender Cycles Path Tracer  An unbiased renderer based on unidirectional path tracing that supports CPU and GPU rendering For acceleration it uses a Bounding Volume Hierarchy (BVH) Supports CUDA, Optix, and OpenCL  3.1. Extensions for Multi-GPU Support Workflow:\n distribute the data structures evenly among all GPUs run the kernel with memory access counters and get the memory access statistics redistribute the data structures among GPUs based on memory access statistics run the original path-tracing kernel with redistributed data  3.2. Multi-GPU Benchmark Systems   BullSequana X410-E5 NVLink-V blade server\n with 4 Tesla V100 GPUs, each with 16 GB of memory and direct NVLink interconnect    NVidia DGX-2\n able to process massive scenes of sizes up to 512 GB in the shared memory of its 16 Tesla V100 GPUs, each with 32 GB of memory The uniqueness of this platform is the enhancement of the NVLink interconnect by using NVSwitches, which enable the connection of all 16 GPUs and higher bandwidth    3.3. Benchmark Scenes 4. Data Distributed Multi-GPU Path Tracing 4.1. Basic Distribution of Entire Data Structures 4.1.1. Memory Access Analysis  Define the order in which data structures are replicated as a ratio of the total memory accesses to a particular data structure over its size The analysis was done on the first sample when rendering the scenes with a resolution of 5,120x2,560 pixels   small_structures: a set of data structures smaller than 16MB (the most important one is svm_nodes, which stores Shader Virtual Machine (SVM) data and codes) bvh_node: stores the BVH tree without its leaves (leaves are stored in a separate structure) prim_tri_verts: holds coordinates of all vertices in the scene prim_tri_index: a set of all triangles in the scene and it contains indices to the prim_tri_verts  We can see that:\n The most important data structure is bvh_nodes, because it is responsible for 79.6% of all memory accesses  If it is replicated in the memory of all GPUs, the 79.6% of all memory accesses will be to the local memory The size of this structure is 7.2 GB, which represents 26.5% of the entire scene size   If in addition to small_structures and bvh_nodes, prim_tri_index and prim_tri_verts are also replicated, then the relative rendering time is only 109% while 40.7% of the scene is replicated and the rest is distributed  4.1.2. Performance and Scalability Evaluation The scalability of the proposed approach is evaluated for four different cases:\n all data structures are replicated‚Äîthis case serves as a baseline as it achieves the best performance and scalability all data structures are evenly distributed  continuous distribution: the structures are divided into large chunks of a size equal to the structure size over a number of GPUs, and each GPU owns one chunk round robin distribution: the distributed structure is divided into chunks of 2 MB, which are distributed in a round robin fashion   small structures and bvh_nodes are replicated while all other data structures are distributed small structures, bvh_nodes, prim_tri_index, and prim_tri_verts are replicated while all other data structures are distributed  Result:\n round robin distribution of small chunks performs better than continuous distribution of large chunks, therefore it is always used to distribute non-replicated data structures path tracing with fully distributed data structures does not scale on both platforms (there is reasonable scalability for two GPUs on DGX-2, but not beyond that) if small structures and bvh_nodes are replicated, the scalability is significantly improved if small structures, bvh_nodes, prim_tri_index, and prim_tri_verts are replicated, the scalability is further improved  4.2. Advanced Distribution Based on Memory Access Pattern and Statistics   The data placement is done with chunks, and hints are set for each chunk individually\n  The optimal chunk size was identified experimentally by benchmarking the path tracer performance for chunks of sizes from 64 kB to 128 MB:\n for scenes smaller than 30 GB the optimal chunk size is 2 MB (smaller chunks are not recommended) for scenes of sizes around 40 GB the optimal chunk size is 16 MB for scenes of sizes above 120 GB the optimal chunk size is 64 MB    The workflow of this data placement strategy:\n copy/distribute every data structure across all GPUs in a round robin fashion using chunks of an optimal size run the path tracing kernel with memory access counters for 1spp to measure the statistics gather the statistics on the CPU and run the proposed algorithm to get the optimal data chunks distribution use cudaMemAdvise to migrate or replicate all chunks run the original unmodified path tracing kernel    4.2.1. Memory Access Pattern Analysis   To identify the memory access pattern, per chunk access counters have been implemented in the GPU path tracing kernel of Cycles\n Independent counters for all data structures and all their chunks A total number of memory accesses per chunk can be recorded for each GPU    The memory analysis starts with all data structures being evenly distributed using a round robin distribution\n The modified path tracing kernel with memory access counters is executed on all GPUs for one sample    When the kernel finishes, then for every chunk of every structure, a number of accesses from all GPUs is recorded\n The workload is distributed among GPUs by horizontal stripes so that each GPU works on one stripe     1% of scene data covers between 56.7% and 74.4% of memory accesses, depending on the scene size This analysis shows that there are clear candidates among the chunks that should be replicated on all GPUs, while a major portion of the data is accessed infrequently and can be distributed with an acceptable impact on performance.  4.2.2. Data Placement Algorithm Based on Memory Access Pattern   The per GPU counters are summed to get the total number of accesses $a_{sum}$ for each chunk $c\\in C$ of each data structure $s\\in S$ and $a(a_{sum},s,c)$ tuple is created\n  All tuples are put into a single 1D array $H_{comb}$. The array is sorted by $a_{sum}$ from largest value to the smallest one and stored in $H_{\u0026lt;}$ array\n  The last input of the algorithm is the number of chunks that can be replicated $N_{dup}$. This value can either be set manually or automatically using formula:\n$$\nN_{dup}=\\dfrac{1}{C_s}\\Big(G_f-\\frac{S_s}{N_g}\\Big)\n$$\n $G_f$: the amount of free memory per GPU in MB available to store scene data $S_s$: scene size in MB $N_g$: total number of GPUs $C_s$: the chunk size in MB    Define a threshold $t$ as the $N_{dup}$-th element in the sorted array $H_\u0026lt;$ and evaluate all tuples in the array $H_\u0026lt;$. If the counter value $a_{sum}$ is larger than $t$, then the corresponding chunk will be set as *SetReadMostly*, and therefore replicated\n In the opposite case, the chunk is set as SetPreferredLocation and is assigned to the GPU with the highest number of accesses to this chunk  If the memory of this GPU is full, then the GPU with second, third, fourth, and so on, highest number of accesses is selected until a GPU with free memory is found.   It the counter value is equal to zero (without any accesses), then the corresponding chunk will be distributed in a round robin fashion across GPUs with free memory    4.2.3. Performance Evaluation  The performance of the proposed algorithm was evaluated for different ratios between replicated and distributed data at a 2 MB chunk level of granularity for the Moana 12 and 27 GB scenes   Figure shows the path tracing performance for one sample per pixel and 5,120x2,560 pixel resolution for both scenes and platforms, and for all available GPUs Once at least 1% of chunks are replicated, the performance is almost identical  4.2.4. Maximum Scene Size Analysis The following equation describes the maximum ratio of replicated data that fits into the memory of GPU memory for a scene of a given size:\n$$\nN_{max_dup}=\\frac{\\Big(G_f-\\frac{S_s}{N_g}\\Big)}{\\Big(S_s-\\frac{S_s}{N_g}\\Big)}\n$$\n $G_f$: the amount of free memory per GPU in MB available to store the scene data $S_s$: the scene size in MB $N_g$: total number of GPUs  5. Performance for massive scenes Group 1: Moana 38 GB, Museum 41 GB, Agent 37 GB, and Spring 41 GB are designed to stress the Barbora GPU server with 64 GB of total GPU memory\n the performance of the Barbora server is almost identical to the performance of DGX-2 for the same amount of scene replication (up to 10%) DGX-2 is able to further replicate scene data up to 60%, which improves performance by 2.8% only in the case of the Moana 38 GB scene (for the other scenes the performance is higher by only less than 1%) This means that for scenes of sizes approximately up to 45 GB distributed over 4 GPUs, the significantly less complex and cheaper GPU interconnect in the Barbora server is sufficient For the Museum, Agent, and Spring scenes 2% of scene replication attains optimal performance. This holds for 4, 8, and 16 GPUs Only the Moana scene needs higher amounts of replicated data, up to 25% for 16 GPUs Scalability can be evaluated on DGX-2 for 4, 8, and 16 GPUs only. For the Moana, Museum, Agent, and Spring scenes, for 5% scene replication, the parallel efficiencies, going from 4 to 16 GPUs, are 82.7%, 97.1%, 97.9%, and 98.1%, respectively. In the case of the Moana scene, a higher replication ratio is needed to improve scalability, e.g., for 25% data replication ratio the parallel efficiency is 94.4%  Group 2: Moana 169 GB, Museum 124 GB, Agent 167 GB, and Spring 137 GB are designed to stress the DGX-2 server with 512 GB of total GPU memory\n The performance is affected by selecting the right chunk size, particularly for the Moana scene For 8 GPUs and the Moana, Agent, and Spring scenes, if the replication ratio is 10% the scene does not fit into GPU shared memory anymore and chunks are swapped between GPU and CPU memory (the default behavior of the CUDA Unified Memory), which makes the rendering several times slower depending on the number of chunks being moved back and forth. This is the point at which our approach stops working, and therefore it is crucial to correctly select the replication ratio to avoid this situation For the Agent scene 1% of scene replication gives optimal performance for both 8 and 16 GPUs. The Spring scenes needs only 0.1% for 8 GPUs and 0.25% for 16 GPUs. The Museum scene needs 1% for 8 GPUs and 2% for 16 GPUs. Finally, the Moana scene requires 2% for 8 GPUs and 5% for 16 GPUs Scalability between 8 and 16 GPUs is good for all scenes. The parallel efficiencies are 93.8%, 98.8%, 98.6%, and 99.0% for the Moana, Museum, Agent, and Spring scenes, respectively   Analyze memory access pattern only for one sample per pixel Rendering times grows linearly with the number of samples   Number of bounces influence  6. Conclusions Contribution\n Presented a solution for path tracing of massive scenes on multiple GPUs Analyzes the memory access pattern of a path tracer and defines how the scene data should be distributed across GPUs with a minimal loss of performance Those parts of the scene data that have the highest memory access rate are replicated on all GPUs, because their distribution would have a major negative impact on performance  Methods\n Uses the same memory management rules for the entire data structure Splits the data structures into chunks and we control the placement and/or replication of each chunk separately  Feature\n Only control the memory allocations, the path tracer data structures do not have to be redesigned Take full advantage of NVLink 2.0 interconnect and its high bandwidth and low latency  ","description":"Siggraph 2021 Paper Reading","id":0,"section":"posts","tags":["Rendering"],"title":"GPU Accelerated Path Tracing of Massive Scenes","uri":"https://chaphlagical.github.io/en/posts/paperreading/gpu_accelerated_path_tracing_of_massive_scenes/"},{"content":"Abstract Real-time Rendering\n Frame rate Latency  Spatial Super Sampling: DLSS\n Decreasing the rendering time of each frame by rendering at a lower resolution  Temporal Super Sampling\n Producing more frames on the fly Problems:  It\u0026rsquo;s own computational cost The latency introduced by interpolating frames from the future    ExtraNet\n  An efficient neural network that predicts accurate shading results on an extrapolated frame, to minimize both the performance overhead and the latency\n  Rendered auxiliary geometry buffers of the extrapolated frame \u0026amp; Temporally reliable motion vectors\n  Train to perform two tasks:\n Irradiance in-painting for regions that cannot find historical correspondences Accurate ghosting-free shading prediction for regions where temporal information is available    A robust hole-marking strategy to automate the classification of these tasks\n  The data generation from a series of high-quality production-ready scenes\n  1. Introduction Some techniques to increase the rendering performance\nLeveraging the temporal coherency between frames: Reduces the sampling rate at each individual frame, and rely on temporal reprojections to combine samples taken over multiple frames to reconstruct a high quality image (Still performing spatial upsampling / reconstruction, with a virtually higher sampling rate acquired temporally to speed up the generation of the current frame of interest)\n TAA (Temporal Anti-Aliasing) TAAU (Temporal Anti-Aliasing Upsample) DLSS (Deep Learning Super Sampling) Ray Tracing Denoising  Limitation of current work\n Frame Interpolation: only when the next frame has been fully rendered, their algorithms can start to work to interpolate one or more frames in between Performance: generating a new frame is already more expensive than a full rendering of it Situation: more information can be used in rendered scene, like G-buffers  Contribution\n A new temporally interleaved real-time rendering and extrapolation architecture that can nearly double the frame rate without introducing additional latency. A novel neural network architecture that leverages G-buffers and temporal motion (computed both from consecutive frames and temporally reliable motion vectors) to perform the extrapolation as a shading prediction task. A lightweight design that incurs low performance overhead (around 8 milliseconds per frame at 720P) but results in high quality and smoothly extrapolated frames.  2. Related work and background 2.1. Temporal Reconstruction  Rely on temporal motion vectors Temporal reconstruction methods reduce the samples taken at each individual frame, and reproject samples over the course of multiple frames using backward motion vectors to reconstruct a high quality image In their paper  Leverage temporal motion vectors to reproject shading of the previous rendered frames to the extrapolated frames But they do not have any samples in the extrapolated frames, unlike temporal reconstruction    Motion Vectors\n$$\n\\pmb p_0^{t}=\\pmb P^t\\pmb M^t\\pmb V^t \\pmb s_0\n$$\n$$\n\\pmb p_0^{t-1}=\\pmb P^{t-1}\\pmb M^{t-1}\\pmb V^{t-1}\\pmb s_0\n$$\nStore screen space motion vectors =\u0026gt; Velocity Buffer\nvertex shader:\n1 2 3 4 5 6 7 8 9 10 11 12  uniform mat4 uModelViewProjectionMat; uniform mat4 uPrevModelViewProjectionMat; smooth out vec4 vPosition; smooth out vec4 vPrevPosition; void main(void) { vPosition = uModelViewProjectionMat * gl_Vertex; vPrevPosition = uPrevModelViewProjectionMat * gl_Vertex; gl_Position = vPosition; }   fragment shader:\n1 2 3 4 5 6 7 8 9 10  smooth in vec4 vPosition; smooth in vec4 vPrevPosition; out vec2 oVelocity; void main(void) { vec2 a = (vPosition.xy / vPosition.w) * 0.5 + 0.5; vec2 b = (vPrevPosition.xy / vPrevPosition.w) * 0.5 + 0.5; oVelocity = a - b; }   2.2. Texture In-painting   Using backward motion vectors to reproject shading from previous frame is that disoccluded regions will not contain any valid information in the previous frame, resulting in \u0026ldquo;holes\u0026rdquo; in the reprojected image\n  Some work:\n Convolution Deep learning Resort to a two-stage prediction strategy to separately predict missing textures in a step-by-step manner    Limitation\n Not designed specifically for real-time rendering Perform the in-painting task completely on single images without temporal information and G-buffers The running time of some methods are in the order of seconds, which are not suitable in our real-time extrapolation task    2.3. Image Warping  Realized by forward scattering or backward gathering, according to their data access patterns Compared with forward mapping, backward mapping is more appealing in real-time rendering due to its high efficiency Although visually plausible contents can be recovered at hole regions after image warping, shadow and highlight movements are almost untouched in these traditional methods, especially those used in the context of frame extrapolation  2.4. Video and Rendering Interpolation  Interpolation based frame rate upsampling is widely used for temporally upsampling videos, using optical flow of the last couple of frames Optical flow is an image based algorithm that often produces inaccurate motion, which will lead to artifacts in the interpolation results Uses accurate motion vectors available from the rendering engine, usually rely on image warping techniques to reuse shading information across frames Deep learning based video interpolation methods are still time-consuming, thus are not suitable for real-time rendering  3. Motivation and Challenge 3.1. Latency  Interpolation =\u0026gt; Produce significant lantency Extrapolation =\u0026gt; doesn\u0026rsquo;t introduce any additional latency  3.2. Challenge Disocclusion\n Backward motion vectors may not always exist  Traditional motion vectors propose to use the same pixel values from previous frames, resulting in ghosting artifacts in the disoccluded regions Occlusion motion vectors alleviates this problem by looking for a nearby similar region in the current frame, then look for the corresponding region from previous frames   Leverage an in-painting network to reshade the occluded regions using occlusion motion vector  Dynamic changes in shading\n Dynamic changes in shading over frames, such as moving shadows, parallax motion from reflections and the colors on rotating objects, may change drastically and are thus unknown in the current frame Always keep the most recently rendered 3 frames for the network to learn to extrapolate the changes  Other challenges\n Extremely low tolerance towards errors and artifacts The inference of the neural network must be (ideally much) faster than the actual rendering process  4. ExtraNet for frame extrapolation Design and train a deep neural network, namely ExtraNet, to predict a new frame from some historical frames and the G-buffers of the extrapolated frame.\n Considering that disoccluded regions of the extrapolated frame lack shading information but reliable and cheap G-buffers are available, we construct an In-painting Network to reshade the disoccluded regions with the help of occlusion motion vectors and those G-buffers In contrast to the general image in-painting task, regions that are not disoccluded may also contain invalid pixels attributing to dynamic shading changes in the scene. To tackle this issue, we design a History Encoder, an additional convolutional neural network, to extract necessary information from historical frames To meet the requirement of high inference speed which is critical to real-time rendering, adopting lightweight gated convolutions in the In-painting Network.  4.1. Problem Formulation  Frame $i$: Denoting the current frame rendered by the graphics engine Frame $i+0.5$: Next frame the network will predict Two historical frames including frame $i-1$ and $i-2$ and the corresponding G-buffers  Pipeline:\n Demodulation: Dividing by the albedo to acquire texture-free irradiance Warp the demodulated frame using both traditional backward motion vectors and occlusion motion vectors The warping operation probably incurs holes in the frame. Using In-painting network to fill the frame Modulation: multiplying by the albedo to re-acquire textured shading Apply regular post-processing(tone mapping ,TAA\u0026hellip;)  4.2. Motion Vectors and Image Warping Instead of computing a zero motion vector in disoccluded regions as the traditional backward motion vector does, occlusion motion vector computes the motion vector in disoccluded regions as the motion vector of the foreground in the previous frame\n4.3. Network Architecture   Similar structure with U-NET\n  Adopt gated convolutions\n$$\n\\pmb M=\\mathrm{Conv}(\\pmb W_m, \\pmb X)\n$$\n$$\n\\pmb F=\\mathrm{Conv}(\\pmb W_f,\\pmb X)\n$$\n$$\n\\pmb O=\\sigma(\\pmb M)\\odot \\pmb F\n$$\n $\\pmb X$: Input feature map $\\pmb W_m$ and $\\pmb W_f$: Two trainable filters $\\odot$ : Element-wise multiplication $\\sigma(\\cdot)$: Sigmoid activation    Gated convolutions increase the inference time\n Resort to a light-weight variant of gated convolution by making $\\pmb M$ a single-channel mask Not use any gated convolution in the upsampling stage  Assume all holes have already been filled in the downsampling stage      Use a residual learning strategy in our pipeline by adding the predicted image of our network with the input image warped by traditional backward motion vectors\n This stabilizes the training process    4.4. History Encoder   Input: Warped images of the current frame $i$ and its two past frames including frame $i-1$ and frame $i-2$\n Warp frames $i-1$ and $i-2$: accumulate the motion vectors Invalid pixels should be marked for these frames    Structure: Nine $3\\times 3$ convolution layers\n Down sample the input Shared by different historical frames    Output: Implicitly encode the high-level representations of shadows and shadings in previous frames, are concatenated together and fed into our In-painting Network\n  Optical flow can be explicitly predicted from several historical frames\n But correct forward optical flow is difficult to acquire in the frame extrapolation pipeline, since only historical frames are available Use backward optical flow as an approximation Not using it    4.5. Loss Function Penalizes pixel-wise error between the predicted frame $\\pmb P$ and ground-truth frame $\\pmb T$\n$$\n\\mathcal L_{l_1}=\\frac{1}{n}\\sum_i |\\pmb P_i - \\pmb T_i|\n$$\nhole-augmented loss: penalize more in the hole regions marked beforehand\n$$\n\\mathcal L_{\\mathrm {hole}}=\\frac{1}{n}\\sum_i |\\pmb P_i-\\pmb T_i|\\odot(1-\\pmb m)\n$$\n $\\pmb m$‚Äã is the binary mask fed into network  The shading-augmented loss: focuses on handling potential shading changes in the predicted frames\n$$\n\\mathcal L_{\\mathrm{shade}}=\\frac{1}{k}\\sum_{i\\in \\Phi_{\\mathrm{top}-k}}|\\pmb P_i-\\pmb T_i|\n$$\n Shading changes: stem from moving shadows due to dynamic lights and specular reflections Considering that shadows and reflections tend to generate large pixel variation among neighboring frames, we select $ùëò$‚Äã pixels with top $ùëò$‚Äã largest errors from the predicted frame and mark these pixels as potential shading change regions $\\Phi_{\\mathrm{top}-k}$ includes the indices of $k$ pixels with top $k$ largest errors  Currently, $k$ set to 10% of total pixels number    Final loss function\n$$\n\\mathcal L=\\mathcal L_{l_1}+\\lambda_{\\mathrm{hole}}\\mathcal L_{\\mathrm{hole}}+\\lambda_{\\mathrm{shade}}\\mathcal L_{\\mathrm{shade}}\n$$\n $\\lambda_{\\mathrm{hole}}$ and $\\lambda_{\\mathrm{shade}}$‚Äã ‚Äãare weights to balance the influence of the losses, here set to 1  4.6. Training detail  PyTorch Mini-batch SGD and Adam optimizer mini-batch size as 8, $\\beta_1 = 0.9$ and $\\beta_2=0.999$ in Adam optimizer default initialization applied HDR image: logarithm transformation $y=\\log(1+x)$ before feeding images into network  5. Dataset 5.1. Scenes and Buffers Each dumped frame comprises 10 buffers which can be divided into three categories:\n Actual shading frame before tone mapping (PreTonemapHDRColor) and albedo (Base color) for demodulation G-buffers used in our network, including scene depth ($ùëë$‚Äã, 1channel), world normal ($\\pmb n_ùë§$‚Äã‚Äã, 3 channels), roughness (1 channel), and metallic (1 channel) Other auxiliary buffers which are necessary for marking holes (invalid pixels) in the warped frame. They are motion vector, world position ($\\pmb p_ùë§$), NoV ($\\pmb n_ùë§\\cdot \\pmb v$, the dot product of world normal $\\pmb n_ùë§$ and view vector $\\pmb v$), and customized stencil ($ùë†$)  5.2. Marking Holes When reusing previous frames to generate a new frame, some pixels in the warped frame will be invalid, due to camera sliding and object moving. These pixels should be marked as holes before feeding into the network\n  For occlusion caused by object moving, used custom stencil to indicate the sharp changes along the edge of the dynamic object. When custom stencil value is different between the current frame ($ùë†$) and the warped frame ($ùë†_ùë§$‚Äã), these pixels will be marked as invalid:\n$$\n\\Phi_{\\mathrm{stencil}}={s_i-s_{w,i}\\neq 0}\n$$\nwhere $ùëñ$ is the pixel index and $\\Phi_{\\mathrm{stencil}}$‚Äã is the set including pixels that are counted as invalid according to stencil value\n  Self occlusions for some dynamic objects when they are moving or the camera is sliding. Resort to world normal since world normal will probably change in these regions. Calculate the cosine value between current frame‚Äôs world normal ($\\pmb n_ùë§$) and warped frame‚Äôs world normal ($\\pmb n_{ùë§ùë§}$), i.e., $\\pmb n_ùë§\\cdot \\pmb n_{ùë§ùë§}$. If this value is large than a predetermined threshold $ùëá_ùëõ$, the corresponding pixel indexed by $ùëñ$ in the warped frame is counted as invalid:\n$$\n\\Phi_{wn}={\\pmb n_{w,i}\\cdot \\pmb n_{ww,i}\u0026gt;T_n}\n$$\nwhere $\\pmb \\Phi _{wn}$‚Äã‚Äã contains invalid pixels marked by differences in world normal\n  Invalid pixels due to camera movement. These pixels can be selected out by world position, considering that static objects‚Äô world positions keep unchanged in a 3D scene. For a given pixel, let $\\pmb p_ùë§$ and $\\pmb p_{ùë§ùë§}$‚Äã‚Äã‚Äã‚Äã be the world position of current frame and warped frame, respectively. We calculate their distance by $|\\pmb p_ùë§ - \\pmb p_{ùë§ùë§}|$. If this distance is larger than a threshold $ùëá_ùëë$ (which is computed by NoV and depth values), we mark this pixel as invalid. Hence, the invalid pixels in this set $\\Phi_{wp}$‚Äã are\n$$\n\\Phi_{wp}={|\\pmb p_{w,i}-\\pmb p_{ww,i}|\u0026gt;T_d}\n$$\n  Finally,\n$$\n\\Phi_{\\mathcal{comb}}=\\Phi_{\\mathrm{stencil}}\\cup\\Phi_{wn}\\cup \\Phi_{wp}\n$$\n6. Result and Comparisons Training setups and time\n6.1. Comparisons against Frame Interpolation Methods 6.2. Comparisons against Frame Extrapolation Methods 6.3. Comparisons against ASW Technology 6.4. Analysis of Runtime Performance and Latency 6.5. Ablation Study Validation of History Encoder\nValidation of occlusion motion vectors\nValidation of the shading-augmented loss\nFailure cases\n","description":"Siggraph Asia 2021 Paper Reading","id":1,"section":"posts","tags":["Rendering"],"title":"ExtraNet: Real-time Extrapolated Rendering for Low-latency Temporal Supersampling","uri":"https://chaphlagical.github.io/en/posts/paperreading/extranet/"},{"content":"I used to use jekyll to build my blog, but it is really slow. Thanks to zzossig and his awesome theme. I finally move my blog to Hugo. And it\u0026rsquo;s really nice looking and convenient. Just keep recording and sharing some interesting knowledge!\n","description":"I move my blog to Hugo!","id":3,"section":"posts","tags":null,"title":"update","uri":"https://chaphlagical.github.io/en/posts/update/"},{"content":"Here I share something interesting!\n","description":"My Blog","id":4,"section":"","tags":null,"title":"About","uri":"https://chaphlagical.github.io/en/about/"}]