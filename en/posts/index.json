[{"content":"Author: JOERG H. MUELLER, THOMAS NEFF, PHILIP VOGLREITER, MARKUS STEINBERGER, DIETER SCHMALSTIEG\nInstitution: Graz University of Technology, Austria\nLink: https://dl.acm.org/doi/10.1145/3446790\nAbstract Motivation\n Temporal coherence has the potential to enable a huge reduction of shading costs in rendering  Current Work\n Existing techniques focus either only on spatial shading reuse or cannot adaptively choose temporal shading frequencies  Idea\n Temporal shading reuse is possible for extended periods of time for a majority of samples Approximate shading gradients to efficiently determine when and how long shading can be reused  1. Introduction   An important strategy to reduce shading load is to exploit spatial and temporal coherence\n Spatial coherence  Checkerboard rendering, Foveated rendering Variable rate shading Motion-adaptive shading \u0026hellip;   Temporal coherence  Temporal anti-aliasing      Four unresolved research questions for shading reuse over longer periods of time poses:\n How do temporal artifacts affect the perceived image quality when reusing shading samples over time?  Perception of shading differences Conducting a controlled user study to determine the perceived effect of shading artifacts due to temporal shading reuse for scenes with advanced shading and animations   What are the limits of shading reuse in scenes with advanced shading and animation?  Temporal coherence Analyzing the potential amount of coherence in shading and visibility over time   How can we determine ahead of time when shading samples become invalid without actually reshading the samples?  Gradients Analyzing analytical and numerical first-order approximations of temporal shading gradients and their ability to predict the magnitude of future shading changes Analyzing the spatial variation of the temporal shading gradient and show how to incorporate spatial information to better predict future shading changes   Given sufficient temporally coherent shading samples, how can they efficiently be reused in practice?  Framework A general-purpose framework for predicting shading changes and temporally reusing shading over time      2. Related Work   Reduce shader invocations by exploiting shading coherence\n Foveated rendering Variable rate shading    Temporal coherence is commonly exploited by using information from previous frames for spatio-temporal filtering\n Temporal anti-aliasing  Use exponential-decay history buffers for filtering Use the temporal variation of sampling position to achieve spatial anti-aliasing      Shading gradients can be used to estimate the variation of shading and are thus often used in spatio-temporal filtering\n Guiding spatio-temporal upsampling filters Denoising filters Reconstruction in adaptive frameless rendering Spatial sampling    Temporal upsampling methods reuse previous shading result without filtering or accumulation\n Warps the image plane of the previous, fully rendered keyframe based on the latest headtracking update Advanced warping and reprojection techniques  Render cache Reverse reprojection caching  Use scene depth or motion vectors for dense 3D warping while reusing the shading from the last keyframe     Temporal methods are based on the assumption that the temporal variation in shading is slow and as spatial reprojection errors accumulate over time, a frequent refresh of the cache is required A fraction of samples typically violate this assumption and thus lead to perceivable artifacts, when not shaded more often    To avoid spatial reprojection errors while reusing shading samples multiple times, shading can be generated in alternative spaces and resampled for display\n Example: Generation of depth of field and motion blur  For efficiency reason, shading in alternative spaces requires GPU extensions   Similarly, the shading cache has been designed to allow for spatio-temporal shading reuse in path tracing Texture-space shading methods have been popularized and have even been used for temporal upsampling on a VR client Cons: All these methods only allow for fixed temporal upsampling rate    Numerous image quality metrics try to model the human perception of images\n peak signal-to-noise ratio (PSNR)  Provides an objective and easy to compute metric Failed to capture human perception   structural similarity index measure(SSIM)  Designed to more closely resemble perception   IW-SSIM  Made to SSIM to enhance the predictions of the metric   HDR-VDP-2  Try to model the visual system to some extent Adapt the metric for the evaluation of foveated rendering   VMAF  Combination of different metrics Especially useful for video game content   FILP  Derived from the manual method of comparing images by alternating between them and provides an error map showing where differences would be perceived between the two images in comparison    Major disadvantage above all: only compare images, disregarding any temporal artifacts, such as flickering\n  3. Perception of Shading Differences   Conducted a controlled user experiment\n  To determine the limits of keeping shading over multiple frames in scenes with advanced shading and animation\n  34 participants were shown two video clips, one generated with forward rendering as ground truth reference, and the other by reusing shading from previous frames and the participants were asked to rate the relative quality of the two video clips, following a pairwise comparison design\n As the order of clips was randomized, they did not know which clip was the reference.    From the rating, we compute an average relative quality score ($Q$​​), ranging from -2 to +2, where +2 means the reference is significantly better and +1 slightly better. 0 indicates that they have been rated equal\n  Compute the probability $p_{ref}$​ of choosing the reference over the reuse approach\n A $p_{ref}$ of 50% indicates that there is no difference between the approaches $p_{ref}$ of 75% is referred to as 1 just-noticeable-difference (JND) unit Staying under 1 JND is considered high quality    For statistical analysis, use repeated-measures ANOVA and Bonferroni adjustment for post hoc tests\n  Test scenes:\n Physically-based materials Animated models Animated light sources Dynamic shadows      Temporal forward rendering (TFR)\n To determine the perceived quality reduction caused by reusing shading samples TFR decouples the temporal changes in shading from other major effects that influence the shading reuse ability  Temporal changes in visibility and spatial sampling   Use a modified fragment shader to compute shading as if a fragment was shaded at a specific time in the past Recreate all input parameters to the shader, including view, light and model matrices, textures and shadow maps for up to 120 frames (2 s) in the past and compare the shading results to the new shading  If the difference of a shading sample$(r,g,b)$ is above a certain threshold $T$, i.e., $T\u0026lt;\\max(|\\Delta r|,|\\Delta g|,|\\Delta b|)$​, we consider the shading to be changed Shading, including gamma-correction and possibly tone-mapping, is computed and compared in floating point This threshold is an approximation of Weber’s law, which states that the just noticeable luminance difference is constant in relation to the base luminance      Result\n Reusing shading samples that are slightly different does not reduce perceived quality For $T=2$ and $T=4$, $Q$ cannot be separated from 0.0 with confidence, and $p_{ref}$ is nearly 50% At a threshold of $T=8$, the mean quality is above 0.0, indicating that some participants see a minor quality deterioration. The distribution is still nearly balanced, with $p_{ref}$=55% At $T=16$, the distribution is just shy of 1 JND($p_{ref}$=75%). $Q$ is closed to 0.5​    4. Temporal Coherence for Shading Reuse 4.1. Temporal coherence of visibility  Project every sample of a current frame back to the previous frame and determine whether the sample was visible before Over 90% of samples stay visible between frames The most significant visibility disruption is caused by large camera movement or fast moving objects  4.2. Temporal coherence of shading  Use temporal forward rendering to determine how shading behaves independently of changes in visibility and spatial sampling More than 75% of all samples change less than the color difference $T=8$ for 120 frames in the test scenes  4.3. Limits of applying temporal coherence  Both the temporal coherence of visibility and the temporal coherence of shading demonstrate a very high potential for reusing shading over many frames Practical implementations need to also consider the spatial sampling of shading  The drift of shading samples, their reprojection error and the required filtering    Evaluate two practical rendering approaches for shading reuse:\n reverse reprojection caching (RRC)  RRC reprojects samples from the previous frame to the current frame, potentially accumulating spatial sampling errors The implementation runs in two passes: a depth pre-pass and a forward rendering pass that either uses the cache or reshades In order to avoid the accumulation of these errors, shading samples can be gathered in a temporally invariant space such as object space or texture space   shading atlas (SA)  Combine the shading atlas with the rendering pipeline of texel shading This method shades pairs of triangles in rectangular blocks that are dynamically allocated in a single texture, the shading atlas The location of the shading samples remains unchanged in the atlas, until the visibility of the triangles changes, or their resolution changes due to a level of detail change, in which case shading is recomputed    Evaluate result:\n Reverse reprojection caching accumulates spatial sampling errors over time, especially when the camera is moving The shading atlas reuse is independent of spatial sampling, and thus the reuse correlates better with the dynamics of the shading  The shading atlas considers samples reusable only when an entire block is reusable, leading to a slightly worse overall reuse    5. Predicting Shading Changes  Existing methods enable us to map shading samples from one frame to the next either through image space reprojection or shading in a temporally unaffected space, such as object space or texture space We require efficient prediction of the point in the future when shading samples will become invalid in order to know how long shading can be reused  5.1. Prediction with fixed upsampling rates  Previous strategies rely on uniform temporal upsampling  i.e., shading samples every $N^{\\mathrm{th}}$ frame   RRC  Updates 16 × 16 pixel tiles with a constant refresh rate A constant fraction of all tiles is updated in each frame, leading to a fixed livespan for each tile   SAU  The livespan of cache entries is also constant, but every cache entry has an individual remaining time to live depending on when it became visible   Evaluate RRC and SAU with the same user study design as described in Section 3 Result  Uniform upsampling is not able to leverage the potential for shading reuse well, even when reusing shading only once (2× upsampling) RRC at 2x temporal upsampling leads to noticeable differences in 82% of the cases For higher upsampling rates (4 and 8), all participants always noticed differences and reported image quality to be close to “significantly worse” A uniform upsampling frequency is not sufficient for longer shading reuse    5.2. Prediction with shading gradients   A first-order gradient analysis is often sufficient in the spatial domain\n  Consider a Taylor approximation of a shading function $s$​​ as an obvious choice for prediction\n A simple linear predictor can be formulated as a first-order Taylor expansion from time $t_0$ to $t$:  $$\ns(t)=s(t_0)+s'(t_0)\\cdot(t-t_0)+e(t)\n$$\nwith a residual error $e(t)$.\n Based on a color threshold $T$, we can predict a reshading deadlien  $$\nd=t-t_0=\\dfrac{T}{s'(t_0)}\n$$\n  Analytic derivatives\n Handle scalar, vector-valued parameters, texture lookup, Poisson-sampled shadow maps Shader inputs such as camera, object or light transformations, are extended with their temporal derivatives All time-varying parameters where a future state can be calculated deterministically (such as prerecorded animations or physical simulations): Obtain their derivatives directly by augmenting the animation code User-driven inputs (Camera movement): compute gradient based on an extrapolation of the input Even though the derivatives can be computed alongside the shading, the overhead is non-negligible  Finite differences\n  An alternative to costly analytic derivatives\n  The simplest case: take the backward difference between two shading results\n It better approximates the limit of finite differences But always requires shading twice in a row    Computed between shading in consecutive frames or between frames that are further apart in time\n  More economical and has a potentially beneficial low-pass filtering effect on spurious shading changes\n  When a shading sample is first considered, shading must always be done twice\n  A first deadline for reshading is extrapolated from the initial gradient\n    Comparison of gradient methods\n  Evaluate these options for temporal forward rendering (TFR), using the previously found threshold $T = 8$​ from the first user experiment\n  Render multiple frames of increasing shading age, resulting in frames with the same sample position, but increasingly outdated shading\n Use this data to retrospectively obtain the ideal deadline Starting from the current frame containing the correct shading result Determine the exact frame in the past where the shading difference exceeds the threshold $T$  Analytical derivatives \u0026amp; finite differences: used to directly predict a future deadline Long-range differences: repeat the process to find the next frame in the past that exceeds the threshold  Limit the search process to 119 frames into the past, effectively clamping the deadline in the range of 1 to 118 frames        Result\n Late shading: cause artifacts in the final image output and thus should be avoided Early shading: harm performance, but does not lower quality Ideally technique:  avoid late shading completely keeping early shadings as low as possible   Simple long-range differences (between two shading points) show the least amount of late shadings, while having only slightly increased early shading    5.3. Spatial filtering of temporal gradients   Propose a simple maximum filter in image space, inspired by the render cache and shading cache\n Make shading decision based on the estimated temporal gradients of neighboring samples    Evaluation\n Using TFR to isolate the effect of the spatial filtering, while avoiding other sources of misprediction, such as reprojection errors Using a downsampling factor of 8 x 8, followed by a convolution with a rectangular kernel size 9 × 9    Result\n The gradient filtering distributes the highly localized gradients of the shadow boundaries to the surroundings   Applying an image-space filter on top of the gradients strongly reduces late shading but increases early shading, leading to less reuse and less performance improvement Long-range differences are most attractive    6. Temporal adaptive shading framework   Temporally adaptive shading (TAS) framework\n Reliably avoids repeating redundant shading computations, while responding instantly to areas where rapid changes of shading occur.    Reuse unit (RU)\n To make the framework largely independent of the rendering algorithm to which it is applied RU is a group of samples for which a uniform decision is made on whether the samples will be shaded anew or shading will be reused The samples of these units are shaded together, and, consequently, must be stored together in the cache data structure The renderer determines visibility independently for each unit An RU can be a single pixel as in the case of reverse reprojection caching or a whole block within the shading atlas    Workflow:\n  Spatially-filtered shading gradients from the last frame are multiplied with the time elapsed since the last shading of each RU and compared to the threshold ($T$​​​) to decide whether reshading is necessary. Newly visible units are always shaded for two consecutive frames to determine a gradient from finite differences\n  The shading is either reused, or the unit is reshaded. In the latter case, a new shading difference to the previous shading result is computed for each sample\n  The shading gradient is estimated based on the shading difference, scaled by the time difference between them, and a spatial filter is applied to distribute the shading gradient information\n    6.1. Temporally adaptive reprojection caching (TARC)  Image-space pixels serve as reuse units Replace the periodic refresh of the reverse reprojection caching shader with the first two steps of the framework Store per-unit and per-sample variables in a double-buffered G-buffer The input buffers are reprojected, and the potentially altered values are stored in the output buffers Store the estimated shading gradient in the G-buffer during the depth pre-pass Implement spatial maximum filtering by downsampling the gradient buffer using a maximum filter with overlapping square kernels In comparison to standard reverse reprojection caching, TARC needs additional memory for screen size buffers to store the shading difference and the time since the last shading  6.2. Temporally adaptive shading atlas (TASA)   Using a texture-space representation for storing shading samples avoids the accumulation of reprojection errors faced by TARC\n  Convenient to define reuse units by proximity of shading samples on object surfaces\n  The reuse units in TASA correspond to two triangles packed into a rectangle of $2^N × 2^M$​ texels, where each unit’s size in the atlas is determined based on its image-space projection\n Other granularities (e.g., 8x8 texels, per-object texture charts, micro-polygons) could be chosen    By retaining the maximum of all shading gradients across an entire reuse unit, a conservative object-space filter is applied to the unit at almost no additional cost\n The samples of a reuse unit are processed together    The resulting object-space filtering is particularly relevant when some of the shading samples are currently occluded in image space\n  Limiting the filter to the boundaries of a reuse unit fails to capture spatial gradients that cross the boundaries of adjacent reuse units\n Example: a shadow boundary may be creeping slowly across an entire surface consisting of multiple neighboring reuse units A solution: extend the object-space filter to support a convolution-style kernel larger than a single reuse unit  But it\u0026rsquo;s a costly operation   Another solution: concatenate the per-reuse-unit filter to an image-space filter that determines the maximum over direct image-space neighbors  Very inexpensive Captures spatial coherence of perspectively close shading samples, which may not be apparent in object space      Resulting Pipeline:\n Exact visibility is computed per frame in a geometry pre-pass and stored in a G-buffer as primitive ID with corresponding shading gradients Reading the primitive ID, the atlas is updated such that it has room for the visible reuse units. The shading gradients are maximum filtered using a 2 × 2 window in image-space for each reuse unit to propagate the maximum gradient in an image-space neighborhood Shading decisions are made on all reuse units. Reuse units for which samples are newly allocated and reallocated in the atlas are always shaded, i.e., they are considered newly visible The shading workload is executed including the computation of the shading differences and shading gradients are directly maximum filtered per reuse unit The G-buffer is revisited for the final deferred rendering pass    The additional memory requirements include a copy of shading atlas to compute the shading differences, per-patch shading differences and times, and a screen space buffer for the spatial filter\n  7. Evaluation and Results  Test TASA with a 16 MPx atlas (TASA16) and with an 8 MPx atlas (TASA8) to evaluate actual use  To avoid sampling artifacts from the atlas when displaying the final image   Use a threshold of $T=8$ in all experiments  Aiming to stay below 1 JND   Present detailed timing results in comparison to Forward+ rendering Experimental setup  Three test scenes An image resolution of 1920 x 1080 Extended the tested sequences to 15 seconds   Run on Intel Core i7-4820K GPU and NVIDIA GTX 1080Ti using a custom rendering framework based on Vulkan  7.1. Reuse   In Section 4.3, the theoretically possible reuse with a perfect prediction of when to shade, resulting in a reuse of 80–90% for both TARC and TASA. About 1–5% of shading is due to changes in visibility\n  Actual reuse for the TAS implementations with a color difference threshold $T=8$:\n TARC shows low reuse for dynamic camera movements  The reprojection error for camera movements also effects shading gradient predictions, which are slightly too high and, in combination with the spatial filter, invalidate shading often While a smaller filter size would increase the reuse potential, it leads to clearly visible artifacts due to missing shading in some scenes A better reprojection filter for gradients and an adaptively sized image-space filter may increase reuse potential for TARC  More advanced filtering and filter size adjustments would also increase overheads     TASA is able to retain a high amount of reuse in comparison to its ideal version  The reuse reduction is similar for both stationary and moving cameras, underlining that shading in texture space enables consistent addressing of shading samples View-dependent shading effects do not heavily influence shading reuse Only in Space with its many highly metallic materials, a moving camera significantly reduces shading reuse      7.2. Quality  For $T=4$， $p_{ref}$ is close to 50%, and $Q$ is at about 0.1 For $T=8$, $p_{ref}$ is about 60%, still significantly below 1 JND, and $Q$ is 0.2, indicating a very high quality A setting of $T = 16$​ is about twice as bad in $Q$​ and very close to 1 JND, thus, we would suggest to use $T = 8$ For $T = 32$​, TARC is already above 1 JND, and $Q$​ is close to “slightly worse” Unknown reason for the slight drop in $Q$ for TASA16 from $T = 2$ to $T = 4$  as the confidence intervals overlap, this may just be a statistical outlier    7.3. Runtime   The overheads of TAS include computing shading differences, spatial filtering, and dynamically deciding whether to shade or not\n May lead to thread divergence during shading and thus reduce the efficiency    Measure the overhead of TARC and TASA in addition to the full shading\n TARC: between 14.5% to 16.9% TASA: between 2.3% to 5%    The actual speedups:\n Among the tested scenes, Space is especially difficult to speed up using temporal coherence, since most of the scene’s surface points are either very dynamic or belong to the sky box TARC does not improve over Forward+ for moving cameras, but it does have some considerable speedups between 1.38× and 2.4× when the camera is stationary The main focus is on the performance gains of TASA  Able to reuse shading across the left and right eye buffers in VR stereo rendering TASA outperforms the other methods for all scenes, both in mono and stereo rendering The speedup in stereo mode over Forward+ is in the range 2 - 5× (1.1 - 3× in mono mode)  TASA must compensate the overhead of its SA foundation SA alone is only around half the speed of Forward+ for monoscopic rendering, most likely due to its 8 MPx atlas size that is 4× the resolution of the final output image        Overall\n Adaptivity is key for temporal shading reuse  Uniform temporal reuse strategies reduce shader invocations, quality drops quickly Using simple shading differences with spatial filtering for gradient estimates works well and is efficient Especially placing shading samples in texture space appears to be an efficient strategy for reusing them over longer periods of time   Using an atlas that matches the screen resolution introduces spatial sampling artifacts and reduces sharpness  TAS can easily compensate for these additional shading samples, leading to overall performance gains    7.4. Free-moving virtual reality experiment   Integrated TASA into Unreal Engine 4 and conducted a small user experiment in VR\n Adapted the Showdown VR Demo scene , a slow motion fly-through of a combat scenario involving several soldiers fighting a giant animated robot The comparison to the threshold $T$​​​ is evaluated after tone mapping with the Academy Color Encoding System (ACES) Filmic Tonemapper used in Unreal Engine 4    For VR user experiment\n Slightly modified the scene by subdividing large primitives that exceed the maximum block size in the shading atlas Modified the scene to include fully dynamic directional lighting with cascaded shadow mapping, which was only approximated in the original scene Eight participants (6 male, 2 female, age 24 to 33, with VR experience) tried the SA baseline, followed by TASA configurations using the thresholds [4, 8, 16, 32, 64] and SAU with 4× and 8× upsampling in a randomized order Used an Intel Core i7-8700K with an NVIDIA RTX 2080Ti and displayed on an HTC Vive at a resolution of 1512 × 1680 per eye, using a fixed frame rate of 90 Hz    Result\n For TASA with $T = 64$ and $T = 32$​, all participants detected artifacts  $T=64$ was “very bad” $T=32$​​​ was “adequate with some annoying artifacts”   For $T=16$​​​​​​, four participants reported an identical experience compared to the baseline, while the remaining four detected minor artifacts on shadows, reflective surfaces and the soldiers in the scene For $T=8$​​​​, six participants reported an identical experience compared to the baseline, while two participants were still able to identify minor artifacts on the soldiers For $T=4$​​, no participant was able to detect any visual artifacts, and all participants reported an identical experience compared to the baseline     For SAU with 4× upsampling, four participants reported artifacts related to “jittery” and “flickery” motion, and they reported “low frame rate” for the reflections For SAU with 8× upsampling, all but one participant reported major artifacts of reflections and shadows, as well as major discomfort, particularly describing the experience as “very uncomfortable when moving around”  One participant even reported a mild case of motion sickness   Constant temporal upsampling is more likely to be perceived as jittery, which according to the participants is more discomforting and distracting than the artifacts of TASA, even for large thresholds TASA with $T = 8$​ resulted in a mostly identical experience to the baseline   TASA provides a more optimal performance-quality tradeoff compared to SAU  8. Discussion and Conclusion  Investigate how shading reuse is perceived and could benefit rendering, considering visibility, spatial sampling and temporal behavior of shading, separated and combined Evaluated the perception of outdated shading using a rendering approach that separates shading from visibility and spatial sampling effects, finding that a shading difference of 3% ($T = 8$​​ after 8-bit quantization) is not noticed by study participants Even in highly dynamic scenes, many shading samples stay valid for extended periods of time, when considered independently of visibility and spatial sampling There is a potential of typically more than 80% shading reuse from frame to frame even in highly dynamic scenes at 60 Hz  The higher frame rates of VR increase this potential further   Accumulating spatial resampling errors limits the temporal reuse  Using texture-space caching of shading samples instead   Fixed upsampling techniques lead to noticable artifacts even at low upsampling rates  Extrapolating shading differences works very well when combined with a simple image-space filter for capturing spatio-temporal effects    8.1. Limitations   The simple box filter with a rather big kernel size used in TARC leads to considerable amounts of unnecessary shading\n A more advanced spatial filter could consider spatial gradients such as optical flow to resolve these issues at the cost of increased runtime and complexity    While the existing measures capture most changes, some less frequent ones can still cause artifacts\n Example: discontinuous rendering or changes that propagate from outside the image or from an occluded area Depending on the use case, specialized cases such as discontinuous changes, e.g. to light sources, can be caught on the scene object level A more general solution to capture artifacts from discontinuous shading could speculatively update samples that are not due yet    Evaluation is based on a single threshold applied to the per-channel maximum RGB color difference after tone mapping\n The threshold might be too conservative in certain areas of the HDR spectrum and overlook additional gains A more advanced method may be necessary    A method for deriving the threshold for noticeable differences from the perception of the human visual system has the potential to lead to further temporal savings\n This can possibly be done in a different color space or in the high dynamic range space before tone mapping Example: a higher threshold could be used for dark pixels that are close to bright ones, or a lower threshold needs to be used in dark areas where the visual system is more sensitive    The shading atlas shades pairs of triangles within rectangular blocks with power-of-two side lengths\n When the level of detail changes, a block of a different size is allocated and shading cannot be reused    8.2. Future work   Temporal reuse and TAS can be applied to other rendering techniques, including global illumination algorithms and ray-tracing\n  For the overall speedup, it is important to not only consider the non-shading workload, such as the geometry stage, but also pre- and post-processing, which do not necessarily lend themselves to shading reuse\n Motion blur and depth of field benefit greatly from spatial shading reuse, especially in object or texture space as shown by stochastic rasterization literature Many of the currently used preand post-processing techniques approximate global shading effects, such as shadows and reflections  If, by virtue of shading reuse, more time can be spent on the samples that actually require shading, this benefits the trend for moving global effect computation from post-processing to ray-tracing      In the worst case (discontinuous view change), the whole scene has to be shaded, this can lead to a higher variability in frame rate: Only some frames can be accelerated; others remain at the baseline speed\n The absolute frame time variability was unchanged in comparison to the baseline, while the mean frame time was reduced The frame time variability depends mostly on the complexity of the current view Lower frame rate variability could be obtained by using TAS as an oracle for a scheduling technique, which uses the predicted shading differences as priority, instead of making decision based on a fixed threshold    TAS can be easily combined with spatial reuse of sampling, such as variable rate shading, foveation and checkerboard rendering\n Bring more physically correct shading and fewer approximations that require pre-processing steps, like rendering shadow maps, or post-processing, like screen-space effects in deferred rendering    ","description":"Siggraph 2021 Paper Reading","id":0,"section":"posts","tags":["Rendering"],"title":"Temporally Adaptive Shading Reuse for Real-time Rendering and Virtual Reality","uri":"https://chaphlagical.github.io/en/posts/paperreading/temporally_adaptive_shading_reuse_for_real_time_rendering_and_virtual_reality/"},{"content":"Author: MILAN JAROŠ, LUBOMÍR ŘÍHA, PETR STRAKOŠ, and MATĚJ ŠPEŤKO\nInstitution: IT4Innovations, VSB–Technical University of Ostrava, Czech Republic\nLink: https://dl.acm.org/doi/10.1145/3447807\n1. Introduction GPUs Rendering vs CPUs Rendering\n Limited memory size Example: Pixar\u0026rsquo;s Coco movie were using up to 120GB, do not fit into the memory of a single GPU  Main contribution\n  A solution to GPUs rendering memory size limitation\n Based on replication of a small amount of scene data, between 1% ~ 5%, and well-chosen distribution of the rest of the data into the memory of several GPUs Both replication and distribution of data is based on a memory access pattern analysis of a path tracer during a 1spp prepass The data with the highest number of memory accesses are replicated and the rest is stored only in the memory of the GPU that had the highest number of accesses to it Minimizes the penalty associated with reading data from the remote memory and is effective at providing near-linear scalability    Demonstration that our proposed approach works on a memory management level\n Any path tracing code that supports GPU acceleration using CUDA can adopt our approach without redesigning its internal data structures    Two key technologies\n NVLink GPU Interconnect  Enables multiple GPUs to efficiently share the content of their memories due to its high bandwidth and low latency   CUDA Unified Memory(UM)  Provides programmers with control over data placement across the memories of interconnected GPUs    2. Related Work 2.1. Out-of-core Rendering  Out of core rendering is the capability to render (with GPUs) scenes requiring more memory than the one directly connected to the device It will use the system\u0026rsquo;s memory instead  2.2. Distributed Rendering  sort first: Image based partitioning sort middle: Related to rasterization only sort last: Use scene data distribution  2.2.1. Image-Parallel Rendering  Distribute among processors or machines per blocks of pixels of a rendered image Most common and efficient way the scene data is fully replicated in all local memories and ray tracing is embarrassingly parallel In the case of ray tracing complex scenes that do not fit into local memory, this approach results in on-demand scene data movement while rays remains fixed (Moving scene data instead of ray data) Our proposed solution is based on scene data communication while rays never leave the GPU they are created on  2.2.2. Data-Parallel Rendering  Distribute the workload by subdividing the scene data In the case of distributed ray tracing, these approaches transfer ray data among processors or machines, while scene data do not move after the initial distribution (Moving ray data instead of scene data)  2.3. Distributed Shared Memory Systems  Shared Memory Processors (SMP) \u0026amp; Distributed Shared Memory (DSM)  Local memory with caches Hardware or software layer transparently creates an illusion of global shared memory for applications   The latency to access remote data is considerably larger than the latency to access local data  good data locality is therefore critical for high performance    2.4. CUDA Unified Memory For Multi-GPU Systems  UM Manages communication between multiple GPUs and CPUs transparently by adopting DSM techniques UM simplifies both out-of-core processing between GPUs and CPUs as well as multi-GPU processing and combinations of both NVLink interconnect is the key enabler of DSM multi-GPU system  3. Blender Cycles Path Tracer  An unbiased renderer based on unidirectional path tracing that supports CPU and GPU rendering For acceleration it uses a Bounding Volume Hierarchy (BVH) Supports CUDA, Optix, and OpenCL  3.1. Extensions for Multi-GPU Support Workflow:\n distribute the data structures evenly among all GPUs run the kernel with memory access counters and get the memory access statistics redistribute the data structures among GPUs based on memory access statistics run the original path-tracing kernel with redistributed data  3.2. Multi-GPU Benchmark Systems   BullSequana X410-E5 NVLink-V blade server\n with 4 Tesla V100 GPUs, each with 16 GB of memory and direct NVLink interconnect    NVidia DGX-2\n able to process massive scenes of sizes up to 512 GB in the shared memory of its 16 Tesla V100 GPUs, each with 32 GB of memory The uniqueness of this platform is the enhancement of the NVLink interconnect by using NVSwitches, which enable the connection of all 16 GPUs and higher bandwidth    3.3. Benchmark Scenes 4. Data Distributed Multi-GPU Path Tracing 4.1. Basic Distribution of Entire Data Structures 4.1.1. Memory Access Analysis  Define the order in which data structures are replicated as a ratio of the total memory accesses to a particular data structure over its size The analysis was done on the first sample when rendering the scenes with a resolution of 5,120x2,560 pixels   small_structures: a set of data structures smaller than 16MB (the most important one is svm_nodes, which stores Shader Virtual Machine (SVM) data and codes) bvh_node: stores the BVH tree without its leaves (leaves are stored in a separate structure) prim_tri_verts: holds coordinates of all vertices in the scene prim_tri_index: a set of all triangles in the scene and it contains indices to the prim_tri_verts  We can see that:\n The most important data structure is bvh_nodes, because it is responsible for 79.6% of all memory accesses  If it is replicated in the memory of all GPUs, the 79.6% of all memory accesses will be to the local memory The size of this structure is 7.2 GB, which represents 26.5% of the entire scene size   If in addition to small_structures and bvh_nodes, prim_tri_index and prim_tri_verts are also replicated, then the relative rendering time is only 109% while 40.7% of the scene is replicated and the rest is distributed  4.1.2. Performance and Scalability Evaluation The scalability of the proposed approach is evaluated for four different cases:\n all data structures are replicated—this case serves as a baseline as it achieves the best performance and scalability all data structures are evenly distributed  continuous distribution: the structures are divided into large chunks of a size equal to the structure size over a number of GPUs, and each GPU owns one chunk round robin distribution: the distributed structure is divided into chunks of 2 MB, which are distributed in a round robin fashion   small structures and bvh_nodes are replicated while all other data structures are distributed small structures, bvh_nodes, prim_tri_index, and prim_tri_verts are replicated while all other data structures are distributed  Result:\n round robin distribution of small chunks performs better than continuous distribution of large chunks, therefore it is always used to distribute non-replicated data structures path tracing with fully distributed data structures does not scale on both platforms (there is reasonable scalability for two GPUs on DGX-2, but not beyond that) if small structures and bvh_nodes are replicated, the scalability is significantly improved if small structures, bvh_nodes, prim_tri_index, and prim_tri_verts are replicated, the scalability is further improved  4.2. Advanced Distribution Based on Memory Access Pattern and Statistics   The data placement is done with chunks, and hints are set for each chunk individually\n  The optimal chunk size was identified experimentally by benchmarking the path tracer performance for chunks of sizes from 64 kB to 128 MB:\n for scenes smaller than 30 GB the optimal chunk size is 2 MB (smaller chunks are not recommended) for scenes of sizes around 40 GB the optimal chunk size is 16 MB for scenes of sizes above 120 GB the optimal chunk size is 64 MB    The workflow of this data placement strategy:\n copy/distribute every data structure across all GPUs in a round robin fashion using chunks of an optimal size run the path tracing kernel with memory access counters for 1spp to measure the statistics gather the statistics on the CPU and run the proposed algorithm to get the optimal data chunks distribution use cudaMemAdvise to migrate or replicate all chunks run the original unmodified path tracing kernel    4.2.1. Memory Access Pattern Analysis   To identify the memory access pattern, per chunk access counters have been implemented in the GPU path tracing kernel of Cycles\n Independent counters for all data structures and all their chunks A total number of memory accesses per chunk can be recorded for each GPU    The memory analysis starts with all data structures being evenly distributed using a round robin distribution\n The modified path tracing kernel with memory access counters is executed on all GPUs for one sample    When the kernel finishes, then for every chunk of every structure, a number of accesses from all GPUs is recorded\n The workload is distributed among GPUs by horizontal stripes so that each GPU works on one stripe     1% of scene data covers between 56.7% and 74.4% of memory accesses, depending on the scene size This analysis shows that there are clear candidates among the chunks that should be replicated on all GPUs, while a major portion of the data is accessed infrequently and can be distributed with an acceptable impact on performance.  4.2.2. Data Placement Algorithm Based on Memory Access Pattern   The per GPU counters are summed to get the total number of accesses $a_{sum}$ for each chunk $c\\in C$ of each data structure $s\\in S$ and $a(a_{sum},s,c)$ tuple is created\n  All tuples are put into a single 1D array $H_{comb}$. The array is sorted by $a_{sum}$ from largest value to the smallest one and stored in $H_{\u0026lt;}$ array\n  The last input of the algorithm is the number of chunks that can be replicated $N_{dup}$. This value can either be set manually or automatically using formula:\n$$\nN_{dup}=\\dfrac{1}{C_s}\\Big(G_f-\\frac{S_s}{N_g}\\Big)\n$$\n $G_f$: the amount of free memory per GPU in MB available to store scene data $S_s$: scene size in MB $N_g$: total number of GPUs $C_s$: the chunk size in MB    Define a threshold $t$ as the $N_{dup}$-th element in the sorted array $H_\u0026lt;$ and evaluate all tuples in the array $H_\u0026lt;$. If the counter value $a_{sum}$ is larger than $t$, then the corresponding chunk will be set as *SetReadMostly*, and therefore replicated\n In the opposite case, the chunk is set as SetPreferredLocation and is assigned to the GPU with the highest number of accesses to this chunk  If the memory of this GPU is full, then the GPU with second, third, fourth, and so on, highest number of accesses is selected until a GPU with free memory is found.   It the counter value is equal to zero (without any accesses), then the corresponding chunk will be distributed in a round robin fashion across GPUs with free memory    4.2.3. Performance Evaluation  The performance of the proposed algorithm was evaluated for different ratios between replicated and distributed data at a 2 MB chunk level of granularity for the Moana 12 and 27 GB scenes   Figure shows the path tracing performance for one sample per pixel and 5,120x2,560 pixel resolution for both scenes and platforms, and for all available GPUs Once at least 1% of chunks are replicated, the performance is almost identical  4.2.4. Maximum Scene Size Analysis The following equation describes the maximum ratio of replicated data that fits into the memory of GPU memory for a scene of a given size:\n$$\nN_{max_dup}=\\frac{\\Big(G_f-\\frac{S_s}{N_g}\\Big)}{\\Big(S_s-\\frac{S_s}{N_g}\\Big)}\n$$\n $G_f$: the amount of free memory per GPU in MB available to store the scene data $S_s$: the scene size in MB $N_g$: total number of GPUs  5. Performance for massive scenes Group 1: Moana 38 GB, Museum 41 GB, Agent 37 GB, and Spring 41 GB are designed to stress the Barbora GPU server with 64 GB of total GPU memory\n the performance of the Barbora server is almost identical to the performance of DGX-2 for the same amount of scene replication (up to 10%) DGX-2 is able to further replicate scene data up to 60%, which improves performance by 2.8% only in the case of the Moana 38 GB scene (for the other scenes the performance is higher by only less than 1%) This means that for scenes of sizes approximately up to 45 GB distributed over 4 GPUs, the significantly less complex and cheaper GPU interconnect in the Barbora server is sufficient For the Museum, Agent, and Spring scenes 2% of scene replication attains optimal performance. This holds for 4, 8, and 16 GPUs Only the Moana scene needs higher amounts of replicated data, up to 25% for 16 GPUs Scalability can be evaluated on DGX-2 for 4, 8, and 16 GPUs only. For the Moana, Museum, Agent, and Spring scenes, for 5% scene replication, the parallel efficiencies, going from 4 to 16 GPUs, are 82.7%, 97.1%, 97.9%, and 98.1%, respectively. In the case of the Moana scene, a higher replication ratio is needed to improve scalability, e.g., for 25% data replication ratio the parallel efficiency is 94.4%  Group 2: Moana 169 GB, Museum 124 GB, Agent 167 GB, and Spring 137 GB are designed to stress the DGX-2 server with 512 GB of total GPU memory\n The performance is affected by selecting the right chunk size, particularly for the Moana scene For 8 GPUs and the Moana, Agent, and Spring scenes, if the replication ratio is 10% the scene does not fit into GPU shared memory anymore and chunks are swapped between GPU and CPU memory (the default behavior of the CUDA Unified Memory), which makes the rendering several times slower depending on the number of chunks being moved back and forth. This is the point at which our approach stops working, and therefore it is crucial to correctly select the replication ratio to avoid this situation For the Agent scene 1% of scene replication gives optimal performance for both 8 and 16 GPUs. The Spring scenes needs only 0.1% for 8 GPUs and 0.25% for 16 GPUs. The Museum scene needs 1% for 8 GPUs and 2% for 16 GPUs. Finally, the Moana scene requires 2% for 8 GPUs and 5% for 16 GPUs Scalability between 8 and 16 GPUs is good for all scenes. The parallel efficiencies are 93.8%, 98.8%, 98.6%, and 99.0% for the Moana, Museum, Agent, and Spring scenes, respectively   Analyze memory access pattern only for one sample per pixel Rendering times grows linearly with the number of samples   Number of bounces influence  6. Conclusions Contribution\n Presented a solution for path tracing of massive scenes on multiple GPUs Analyzes the memory access pattern of a path tracer and defines how the scene data should be distributed across GPUs with a minimal loss of performance Those parts of the scene data that have the highest memory access rate are replicated on all GPUs, because their distribution would have a major negative impact on performance  Methods\n Uses the same memory management rules for the entire data structure Splits the data structures into chunks and we control the placement and/or replication of each chunk separately  Feature\n Only control the memory allocations, the path tracer data structures do not have to be redesigned Take full advantage of NVLink 2.0 interconnect and its high bandwidth and low latency  ","description":"Siggraph 2021 Paper Reading","id":1,"section":"posts","tags":["Rendering"],"title":"GPU Accelerated Path Tracing of Massive Scenes","uri":"https://chaphlagical.github.io/en/posts/paperreading/gpu_accelerated_path_tracing_of_massive_scenes/"},{"content":"Abstract Real-time Rendering\n Frame rate Latency  Spatial Super Sampling: DLSS\n Decreasing the rendering time of each frame by rendering at a lower resolution  Temporal Super Sampling\n Producing more frames on the fly Problems:  It\u0026rsquo;s own computational cost The latency introduced by interpolating frames from the future    ExtraNet\n  An efficient neural network that predicts accurate shading results on an extrapolated frame, to minimize both the performance overhead and the latency\n  Rendered auxiliary geometry buffers of the extrapolated frame \u0026amp; Temporally reliable motion vectors\n  Train to perform two tasks:\n Irradiance in-painting for regions that cannot find historical correspondences Accurate ghosting-free shading prediction for regions where temporal information is available    A robust hole-marking strategy to automate the classification of these tasks\n  The data generation from a series of high-quality production-ready scenes\n  1. Introduction Some techniques to increase the rendering performance\nLeveraging the temporal coherency between frames: Reduces the sampling rate at each individual frame, and rely on temporal reprojections to combine samples taken over multiple frames to reconstruct a high quality image (Still performing spatial upsampling / reconstruction, with a virtually higher sampling rate acquired temporally to speed up the generation of the current frame of interest)\n TAA (Temporal Anti-Aliasing) TAAU (Temporal Anti-Aliasing Upsample) DLSS (Deep Learning Super Sampling) Ray Tracing Denoising  Limitation of current work\n Frame Interpolation: only when the next frame has been fully rendered, their algorithms can start to work to interpolate one or more frames in between Performance: generating a new frame is already more expensive than a full rendering of it Situation: more information can be used in rendered scene, like G-buffers  Contribution\n A new temporally interleaved real-time rendering and extrapolation architecture that can nearly double the frame rate without introducing additional latency. A novel neural network architecture that leverages G-buffers and temporal motion (computed both from consecutive frames and temporally reliable motion vectors) to perform the extrapolation as a shading prediction task. A lightweight design that incurs low performance overhead (around 8 milliseconds per frame at 720P) but results in high quality and smoothly extrapolated frames.  2. Related work and background 2.1. Temporal Reconstruction  Rely on temporal motion vectors Temporal reconstruction methods reduce the samples taken at each individual frame, and reproject samples over the course of multiple frames using backward motion vectors to reconstruct a high quality image In their paper  Leverage temporal motion vectors to reproject shading of the previous rendered frames to the extrapolated frames But they do not have any samples in the extrapolated frames, unlike temporal reconstruction    Motion Vectors\n$$\n\\pmb p_0^{t}=\\pmb P^t\\pmb M^t\\pmb V^t \\pmb s_0\n$$\n$$\n\\pmb p_0^{t-1}=\\pmb P^{t-1}\\pmb M^{t-1}\\pmb V^{t-1}\\pmb s_0\n$$\nStore screen space motion vectors =\u0026gt; Velocity Buffer\nvertex shader:\n1 2 3 4 5 6 7 8 9 10 11 12  uniform mat4 uModelViewProjectionMat; uniform mat4 uPrevModelViewProjectionMat; smooth out vec4 vPosition; smooth out vec4 vPrevPosition; void main(void) { vPosition = uModelViewProjectionMat * gl_Vertex; vPrevPosition = uPrevModelViewProjectionMat * gl_Vertex; gl_Position = vPosition; }   fragment shader:\n1 2 3 4 5 6 7 8 9 10  smooth in vec4 vPosition; smooth in vec4 vPrevPosition; out vec2 oVelocity; void main(void) { vec2 a = (vPosition.xy / vPosition.w) * 0.5 + 0.5; vec2 b = (vPrevPosition.xy / vPrevPosition.w) * 0.5 + 0.5; oVelocity = a - b; }   2.2. Texture In-painting   Using backward motion vectors to reproject shading from previous frame is that disoccluded regions will not contain any valid information in the previous frame, resulting in \u0026ldquo;holes\u0026rdquo; in the reprojected image\n  Some work:\n Convolution Deep learning Resort to a two-stage prediction strategy to separately predict missing textures in a step-by-step manner    Limitation\n Not designed specifically for real-time rendering Perform the in-painting task completely on single images without temporal information and G-buffers The running time of some methods are in the order of seconds, which are not suitable in our real-time extrapolation task    2.3. Image Warping  Realized by forward scattering or backward gathering, according to their data access patterns Compared with forward mapping, backward mapping is more appealing in real-time rendering due to its high efficiency Although visually plausible contents can be recovered at hole regions after image warping, shadow and highlight movements are almost untouched in these traditional methods, especially those used in the context of frame extrapolation  2.4. Video and Rendering Interpolation  Interpolation based frame rate upsampling is widely used for temporally upsampling videos, using optical flow of the last couple of frames Optical flow is an image based algorithm that often produces inaccurate motion, which will lead to artifacts in the interpolation results Uses accurate motion vectors available from the rendering engine, usually rely on image warping techniques to reuse shading information across frames Deep learning based video interpolation methods are still time-consuming, thus are not suitable for real-time rendering  3. Motivation and Challenge 3.1. Latency  Interpolation =\u0026gt; Produce significant lantency Extrapolation =\u0026gt; doesn\u0026rsquo;t introduce any additional latency  3.2. Challenge Disocclusion\n Backward motion vectors may not always exist  Traditional motion vectors propose to use the same pixel values from previous frames, resulting in ghosting artifacts in the disoccluded regions Occlusion motion vectors alleviates this problem by looking for a nearby similar region in the current frame, then look for the corresponding region from previous frames   Leverage an in-painting network to reshade the occluded regions using occlusion motion vector  Dynamic changes in shading\n Dynamic changes in shading over frames, such as moving shadows, parallax motion from reflections and the colors on rotating objects, may change drastically and are thus unknown in the current frame Always keep the most recently rendered 3 frames for the network to learn to extrapolate the changes  Other challenges\n Extremely low tolerance towards errors and artifacts The inference of the neural network must be (ideally much) faster than the actual rendering process  4. ExtraNet for frame extrapolation Design and train a deep neural network, namely ExtraNet, to predict a new frame from some historical frames and the G-buffers of the extrapolated frame.\n Considering that disoccluded regions of the extrapolated frame lack shading information but reliable and cheap G-buffers are available, we construct an In-painting Network to reshade the disoccluded regions with the help of occlusion motion vectors and those G-buffers In contrast to the general image in-painting task, regions that are not disoccluded may also contain invalid pixels attributing to dynamic shading changes in the scene. To tackle this issue, we design a History Encoder, an additional convolutional neural network, to extract necessary information from historical frames To meet the requirement of high inference speed which is critical to real-time rendering, adopting lightweight gated convolutions in the In-painting Network.  4.1. Problem Formulation  Frame $i$: Denoting the current frame rendered by the graphics engine Frame $i+0.5$: Next frame the network will predict Two historical frames including frame $i-1$ and $i-2$ and the corresponding G-buffers  Pipeline:\n Demodulation: Dividing by the albedo to acquire texture-free irradiance Warp the demodulated frame using both traditional backward motion vectors and occlusion motion vectors The warping operation probably incurs holes in the frame. Using In-painting network to fill the frame Modulation: multiplying by the albedo to re-acquire textured shading Apply regular post-processing(tone mapping ,TAA\u0026hellip;)  4.2. Motion Vectors and Image Warping Instead of computing a zero motion vector in disoccluded regions as the traditional backward motion vector does, occlusion motion vector computes the motion vector in disoccluded regions as the motion vector of the foreground in the previous frame\n4.3. Network Architecture   Similar structure with U-NET\n  Adopt gated convolutions\n$$\n\\pmb M=\\mathrm{Conv}(\\pmb W_m, \\pmb X)\n$$\n$$\n\\pmb F=\\mathrm{Conv}(\\pmb W_f,\\pmb X)\n$$\n$$\n\\pmb O=\\sigma(\\pmb M)\\odot \\pmb F\n$$\n $\\pmb X$: Input feature map $\\pmb W_m$ and $\\pmb W_f$: Two trainable filters $\\odot$ : Element-wise multiplication $\\sigma(\\cdot)$: Sigmoid activation    Gated convolutions increase the inference time\n Resort to a light-weight variant of gated convolution by making $\\pmb M$ a single-channel mask Not use any gated convolution in the upsampling stage  Assume all holes have already been filled in the downsampling stage      Use a residual learning strategy in our pipeline by adding the predicted image of our network with the input image warped by traditional backward motion vectors\n This stabilizes the training process    4.4. History Encoder   Input: Warped images of the current frame $i$ and its two past frames including frame $i-1$ and frame $i-2$\n Warp frames $i-1$ and $i-2$: accumulate the motion vectors Invalid pixels should be marked for these frames    Structure: Nine $3\\times 3$ convolution layers\n Down sample the input Shared by different historical frames    Output: Implicitly encode the high-level representations of shadows and shadings in previous frames, are concatenated together and fed into our In-painting Network\n  Optical flow can be explicitly predicted from several historical frames\n But correct forward optical flow is difficult to acquire in the frame extrapolation pipeline, since only historical frames are available Use backward optical flow as an approximation Not using it    4.5. Loss Function Penalizes pixel-wise error between the predicted frame $\\pmb P$ and ground-truth frame $\\pmb T$\n$$\n\\mathcal L_{l_1}=\\frac{1}{n}\\sum_i |\\pmb P_i - \\pmb T_i|\n$$\nhole-augmented loss: penalize more in the hole regions marked beforehand\n$$\n\\mathcal L_{\\mathrm {hole}}=\\frac{1}{n}\\sum_i |\\pmb P_i-\\pmb T_i|\\odot(1-\\pmb m)\n$$\n $\\pmb m$​ is the binary mask fed into network  The shading-augmented loss: focuses on handling potential shading changes in the predicted frames\n$$\n\\mathcal L_{\\mathrm{shade}}=\\frac{1}{k}\\sum_{i\\in \\Phi_{\\mathrm{top}-k}}|\\pmb P_i-\\pmb T_i|\n$$\n Shading changes: stem from moving shadows due to dynamic lights and specular reflections Considering that shadows and reflections tend to generate large pixel variation among neighboring frames, we select $𝑘$​ pixels with top $𝑘$​ largest errors from the predicted frame and mark these pixels as potential shading change regions $\\Phi_{\\mathrm{top}-k}$ includes the indices of $k$ pixels with top $k$ largest errors  Currently, $k$ set to 10% of total pixels number    Final loss function\n$$\n\\mathcal L=\\mathcal L_{l_1}+\\lambda_{\\mathrm{hole}}\\mathcal L_{\\mathrm{hole}}+\\lambda_{\\mathrm{shade}}\\mathcal L_{\\mathrm{shade}}\n$$\n $\\lambda_{\\mathrm{hole}}$ and $\\lambda_{\\mathrm{shade}}$​ ​are weights to balance the influence of the losses, here set to 1  4.6. Training detail  PyTorch Mini-batch SGD and Adam optimizer mini-batch size as 8, $\\beta_1 = 0.9$ and $\\beta_2=0.999$ in Adam optimizer default initialization applied HDR image: logarithm transformation $y=\\log(1+x)$ before feeding images into network  5. Dataset 5.1. Scenes and Buffers Each dumped frame comprises 10 buffers which can be divided into three categories:\n Actual shading frame before tone mapping (PreTonemapHDRColor) and albedo (Base color) for demodulation G-buffers used in our network, including scene depth ($𝑑$​, 1channel), world normal ($\\pmb n_𝑤$​​, 3 channels), roughness (1 channel), and metallic (1 channel) Other auxiliary buffers which are necessary for marking holes (invalid pixels) in the warped frame. They are motion vector, world position ($\\pmb p_𝑤$), NoV ($\\pmb n_𝑤\\cdot \\pmb v$, the dot product of world normal $\\pmb n_𝑤$ and view vector $\\pmb v$), and customized stencil ($𝑠$)  5.2. Marking Holes When reusing previous frames to generate a new frame, some pixels in the warped frame will be invalid, due to camera sliding and object moving. These pixels should be marked as holes before feeding into the network\n  For occlusion caused by object moving, used custom stencil to indicate the sharp changes along the edge of the dynamic object. When custom stencil value is different between the current frame ($𝑠$) and the warped frame ($𝑠_𝑤$​), these pixels will be marked as invalid:\n$$\n\\Phi_{\\mathrm{stencil}}={s_i-s_{w,i}\\neq 0}\n$$\nwhere $𝑖$ is the pixel index and $\\Phi_{\\mathrm{stencil}}$​ is the set including pixels that are counted as invalid according to stencil value\n  Self occlusions for some dynamic objects when they are moving or the camera is sliding. Resort to world normal since world normal will probably change in these regions. Calculate the cosine value between current frame’s world normal ($\\pmb n_𝑤$) and warped frame’s world normal ($\\pmb n_{𝑤𝑤}$), i.e., $\\pmb n_𝑤\\cdot \\pmb n_{𝑤𝑤}$. If this value is large than a predetermined threshold $𝑇_𝑛$, the corresponding pixel indexed by $𝑖$ in the warped frame is counted as invalid:\n$$\n\\Phi_{wn}={\\pmb n_{w,i}\\cdot \\pmb n_{ww,i}\u0026gt;T_n}\n$$\nwhere $\\pmb \\Phi _{wn}$​​ contains invalid pixels marked by differences in world normal\n  Invalid pixels due to camera movement. These pixels can be selected out by world position, considering that static objects’ world positions keep unchanged in a 3D scene. For a given pixel, let $\\pmb p_𝑤$ and $\\pmb p_{𝑤𝑤}$​​​​ be the world position of current frame and warped frame, respectively. We calculate their distance by $|\\pmb p_𝑤 - \\pmb p_{𝑤𝑤}|$. If this distance is larger than a threshold $𝑇_𝑑$ (which is computed by NoV and depth values), we mark this pixel as invalid. Hence, the invalid pixels in this set $\\Phi_{wp}$​ are\n$$\n\\Phi_{wp}={|\\pmb p_{w,i}-\\pmb p_{ww,i}|\u0026gt;T_d}\n$$\n  Finally,\n$$\n\\Phi_{\\mathcal{comb}}=\\Phi_{\\mathrm{stencil}}\\cup\\Phi_{wn}\\cup \\Phi_{wp}\n$$\n6. Result and Comparisons Training setups and time\n6.1. Comparisons against Frame Interpolation Methods 6.2. Comparisons against Frame Extrapolation Methods 6.3. Comparisons against ASW Technology 6.4. Analysis of Runtime Performance and Latency 6.5. Ablation Study Validation of History Encoder\nValidation of occlusion motion vectors\nValidation of the shading-augmented loss\nFailure cases\n","description":"Siggraph Asia 2021 Paper Reading","id":2,"section":"posts","tags":["Rendering"],"title":"ExtraNet: Real-time Extrapolated Rendering for Low-latency Temporal Supersampling","uri":"https://chaphlagical.github.io/en/posts/paperreading/extranet/"},{"content":"I used to use jekyll to build my blog, but it is really slow. Thanks to zzossig and his awesome theme. I finally move my blog to Hugo. And it\u0026rsquo;s really nice looking and convenient. Just keep recording and sharing some interesting knowledge!\n","description":"I move my blog to Hugo!","id":4,"section":"posts","tags":null,"title":"update","uri":"https://chaphlagical.github.io/en/posts/update/"},{"content":"Here I share something interesting!\n","description":"My Blog","id":5,"section":"","tags":null,"title":"About","uri":"https://chaphlagical.github.io/en/about/"}]