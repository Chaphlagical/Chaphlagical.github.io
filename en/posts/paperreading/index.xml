<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" 
  xmlns:content="http://purl.org/rss/1.0/modules/content/" 
  xmlns:dc="http://purl.org/dc/elements/1.1/" 
  xmlns:atom="http://www.w3.org/2005/Atom" 
  xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" 
  xmlns:media="http://search.yahoo.com/mrss/">
  <channel>
    <title>Paper Reading on Chaf&#39;s Blog</title>
    <link>https://chaphlagical.github.io/en/posts/paperreading/</link>
    <description>Recent content in Paper Reading on Chaf&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>chaf@mail.ustc.edu.cn (Wenbo Chen)</managingEditor>
    <webMaster>chaf@mail.ustc.edu.cn (Wenbo Chen)</webMaster>
    <copyright>&amp;copy;{year}, All Rights Reserved</copyright>
    <lastBuildDate>Tue, 10 Aug 2021 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://chaphlagical.github.io/en/posts/paperreading/index.xml" rel="self" type="application/rss+xml" />
    
    
    

      
      <item>
        <title>Fast Diffraction Pathfinding for Dynamic Sound Propagation</title>
        <link>https://chaphlagical.github.io/en/posts/paperreading/fast_diffraction_pathfinding_for_dynamic_sound_propagation/</link>
        <pubDate>Mon, 23 Aug 2021 22:20:00 +0100</pubDate>
        <author>chaf@mail.ustc.edu.cn (Wenbo Chen)</author>
        <atom:modified>Mon, 23 Aug 2021 22:20:00 +0100</atom:modified>
        <guid>https://chaphlagical.github.io/en/posts/paperreading/fast_diffraction_pathfinding_for_dynamic_sound_propagation/</guid>
        <description>Author: Carl Schissler, Gregor Mückl, Paul Calamia
Institution: Facebook Reality Labs Research, USA
Link: https://dl.acm.org/doi/10.1145/3450626.3459751
Abstract  Diffraction is one the the most perceptually important yet difficult to simulate acoustic effects  A phenomenon that allows sound to propagate around obstructions and corners   A significant bottleneck in real-time simulation of diffraction:  The enumeration of high-order diffraction propagation paths in scenes with complex geometry   The paper present a dynamic geometric diffraction approach that consists of an extensive mesh preprocessing pipeline and complementary runtime algorithm  Preprocessing module identifies a small subset of edges that are important for diffraction using a novel silhouette edge detection heuristic  It also extends these edges with planar diffraction geometry and precomputes a graph data structure encoding the visibility between the edges   The runtime module uses bidirectional path tracing against the diffraction geometry to probabilistically explore potential paths between sources and listeners, then evaluates the intensities for these paths using the Uniform Theory of Diffraction  It uses the edge visibility graph and the A* pathfinding algorithm to robustly and efficiently find additional high-order diffraction paths     The paper demonstrate how this technique can simulate 10th-order diffraction up to 568 times faster than the previous state of the art, and can efficiently handle large scenes with both high geometric complexity and high numbers of sources  1.</description>
        
        <dc:creator>Wenbo Chen</dc:creator>
        
        
        
        
          
            
              <category>Rendering</category>
            
          
        
        
          
            
              <category>paper reading</category>
            
          
        
        
      </item>
      
      <item>
        <title>Temporally Adaptive Shading Reuse for Real-time Rendering and Virtual Reality</title>
        <link>https://chaphlagical.github.io/en/posts/paperreading/temporally_adaptive_shading_reuse_for_real_time_rendering_and_virtual_reality/</link>
        <pubDate>Wed, 18 Aug 2021 15:44:00 +0100</pubDate>
        <author>chaf@mail.ustc.edu.cn (Wenbo Chen)</author>
        <atom:modified>Wed, 18 Aug 2021 15:44:00 +0100</atom:modified>
        <guid>https://chaphlagical.github.io/en/posts/paperreading/temporally_adaptive_shading_reuse_for_real_time_rendering_and_virtual_reality/</guid>
        <description>Author: JOERG H. MUELLER, THOMAS NEFF, PHILIP VOGLREITER, MARKUS STEINBERGER, DIETER SCHMALSTIEG
Institution: Graz University of Technology, Austria
Link: https://dl.acm.org/doi/10.1145/3446790
Abstract Motivation
 Temporal coherence has the potential to enable a huge reduction of shading costs in rendering  Current Work
 Existing techniques focus either only on spatial shading reuse or cannot adaptively choose temporal shading frequencies  Idea
 Temporal shading reuse is possible for extended periods of time for a majority of samples Approximate shading gradients to efficiently determine when and how long shading can be reused  1.</description>
        
        <dc:creator>Wenbo Chen</dc:creator>
        
        
        
        
          
            
              <category>Rendering</category>
            
          
        
        
          
            
              <category>paper reading</category>
            
          
        
        
      </item>
      
      <item>
        <title>GPU Accelerated Path Tracing of Massive Scenes</title>
        <link>https://chaphlagical.github.io/en/posts/paperreading/gpu_accelerated_path_tracing_of_massive_scenes/</link>
        <pubDate>Sun, 15 Aug 2021 21:34:00 +0100</pubDate>
        <author>chaf@mail.ustc.edu.cn (Wenbo Chen)</author>
        <atom:modified>Sun, 15 Aug 2021 21:34:00 +0100</atom:modified>
        <guid>https://chaphlagical.github.io/en/posts/paperreading/gpu_accelerated_path_tracing_of_massive_scenes/</guid>
        <description>Author: MILAN JAROŠ, LUBOMÍR ŘÍHA, PETR STRAKOŠ, and MATĚJ ŠPEŤKO
Institution: IT4Innovations, VSB–Technical University of Ostrava, Czech Republic
Link: https://dl.acm.org/doi/10.1145/3447807
1. Introduction GPUs Rendering vs CPUs Rendering
 Limited memory size Example: Pixar&amp;rsquo;s Coco movie were using up to 120GB, do not fit into the memory of a single GPU  Main contribution
  A solution to GPUs rendering memory size limitation
 Based on replication of a small amount of scene data, between 1% ~ 5%, and well-chosen distribution of the rest of the data into the memory of several GPUs Both replication and distribution of data is based on a memory access pattern analysis of a path tracer during a 1spp prepass The data with the highest number of memory accesses are replicated and the rest is stored only in the memory of the GPU that had the highest number of accesses to it Minimizes the penalty associated with reading data from the remote memory and is effective at providing near-linear scalability    Demonstration that our proposed approach works on a memory management level</description>
        
        <dc:creator>Wenbo Chen</dc:creator>
        
        
        
        
          
            
              <category>Rendering</category>
            
          
        
        
          
            
              <category>paper reading</category>
            
          
        
        
      </item>
      
      <item>
        <title>ExtraNet: Real-time Extrapolated Rendering for Low-latency Temporal Supersampling</title>
        <link>https://chaphlagical.github.io/en/posts/paperreading/extranet/</link>
        <pubDate>Tue, 10 Aug 2021 22:13:11 +0100</pubDate>
        <author>chaf@mail.ustc.edu.cn (Wenbo Chen)</author>
        <atom:modified>Tue, 10 Aug 2021 22:13:11 +0100</atom:modified>
        <guid>https://chaphlagical.github.io/en/posts/paperreading/extranet/</guid>
        <description>Abstract Real-time Rendering
 Frame rate Latency  Spatial Super Sampling: DLSS
 Decreasing the rendering time of each frame by rendering at a lower resolution  Temporal Super Sampling
 Producing more frames on the fly Problems:  It&amp;rsquo;s own computational cost The latency introduced by interpolating frames from the future    ExtraNet
  An efficient neural network that predicts accurate shading results on an extrapolated frame, to minimize both the performance overhead and the latency</description>
        
        <dc:creator>Wenbo Chen</dc:creator>
        
        
        
        
          
            
              <category>Rendering</category>
            
          
        
        
          
            
              <category>paper reading</category>
            
          
        
        
      </item>
      

    
  </channel>
</rss>