[{"content":"鼠标拾取是引擎场景编辑器的一个非常基础的功能，通过点击屏幕像素选择场景中的物体，能够使用户更加方便地选择物体、编辑场景。利用空闲时间我将IlumEngine场景编辑器的鼠标拾取功能做了一个优化，从原来的基于Ray Casting方法到现在所使用的G-Buffer回读的方法，两种方法各有优劣，下面详细介绍这两种鼠标拾取的方法。\n1. 基于Ray Casting的鼠标拾取方法 基于Ray Casting的鼠标拾取是一种几何方法，其基本原理如下：\n 由鼠标点击的屏幕像素坐标，生成一条从摄像机发射的射线 对场景作求交计算（与Ray Tracing中的相交检测相同） 寻找与光线相交的最近包围盒，其对应的物体即为鼠标将选中的物体  已知我们已从窗口/UI系统中得到鼠标点击的像素坐标click_pos，首先将其转化为屏幕空间坐标：\n1 2  float x = (click_pos.x / scene_view_size.x) * 2.f - 1.f; float y = -((click_pos.y / scene_view_size.y) * 2.f - 1.f);   我们希望利用拾取点的屏幕空间坐标从相机发射一条射线，一种思路是计算拾取点的远近平面投影坐标，然后将它们连起来即可：\n1 2 3 4 5 6 7 8 9 10  glm::mat4 inv = glm::inverse(main_camera.view_projection); glm::vec4 near_point = inv * glm::vec4(x, y, 0.f, 1.f); near_point /= near_point.w; glm::vec4 far_point = inv * glm::vec4(x, y, 1.f, 1.f); far_point /= far_point.w; geometry::Ray ray; ray.origin = main_camera.position; ray.direction = glm::normalize(glm::vec3(far_point - near_point));   最后，用射线做与包围盒的求交，求交的计算也可用BVH、KD-Tree等加速结构进行加速，这里为了快速实现只是简单遍历并对每个包围盒进行求交：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  float distance = std::numeric_limits\u0026lt;float\u0026gt;::infinity(); const auto group = Scene::instance()-\u0026gt;getRegistry().group\u0026lt;\u0026gt;(entt::get\u0026lt;cmpt::MeshRenderer, cmpt::Transform\u0026gt;); group.each([\u0026amp;](const entt::entity \u0026amp;entity, const cmpt::MeshRenderer \u0026amp;mesh_renderer, const cmpt::Transform \u0026amp;transform) { if (!Renderer::instance()-\u0026gt;getResourceCache().hasModel(mesh_renderer.model)) { return; } auto \u0026amp;model = Renderer::instance()-getResourceCache().loadModel(mesh_renderer.model); float hit_distance = ray.hit(model.get().bounding_box.transform(transform.world_transform)); if (distance \u0026gt; hit_distance) { distance = hit_distance; Editor::instance()-\u0026gt;select(Entity(entity)); } }); }   具体求交的计算后续光线追踪模块的开发将会提到，这里不作具体阐述。\n2. 基于G-Buffer回读的鼠标拾取方法 基于G-Buffer的鼠标拾取是一种图像方法，在几何阶段生成G-Buffers时我们顺带生成一张带有场景物体实体ID的G-Buffer，格式为VK_FORMAT_R32_UINT。在得到鼠标响应时，将该G-Buffer的数据回读到CPU中，利用像素坐标查找相应的实体ID，得到拾取到的对象。完整过程的源码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  ImageReference entity_id_buffer = Renderer::instance()-\u0026gt;getRenderGraph()-\u0026gt;getAttachment(\u0026#34;debug - entity\u0026#34;); CommandBuffer cmd_buffer; cmd_buffer.begin(); Buffer staging_buffer(static_cast\u0026lt;VkDeviceSize\u0026gt;(entity_id_buffer.get().getWidth() * entity_id_buffer.get().getHeight()) * sizeof(uint32_t), VK_BUFFER_USAGE_TRANSFER_DST_BIT, VMA_MEMORY_USAGE_GPU_TO_CPU); cmd_buffer.transferLayout(entity_id_buffer, VK_IMAGE_USAGE_SAMPLED_BIT, VK_IMAGE_USAGE_TRANSFER_SRC_BIT); cmd_buffer.copyImageToBuffer(ImageInfo{entity_id_buffer, VK_IMAGE_USAGE_TRANSFER_SRC_BIT, 0, 0}, BufferInfo{staging_buffer, 0}); cmd_buffer.transferLayout(entity_id_buffer, VK_IMAGE_USAGE_TRANSFER_SRC_BIT, VK_IMAGE_USAGE_SAMPLED_BIT); cmd_buffer.end(); cmd_buffer.submitIdle(); std::vector\u0026lt;uint32_t\u0026gt; image_data(entity_id_buffer.get().getWidth() * entity_id_buffer.get().getHeight()); std::memcpy(image_data.data(), staging_buffer.map(), image_data.size() * sizeof(uint32_t)); click_pos.x = glm::clamp(click_pos.x, 0.f, static_cast\u0026lt;float\u0026gt;(entity_id_buffer.get().getWidth())); click_pos.y = glm::clamp(click_pos.y, 0.f, static_cast\u0026lt;float\u0026gt;(entity_id_buffer.get().getHeight())); auto entity = Entity(static_cast\u0026lt;entt::entity\u0026gt;(image_data[static_cast\u0026lt;uint32_t\u0026gt;(click_pos.y) * entity_id_buffer.get().getWidth() + static_cast\u0026lt;uint32_t\u0026gt;(click_pos.x)])); Editor::instance()-\u0026gt;select(entity); staging_buffer.unmap();   由于G-Buffer的内存访问方式均为GPU_only的，我们需要使用一块GPU_to_CPU的Buffer进行暂存，最后Map到CPU内存中。\n3. 比较与选择  Ray Casting方法  优点  CPU实现，不依赖于渲染管线，能够很方便地集成   缺点  不够精确，由于是射线与包围盒求交，拾取的实际上是物体对应的包围盒而不是物体本身，有时候会带来误差，在场景复杂时效果不好 性能受场景规模影响较大，而使用加速结构进行求交加速实际上也增加了集成复杂度（需要引擎具有光追或物理模块支持）     G-Buffer方法  优点  精准，由于是直接把实体ID贴到纹理上，因此能够做到像素级的拾取   缺点  需要一张G-Buffer，增加了带宽开销 需要回读GPU数据，不过只有在鼠标点击时才会触发，影响并不大 需要渲染管线支持，需要配合整个渲染系统进行设计      在开发前期，Render Graph还不够完善，渲染管线扩展能力一般，为了简便，我先直接用Ray Casting的方法给IlumEngine加上一个基本能用的拾取方法，后来为了拾取精度的需要，将拾取算法改为了基于G-Buffer方法。\n","description":"IlumEngine编辑器鼠标拾取原理","id":5,"section":"posts","tags":["IlumEngine"],"title":"场景编辑器：鼠标拾取","uri":"https://chaphlagical.github.io/zh/posts/rendering/mouse_picking/"},{"content":"前段时间初步完成了个人图形引擎IlumEngine的第一次性能优化。此次优化主要集中在几何渲染与纹理系统上，主要内容大致有：\n Vertex/Index Buffer Packing GPU Driven Rendering  GPU Based Culling  Frustum Culling Hierarchy Z Buffer Occlusion Culling Cone Back Face Culling   Multi Draw Indirect Bindless Texture System    利用现代API灵活的可操作性，解决了几何阶段大量DrawCalls带来的CPU压力，以及提高顶点、索引数据的利用率，并且该架构也有利于后续集成实时光线追踪等功能\n1. IlumEngine简介 IlumEngine是我目前正在开发的一个玩具图形引擎，名字取自*《星球大战》*中凯伯水晶的产地伊冷（后传中被第一军团改造为弑星者基地），引擎使用Vulkan作为图形API（后续或重构为RHI层以支持DX12甚至向下兼容OpenGL），目的是锻炼软件系统工程能力和作为学习图形学经典技术复现和前沿技术的实验平台，预期功能：\n  Render Graph高灵活度渲染管线架构\n  基于ImGui的交互友好的编辑器\n  异步资源加载系统（或进化至流式加载系统）\n  集成基本几何造型算法\n Bezier曲线 三次样条曲线 B样条曲线 有理样条曲线 有理样条曲面等    集成基本数字几何处理算法\n 网格参数化 网格简化与细分 网格变形等    集成基本物理模拟算法\n 刚体模拟 布料模拟 柔性体模拟 流体模拟    集成基本光栅渲染算法\n Forward/Deferred/Tile Based渲染管线 实时阴影  PCF、PCSS VSM CSM   环境光照：IBL、PRT 全局光照  DDGI VXGI等   屏幕空间后处理  Blooming SSGI SSR等      集成基本离线渲染算法\n PT PM BDPT等    目前已将基础的架构部分搭建完成，能够支持Disney PBR材质的延迟渲染管线：\n接下来几节将从存储优化、CPU负载优化、GPU负载优化方面来介绍此次引擎优化的主要内容。\n2. 几何缓存优化 在介绍引擎的几何缓存优化之前，先介绍一下目前引擎使用的场景图和几何模型存储结构。\n2.1. 场景图 场景图是渲染引擎中重要的一个部分，通常采用树状结构（有向无环图）进行组织，IlumEngine中使用基于entt的实体组件系统ECS来实现场景图：\n 每个实体（Entity）作为场景图中的一个结点 每个实体可以挂上若干个组件（Component） 实体只是拓扑关系的结点，不存储实际数据 组件仅存储数据，而不存储逻辑（函数、方法） 组件中的数据由系统（System）使用，实现场景图的更新  想让一个实体拥有几何数据，则将该实体挂载上MeshRenderer组件，在渲染循环系统中，将从所有实体的MeshRenderer中获取渲染所需的几何数据，以完成几何阶段的渲染。\n2.2. 几何模型存储结构 组件MeshRenderer定义如下：\n1 2 3 4 5 6  struct MeshRenderer { std::string model; std::vector\u0026lt;scope\u0026lt;IMaterial\u0026gt;\u0026gt; materials; inline static bool update = false; };   其中，\n model为模型数据的索引，这里使用模型文件所在位置的相对路径表示 materials为模型的材质，初始化时将拷贝为模型的默认材质，在编辑器中也可对某个实体的材质进行修改 update为全局静态更新变量，表示在某循环中与MeshRenderer相关的更新  为得到实际的几何数据，我们还需要利用索引model在资源管理器ResourceCache中查询几何模型，ResourceCache实现了模型与贴图的多线程异步加载和缓存查询等功能，这里不作展开。通过查询，将得到实际模型对象的引用ModelReference：\n1  using ModelReference = std::reference_wrapper\u0026lt;Model\u0026gt;;   而Model便是我们实际存储几何数据的对象了，Model中又有如下数据：\n1 2 3 4 5 6 7  struct Model { std::vector\u0026lt;SubMesh\u0026gt; submeshes; geometry::BoundingBox bounding_box; VertexInfo vertex_info; IndexInfo index_info; }   其中，Submesh为模型的子网格，为导入方便以及支持单个模型不同部分使用不同材质，IlumEngine采用了子网格的形式来组织大型模型，每个子网格拥有以下信息：\n1 2 3 4 5 6 7 8 9  struct Submesh { uint32_t index = 0; glm::mat4 pre_transform = glm::mat4(1.f); material::DisneyPBR material; geometry::BoundingBox bounding_box; VertexInfo vertex_info; IndexInfo index_info; }   其实和Model是差不多的，Submesh是目前几何数据渲染的最小独立单位。\n2.3. 几何存储方案 上文已介绍了几何模型的一个存储结构，但是并未涉及具体的几何数据存储方案，所谓的几何数据存储方案，一个是CPU端的存储，即顶点和索引数据的存储；一个是GPU端的存储，即Vertex Buffer与Index Buffer的存储。对于静态网格模型而已，完全可以将几何数据送入GPU后删除CPU端的数据，但由于本引擎后续需要加入几何处理的功能，为了方便起见依旧全部保留CPU端的几何信息。\n2.3.1. 极简方案 最简单的一种也是最直观的一种策略，便是每个子网格存一份位置的几何信息，即：\n1 2 3 4 5 6 7 8 9 10 11  struct Submesh { uint32_t index = 0; glm::mat4 pre_transform = glm::mat4(1.f); material::DisneyPBR material; geometry::BoundingBox bounding_box; std::vector\u0026lt;Vertex\u0026gt; vertices; std::vector\u0026lt;uint32_t\u0026gt; indices; Buffer vertex_buffer; Buffer index_buffer; }   这种方法最为简单直观，也方便编程，但在实际渲染过程中会有渲染状态频繁切换的问题。假设有$N$个模型，第$i$个模型具有$M_i$个子网格，那么在一个渲染循环内需要做以下操作：\n  遍历$N$个模型\n  遍历模型$i$中的$M_i$个子网格\n  绑定子网格对应的GPU资源\n1 2  vkCmdBindVertexBuffers(cmd_buffer, 0, 1, \u0026amp;vertex_buffer, 0); vkCmdBindIndexBuffer(cmd_buffer, index_buffer, 0, VK_INDEX_TYPE_UINT32);     执行渲染绘制操作\n1  vkCmdDrawIndexed(cmd_buffer, index_count, 1, 0, 0, 0);     因此在一个渲染循环中需要切换绑定$N*M$次顶点/索引缓冲，当模型数量增加时会明显影响效率，而且多块小存储空间也不是一种好的存储分配策略，容易带来内存碎片等问题，同时，当模型具有多个重复的子网格时，这种存储策略将造成数据冗余，降低存储资源的利用率\n2.3.2. 基于模型的优化方案 既然子网格存储所有的几何数据不太好，那我就每个模型存储一份几何数据，然后子网格只存偏移和长度咯。基于模型的优化方案也确实是这样的设计思路：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  struct Model { std::vector\u0026lt;SubMesh\u0026gt; submeshes; geometry::BoundingBox bounding_box; std::vector\u0026lt;Vertex\u0026gt; vertices; std::vector\u0026lt;uint32_t\u0026gt; indices; Buffer vertex_buffer; Buffer index_buffer; } struct Submesh { uint32_t index = 0; glm::mat4 pre_transform = glm::mat4(1.f); material::DisneyPBR material; geometry::BoundingBox bounding_box; uint32_t indices_offset; uint32_t indices_count; }   模型中存储了所有子网格的几何数据，通过顶点索引的偏移offset和顶点索引数量count即可绘制出相应的子网格。由于目前的索引均从顶点缓冲的开头开始，因此暂不需要vertex_offset的参与。\n假设有$N$个模型，第$i$个模型具有$M_i$个子网格，那么在一个渲染循环内需要做以下操作：\n  遍历$N$个模型\n  绑定模型对应的GPU资源\n1 2  vkCmdBindVertexBuffers(cmd_buffer, 0, 1, \u0026amp;vertex_buffer, 0); vkCmdBindIndexBuffer(cmd_buffer, index_buffer, 0, VK_INDEX_TYPE_UINT32);     遍历模型$i$中的$M_i$个子网格\n  执行渲染绘制操作\n1  vkCmdDrawIndexed(cmd_buffer, index_count, 1, index_offset, 0, 0);     因此在一个渲染循环中需要切换绑定$N$次顶点/索引缓冲，比前述的极简方案要好不少，同时模型存储也避免了多个重复子网格冗余的问题，相同的子网格只要有相同的索引偏移和数量即可。\n2.3.3. 统一存储的优化方案 基于模型的方案在渲染每个模型时依然需要切换绑定顶点索引缓冲，在模型数量很多时同样可能带来瓶颈，同时也不利于我们后面进行GPU Driven Rendering的single drawcall设计。所以这次一劳永逸，分配一个大块的GPU显存资源来存储所有的顶点和索引缓冲，而CPU端的几何数据则仍按基于模型的方案设计。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  struct Model { std::vector\u0026lt;SubMesh\u0026gt; submeshes; geometry::BoundingBox bounding_box; std::vector\u0026lt;Vertex\u0026gt; vertices; std::vector\u0026lt;uint32_t\u0026gt; indices; uint32_t indices_offset; uint32_t vertices_offset; } struct Submesh { uint32_t index = 0; glm::mat4 pre_transform = glm::mat4(1.f); material::DisneyPBR material; geometry::BoundingBox bounding_box; uint32_t vertex_offset; uint32_t indices_offset; uint32_t indices_count; } class Renderer { Buffer vertex_buffer; Buffer index_buffer; ... }   该方案的麻烦之处在于，每当有模型添加、修改或删除时需要对全局缓冲进行更新，同时也需要更新每个模型的偏移。下图为各个存储索引关系示例：\n现在，假设有$N$个模型，第$i$个模型具有$M_i$个子网格，那么在一个渲染循环内需要做以下操作：\n  绑定几何数据对应的GPU资源\n1 2  vkCmdBindVertexBuffers(cmd_buffer, 0, 1, \u0026amp;vertex_buffer, 0); vkCmdBindIndexBuffer(cmd_buffer, index_buffer, 0, VK_INDEX_TYPE_UINT32);     遍历$N$个模型\n  遍历模型$i$中的$M_i$个子网格\n  执行渲染绘制操作\n1  vkCmdDrawIndexed(cmd_buffer, index_count, 1, index_offset, vertex_offset, 0);     现在，我们彻底地将几何资源绑定次数降低至single bind，无论场景多大，模型数量多少我们均只需单次绑定开销，而在后续的GPU Driven Rendering的管线设计中，我们也会看到这种Vertex/Index Buffer packing的方法具有的巨大优势。\n3. GPU驱动渲染管线 GPU Driven Rendering Pipelien的概念最早在Siggraph2015上由育碧Ubisoft提出[1]，其相应技术也已在《刺客信条：大革命》中得以落地，在当时可以说是相当前卫的一种设计，但由于当年硬件条件所限，《刺客信条：大革命》却因为层出不穷的Bug被当时的玩家所诟病，一度将育碧和刺客信条系列推向低谷，不过回过头看，《刺客信条：大革命》确实在大型场景和复杂建筑、海量NPC、真实感渲染等方面都是前作所不能比拟的，可以算是3A大作进入画质内卷的一个分界线。\n在IlumEngine中，我也尝试了使用GPU Driven Rendering Pipeline的思想，来对引擎进行性能调优。\n3.1. 无绑定纹理 Bindless方法指不通过传统方法将资源通过bindTexture/bindBuffer的方式进行绑定，而是直接将Texture/Buffer等GPU资源的虚拟地址直接存储在Bindless Buffer中，在着色器中可以直接使用索引进行访问。Bindless技术最早来源于Nvidia提出的 AZDO（Approaching Zero Driver Overhead）技术框架，2008年Nvidia的Tesla架构就已经实现了Bindless Buffer，而在2012年的Kepler架构正式加入了Bindless Texture特性。\n对于传统的绑定模型，我们往往需要在着色器中声明所需要的纹理/缓冲资源，并且分配相应的槽位（slot）：\n1 2 3  layout (binding = 0) uniform sampler2D tex0; layout (binding = 1) uniform sampler2D tex1; layout (binding = 2) uniform sampler2D tex2;   在CPU端，需要显式绑定所有纹理资源：\n而使用Bindless绑定模型，在着色器中，我们相当于使用了一个无穷大的纹理数组：\n1  layout (binding = 0) uniform sampler2D textureArray[];   所有的纹理数据可以一次性全部灌入其中，需要用到时，我们只需要一个下标索引即可进行访问，而对于材质而言，也不再像下图那样的贴图绑定：\n1 2 3  layout (binding = 0) uniform sampler2D Albedo; layout (binding = 1) uniform sampler2D Metallic; layout (binding = 2) uniform sampler2D Roughness;   而是使用一个结构体，存储所有的材质贴图索引：\n1 2 3 4 5 6  struct Material { uint Albedo; uint Metallic; uint Roughness; }   访问时只需：\n1  vec4 albedo = texture(textureArray[nonuniformEXT(material.Albedo)], inUV);   即可。对于GLSL，记得开启扩展GL_EXT_nonuniform_qualifier\nBindless访问模型如下：\nBindless对GPU Driven Rendering Pipeline有至关重要的作用，它主要解决了传统API下绑定资源到管线的开销问题，同时突破了着色器的硬件访问限制，进一步降低CPU-GPU的交互，我们不需要在CPU端设置Bindless资源的绑定状态，是之后实现single drawcall for everything的基础。\nVulkan中与Bindless相关的技术叫descriptor_indexing，在Vulkan 1.0属于EXT特性，但在Vulkan 1.2中已升为Core特性。在Logical Device的创建时指定：\n1 2 3 4 5 6 7  VkPhysicalDeviceVulkan12Features vulkan12_features = {}; vulkan12_features.sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VULKAN_1_2_FEATURES; vulkan12_features.shaderSampledImageArrayNonUniformIndexing = VK_TRUE; vulkan12_features.runtimeDescriptorArray = VK_TRUE; vulkan12_features.descriptorBindingVariableDescriptorCount = VK_TRUE; vulkan12_features.descriptorBindingPartiallyBound = VK_TRUE;   其中，shaderSampledImageArrayNonUniformIndexing、runtimeDescriptorArray、descriptorBindingVariableDescriptorCount指定开启descriptor_indexing特性，descriptorBindingPartiallyBound解决了缺省绑定的问题。\n在创建Bindless Texture过程中，需要指定Bindless Texture的数组支持的最大容量，通常会指定为一个较大的数（如1024）以避免反复扩容，而大部分情况下场景中的纹理都不会填满最大容量，此时需要开启descriptorBindingPartiallyBound支持缺省绑定，以防止出错。\nBindless Texture可视化：\n至此，我们又将一个费时的操作从CPU端移走了。\n3.2. 多重间接绘制 此前的一章一节中，我们将几何数据资源绑定的CPU开销降至最低，将纹理资源绑定的CPU开销给完全移走了，在本节中，我们将要把绘制开销降至最低，实现心心念念的single drawcall for everything。\n在最开始的设计中，我以一种非常低效的方式进行几何阶段的渲染，流程如下：\n 绑定Pipeline、DescriptorSet、Vertex/Index Buffer 遍历模型 遍历子网格 收集材质信息，使用Push Constant操作将材质数据送往着色器 调用绘制命令  可以看到，每一个子网格都将贡献一次Push Constant开销和一次Drawcall的开销，更不用说其他的逻辑判断操作，结果可想而知，场景复杂度一上去，CPU开销裂开，非常不贴合现代图形API的设计初衷，我们需要更多类似Bindless Texture的低CPU开销设计。\n好在现代图形API已经帮我们考虑好了，多重间接绘制Multi Draw Indirect能够完美地满足我们的需求。不同于显式调用绘制命令，Multi Draw Indirect允许我们实现将绘制命令预存在GPU的显存中，在需要绘制时调用：\n1  vkCmdDrawIndexedIndirect(cmd_buffer, draw_buffer, buffer_size, draw_count, sizeof(VkDrawIndexedIndirectCommand));   一个Drawcall即可完成所有的绘制指令提交。\n下面介绍多重间接绘制相关的技术细节：\n3.2.1. 指令缓冲 前述中，Multi Draw Indirect使用我们预存在GPU显存中的绘制命令进行提交，这些绘制命令存储在指令缓冲。在Vulkan中，有结构体VkDrawIndexedIndirectCommand或VkDrawIndirectCommand帮助我们指定指令缓冲中需要存哪些信息，一般来讲我们使用索引进行绘制，因此用的是VkDrawIndexedIndirectCommand，其数据结构定义为：\n1 2 3 4 5 6 7  typedef struct VkDrawIndexedIndirectCommand { uint32_t indexCount; uint32_t instanceCount; uint32_t firstIndex; int32_t vertexOffset; uint32_t firstInstance; } VkDrawIndexedIndirectCommand;   是不是和我们显式绘制指令的参数不能说很像，只能说一模一样？\n1 2 3 4 5 6 7  void vkCmdDrawIndexed( VkCommandBuffer commandBuffer, uint32_t indexCount, uint32_t instanceCount, uint32_t firstIndex, int32_t vertexOffset, uint32_t firstInstance);   当然用法也一样，就是把之前要在渲染循环里指定的参数一一写入一个std::vector\u0026lt;VkDrawIndexedIndirectCommand\u0026gt;容器里，然后在把里面的数据传到GPU显存中，使用其缓冲句柄即可调用vkCmdDrawIndexedIndirect了。\n3.2.2. 材质缓冲 前文提到过，在一开始的实现中，我们将材质信息使用Push Constant的方法在几何遍历时传到着色器中，而使用多重间接绘制时我们不再需要遍历几何体，没办法使用Push Constant方法传送逐子网格数据，因此我们需要一种新的传送材质数据的方法。\n这里我也采用了一种比较暴力的方法，那就是将所有的材质数据都存在一个大的Storage Buffer，鉴于材质数据结构中只需存各个贴图的索引和一些简单的参数，需要的显存并不算多，在每帧循环时，根据需要进行更新。\n由于材质信息是每个子网格拥有一份（不支持多材质、分层材质等），连同如预变换（Pre-Transform，与模型变换矩阵相乘构成世界变换矩阵）、包围盒等信息组成PerInstanceData：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  struct PerInstanceData { mat4 world_transform; mat4 pre_transform; vec4 base_color; vec3 emissive_color; float metallic_factor; float roughness_factor; float emissive_intensity; uint albedo_map; uint normal_map; uint metallic_map; uint roughness_map; uint emissive_map; uint ao_map; vec3 min_; float displacement_height; vec3 max_; uint displacement_map; };   （变量顺序是为了内存对齐需要）\n在着色器中，通过扩展GL_ARB_shader_draw_parameters，能够获得当前绘制物体的索引gl_DrawIDARB，由此来访问相应的PerInstanceData。\n这样一来，我们也顺利地使用一个DrawCall完成了所有的绘制指令提交，实际实验结果也很让人满意，CPU开销有了显著的降低，耗时仅为原来的十分之一不到，CPU也不再成为了渲染的瓶颈。\n3.3. 基于GPU的剔除 在前面几节中，我们已经彻底完成了CPU端的瓶颈解除，但我们也不应止步于此，接下来将进行GPU端的性能优化。要想在不减少场景规模的前提下减少GPU的计算耗时，一个基本的想法就是告诉GPU哪些东西是需要渲染的、哪些东西是不需要渲染的，也就是本节的主角——剔除技术了。\n剔除的本质是一种可见性测试，最常见的剔除有两种：视锥体剔除和遮挡剔除，这两种剔除方法能够排除大量不可见的可渲染物体，当然还有小片元剔除、背面剔除等其他方法。\n在传统管线中，通常采用CPU进行剔除处理，通过SSIM等硬件加速手段提高求交检测来实现各种剔除技术。但在本GPU Driven Rendering Pipeline中，我们已经将所有渲染数据和参数放在显存上了，很自然地，我们将利用现代GPU的通用计算功能（GPGPU），使用计算着色器来帮助我们完成剔除的操作。\n3.3.1. 视锥剔除 在学习计算机图形学基础时，我们都会接触到相机投影等相关知识，简单来讲，相机投影定义了一个裁剪空间，对于正交相机，其平截头体是一个长方体：\n而对于透视相机，其平截头体是一个台体\n在平截头体（或视锥体）之外的顶点将在裁剪阶段被渲染管线丢弃，尽管这些顶点不会参与最后的光栅化阶段，但还是会在顶点着色器中进行计算处理，造成不必要的性能浪费。通过视锥剔除计算，在不可见物体送入渲染管线前就进行排除，能够提高我们的计算效率和计算资源利用率。\n视锥剔除的检测，即包围盒与视锥平面的求交检测，这里涉及两个步骤：视锥平面的求算与包围盒的求交。\n视锥平面的求算\n在IlumEngine中，视锥平面的计算使用了Gribb-Hartmann方法进行求解，其详细数学推导可参考[2]。\n已知当前场景主摄像机的投影矩阵为$M_{P}$，视图矩阵为$M_V$，定义投影视图矩阵：\n$$\nM_{PV}=M_PM_V=\\begin{bmatrix}\nm_{11}\u0026amp;m_{12}\u0026amp;m_{13}\u0026amp;m_{14}\\\\\nm_{21}\u0026amp;m_{22}\u0026amp;m_{23}\u0026amp;m_{24}\\\\\nm_{31}\u0026amp;m_{32}\u0026amp;m_{33}\u0026amp;m_{34}\\\\\nm_{41}\u0026amp;m_{42}\u0026amp;m_{43}\u0026amp;m_{44}\n\\end{bmatrix}\n$$\n则六个视锥面方程如下：\n$$\n\\begin{aligned}\n\\begin{matrix}\n\\mathrm{Left: }\u0026amp;(m_{41}+m_{11})x+(m_{42}+m_{12})y+(m_{43}+m_{13})z+(m_{44}+m_{14})=0\\\\\n\\mathrm{Right: }\u0026amp;(m_{41}-m_{11})x+(m_{42}-m_{12})y+(m_{43}-m_{13})z+(m_{44}-m_{14})=0\\\\\n\\mathrm{Bottom: }\u0026amp;(m_{41}+m_{21})x+(m_{42}+m_{22})y+(m_{43}+m_{23})z+(m_{44}+m_{24})=0\\\\\n\\mathrm{Top: }\u0026amp;(m_{41}-m_{21})x+(m_{42}-m_{22})y+(m_{43}-m_{23})z+(m_{44}-m_{24})=0\\\\\n\\mathrm{Near: }\u0026amp;(m_{41}+m_{31})x+(m_{42}+m_{32})y+(m_{43}+m_{33})z+(m_{44}+m_{34})=0\\\\\n\\mathrm{Far: }\u0026amp;(m_{41}-m_{31})x+(m_{42}-m_{32})y+(m_{43}-m_{33})z+(m_{44}-m_{34})=0\\\\\n\\end{matrix}\n\\end{aligned}\n$$\n包围盒的求交\nIlumEngine中使用了包围球和AABB包围盒两种包围结构，包围球具有旋转不变性、求交方便等优点，AABB包围盒的紧致性比包围球更胜一筹，可以提高更细粒度的剔除。\n对于点$\\pmb p(\\pmb p_x, \\pmb p_y,\\pmb p_z)$，设视锥平面为$a_ix+b_iy+c_iz+d_i=0$，$(i=0,1,\\cdots,6)$，则点$\\pmb p$处于视锥体内，当且仅当：\n$$\na_i\\pmb p_x+b_i\\pmb p_y+c_i\\pmb p_z+d_i\u0026lt;0,\\forall i=0,1,\\cdots,6\n$$\n成立。\n对于球心坐标为点$\\pmb p(\\pmb p_x, \\pmb p_y,\\pmb p_z)$，半径为$r$的包围球，当且仅当：\n$$\na_i\\pmb p_x+b_i\\pmb p_y+c_i\\pmb p_z+d_i+r\u0026lt;0,\\forall i=0,1,\\cdots,6\n$$\n成立时，物体不会被剔除，代码如下所示：\n1 2 3 4 5 6 7 8 9 10 11  bool checkSphere(vec3 pos, float radius) { for (uint i = 0; i \u0026lt; 6; i++) { if (dot(camera.frustum[i], vec4(pos, 1)) + radius \u0026lt; 0.0) { return false; } } return true; }   对于两端点为$\\pmb p_{max}$和$\\pmb p_{min}$的AABB包围盒，我们可以组合出八个包围盒顶点和视锥体的六个平面分别求交，但这样计算量太大，也没必要，对于每个视锥平面，我们只需挑选出其中离它最远的那个顶点进行判断即可，最远顶点可有下述公式决定：\n$$\n\\begin{matrix}\n\\pmb p_x=\\begin{cases}\np_{min_x}\u0026amp;a_i\u0026lt;0\\\\\np_{max_x}\u0026amp;\\mathrm{otherwise}\n\\end{cases}\n\u0026amp;\n\\pmb p_y=\\begin{cases}\np_{min_y}\u0026amp;b_i\u0026lt;0\\\\\np_{max_y}\u0026amp;\\mathrm{otherwise}\n\\end{cases}\n\u0026amp;\n\\pmb p_z=\\begin{cases}\np_{min_z}\u0026amp;c_i\u0026lt;0\\\\\np_{max_z}\u0026amp;\\mathrm{otherwise}\n\\end{cases}\n\\end{matrix}\n$$\n使用该点进行判断即可。代码如下所示：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  bool checkAABB(vec3 min_val, vec3 max_val) { for (uint i=0; i \u0026lt; 6; i++) { vec4 plane = camera.frustum[i]; vec3 plane_normal = { plane.x, plane.y, plane.z }; float plane_constant = plane.w; vec3 axis_vert = { 0.0, 0.0, 0.0 }; axis_vert.x = plane.x \u0026lt; 0.0 ? min_val.x : max_val.x; axis_vert.y = plane.y \u0026lt; 0.0 ? min_val.y : max_val.y; axis_vert.z = plane.z \u0026lt; 0.0 ? min_val.z : max_val.z; if (dot(axis_vert, plane_normal) + plane_constant \u0026gt; 0.0) { return false; } } return true; }   3.3.2. Hierarchy Z-Buffer遮挡剔除 除了视锥剔除，遮挡剔除也是一种重要的剔除技术，当场景中有大量体积较大的遮挡物时有比较好的性能提升效果。遮挡剔除的实现手段有很多种，有使用硬件的遮挡查询策略，不过开销较大一般不建议使用；有手动指定Occlude和Occluder，使用CPU低分辨率软光栅进行剔除（参考Battlefield 3实现）；也有通过离线烘培的方法来实现遮挡剔除。在IlumEngine中则利用帧间信息连续性的原理，使用前一帧的深度缓冲，利用计算着色器生成层级Mipmap，再通过屏幕空间包围结构在计算着色器中实现遮挡剔除。\nHierarchy Z-Buffer的生成\n有了深度缓冲，要计算某物体是否被遮挡，一个直接的想法就是在深度图中采样该物体所在的像素位置，比较深度图采样值和该物体实际的深度值，若采样值小于深度值，则认为物体在该像素下被遮挡，当然物体的实际深度和像素位置在光栅化之前是很难算出来的，我们可以用简单的几何体例如包围结构来代替实际的物体，但即便如此，进行全分辨率的搜索和比较也是一个相当耗时的操作，这也是我们为什么需要层级深度缓冲的原因：通过包围结构在屏幕空间的投影大小，能够计算出相应的MipLevel，使得在该MipLevel下一个像素刚好能够覆盖全分辨率下包围结构的大小，这样一来，搜索和比较操作从原来的需要对包围结构所占像素逐个比对，优化至只需要搜索相应的MipLevel，通过一次采样即可完成比对。当然下采样也会带来一定的信息量损失，带来剔除精度的损失，但在剔除精度与开销之间的权衡，我们更倾向于后者。\n在每一帧的几何阶段中，我们都将使用一个格式为VK_FORMAT_D32_SFLOAT_S8_UINT的纹理来作为我们的Depth Stencil Buffer，由于深度图格式无法直接进行Mipmap操作，我们需要自己手动生成相应的Mipmap。\n出于框架的局限性，需要在每个渲染流程的结尾将Deth Stencil Buffer拷贝到另外一张深度贴图Last_Frame.depth_buffer中，在HizPass中，首先我们需要准备好Last_Frame.hiz_buffer的各个层级的VkImageView，以方便后续数据的写入：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  m_views.resize(Renderer::instance()-\u0026gt;Last_Frame.hiz_buffer-\u0026gt;getMipLevelCount()); VkImageViewCreateInfo image_view_create_info = {}; image_view_create_info.sType = VK_STRUCTURE_TYPE_IMAGE_VIEW_CREATE_INFO; image_view_create_info.viewType = VK_IMAGE_VIEW_TYPE_2D; image_view_create_info.format = VK_FORMAT_R32_SFLOAT; image_view_create_info.components = {VK_COMPONENT_SWIZZLE_R}; image_view_create_info.subresourceRange.layerCount = 1; image_view_create_info.image = *Renderer::instance()-\u0026gt;Last_Frame.hiz_buffer; for (uint32_t i = 0; i \u0026lt; m_views.size(); i++) { image_view_create_info.subresourceRange.aspectMask = VK_IMAGE_ASPECT_COLOR_BIT; image_view_create_info.subresourceRange.baseArrayLayer = 0; image_view_create_info.subresourceRange.layerCount = 1; image_view_create_info.subresourceRange.baseMipLevel = i; image_view_create_info.subresourceRange.levelCount = 1; vkCreateImageView(GraphicsContext::instance()-\u0026gt;getLogicalDevice(), \u0026amp;image_view_create_info, nullptr, \u0026amp;m_views[i]); }   出于保守剔除策略，在下采样过程中，我们不应使用线性过滤等方法进行处理，而是考虑选择一个$4\\times 4$ Texels中最大的值（不使用反向Z缓冲），在Vulkan中，可以使用Reduction Mode在VkSampler创建时指定VK_SAMPLER_REDUCTION_MODE_MAX：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  VkSamplerCreateInfo createInfo = {VK_STRUCTURE_TYPE_SAMPLER_CREATE_INFO}; createInfo.magFilter = VK_FILTER_LINEAR; createInfo.minFilter = VK_FILTER_LINEAR; createInfo.mipmapMode = VK_SAMPLER_MIPMAP_MODE_NEAREST; createInfo.addressModeU = VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE; createInfo.addressModeV = VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE; createInfo.addressModeW = VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE; createInfo.minLod = 0; createInfo.maxLod = 16.f; VkSamplerReductionModeCreateInfo createInfoReduction = {VK_STRUCTURE_TYPE_SAMPLER_REDUCTION_MODE_CREATE_INFO_EXT}; createInfoReduction.reductionMode = VK_SAMPLER_REDUCTION_MODE_MAX; createInfo.pNext = \u0026amp;createInfoReduction; vkCreateSampler(GraphicsContext::instance()-\u0026gt;getLogicalDevice(), \u0026amp;createInfo, 0, \u0026amp;m_hiz_sampler);   接着生成好需要用到的descriptor_sets：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45  for (uint32_t level = 0; level \u0026lt; m_views.size(); level++) { VkDescriptorImageInfo dstTarget; dstTarget.sampler = m_hiz_sampler; dstTarget.imageView = m_views[level]; dstTarget.imageLayout = VK_IMAGE_LAYOUT_GENERAL; VkDescriptorImageInfo srcTarget; srcTarget.sampler = m_hiz_sampler; if (level == 0) { srcTarget.imageView = Renderer::instance()-\u0026gt;Last_Frame.depth_buffer-\u0026gt;getView(ImageViewType::Depth_Only); srcTarget.imageLayout = VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL; } else { srcTarget.imageView = m_views[level - 1]; srcTarget.imageLayout = VK_IMAGE_LAYOUT_GENERAL; } std::vector\u0026lt;VkWriteDescriptorSet\u0026gt; write_descriptor_sets(2); write_descriptor_sets[0] = {}; write_descriptor_sets[0].sType = VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET; write_descriptor_sets[0].dstSet = m_descriptor_sets[level]; write_descriptor_sets[0].descriptorType = VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER; write_descriptor_sets[0].dstBinding = 0; write_descriptor_sets[0].pImageInfo = \u0026amp;srcTarget; write_descriptor_sets[0].pBufferInfo = nullptr; write_descriptor_sets[0].pTexelBufferView = nullptr; write_descriptor_sets[0].descriptorCount = 1; write_descriptor_sets[0].pNext = nullptr; write_descriptor_sets[1] = {}; write_descriptor_sets[1].sType = VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET; write_descriptor_sets[1].dstSet = m_descriptor_sets[level]; write_descriptor_sets[1].descriptorType = VK_DESCRIPTOR_TYPE_STORAGE_IMAGE; write_descriptor_sets[1].dstBinding = 1; write_descriptor_sets[1].pImageInfo = \u0026amp;dstTarget; write_descriptor_sets[1].pBufferInfo = nullptr; write_descriptor_sets[1].pTexelBufferView = nullptr; write_descriptor_sets[1].descriptorCount = 1; write_descriptor_sets[1].pNext = nullptr; m_descriptor_sets[level].update(write_descriptor_sets); }   每个descriptor_set规定了需要读取和写入的数据，在第一轮中，读取的应为上一帧的深度图，写入MipLevel为0的Hi-Z Buffer，而之后的每一轮都是读取MipLevel为$i$的Hi-Z Buffer，写入MipLevel为$i+1$的Hi-Z Buffer。\n至此准备阶段已经结束了，在每一轮渲染循环中，需要进行以下更新：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56  for (uint32_t level = 0; level \u0026lt; views.size(); level++) { { VkImageMemoryBarrier read_to_write{}; read_to_write.sType = VK_STRUCTURE_TYPE_IMAGE_MEMORY_BARRIER; read_to_write.oldLayout = VK_IMAGE_LAYOUT_GENERAL; read_to_write.newLayout = VK_IMAGE_LAYOUT_GENERAL; read_to_write.srcAccessMask = VK_ACCESS_SHADER_READ_BIT; read_to_write.dstAccessMask = VK_ACCESS_SHADER_WRITE_BIT; read_to_write.image = *Renderer::instance()-\u0026gt;Last_Frame.hiz_buffer; read_to_write.subresourceRange = VkImageSubresourceRange{VK_IMAGE_ASPECT_COLOR_BIT, level, 1, 0, 1}; read_to_write.srcQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED; read_to_write.dstQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED; vkCmdPipelineBarrier( cmd_buffer, VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT, VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT, VK_DEPENDENCY_BY_REGION_BIT, 0, nullptr, 0, nullptr, 1, \u0026amp;read_to_write); } vkCmdBindDescriptorSets(cmd_buffer, state.pass.bind_point, state.pass.pipeline_layout, 0, 1, \u0026amp;m_descriptor_sets[level].getDescriptorSet(), 0, nullptr); uint32_t level_width = std::max(1u, Renderer::instance()-\u0026gt;Last_Frame.hiz_buffer-\u0026gt;getWidth() \u0026gt;\u0026gt; level); uint32_t level_height = std::max(1u, Renderer::instance()-\u0026gt;Last_Frame.hiz_buffer-\u0026gt;getHeight() \u0026gt;\u0026gt; level); VkExtent2D extent = {level_width, level_height}; vkCmdPushConstants(cmd_buffer, state.pass.pipeline_layout, VK_SHADER_STAGE_COMPUTE_BIT, 0, sizeof(VkExtent2D), \u0026amp;extent); uint32_t group_count_x = (Renderer::instance()-\u0026gt;getRenderTargetExtent().width + 32 - 1) / 32; uint32_t group_count_y = (Renderer::instance()-\u0026gt;getRenderTargetExtent().height + 32 - 1) / 32; vkCmdDispatch(cmd_buffer, group_count_x, group_count_y, 1); { VkImageMemoryBarrier write_to_read{}; write_to_read.sType = VK_STRUCTURE_TYPE_IMAGE_MEMORY_BARRIER; write_to_read.oldLayout = VK_IMAGE_LAYOUT_GENERAL; write_to_read.newLayout = VK_IMAGE_LAYOUT_GENERAL; write_to_read.srcAccessMask = VK_ACCESS_SHADER_WRITE_BIT; write_to_read.dstAccessMask = VK_ACCESS_SHADER_READ_BIT; write_to_read.image = *Renderer::instance()-\u0026gt;Last_Frame.hiz_buffer; write_to_read.subresourceRange = VkImageSubresourceRange{VK_IMAGE_ASPECT_COLOR_BIT, level, 1, 0, 1}; write_to_read.srcQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED; write_to_read.dstQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED; vkCmdPipelineBarrier( cmd_buffer, VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT, VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT, VK_DEPENDENCY_BY_REGION_BIT, 0, nullptr, 0, nullptr, 1, \u0026amp;write_to_read); } }   每一次计算循环开始，需要切换相应的读写状态、绑定相应的descriptor_set，进行计算着色器的dispatch，最后再进行下一轮的状态切换准备，直到填满Hi-Z Buffer的所有Mipmap层级为止。\n屏幕空间包围结构的计算\n有了Hi-Z Buffer，现在我们需要得到物体包围结构屏幕空间的投影，为了计算方便，只考虑包围球形式，使用的计算方法参考文献[3]，该算法将世界空间的包围球变换为屏幕空间AABB包围盒。\n首先我们回顾一下图形学基础中视图矩阵和投影矩阵的相关概念，视图矩阵$M_{view}$主要作用是将场景变换到相机空间中，所谓的相机空间，就是以相机为原点所定义的空间，视图矩阵可以由相机的模型矩阵求逆得到：$M_{view}=M_{camera\\_position}^{-1}$，当然一般很少通过这种方法来求取视图矩阵，因为不够直观，而是通过摄像机的朝向和摄像机的位置来进行求取：\n$$\nM_{view}=\\begin{bmatrix}\n\\pmb R_x\u0026amp;\\pmb R_y\u0026amp;\\pmb R_z\u0026amp;0\\\\\n\\pmb U_x\u0026amp;\\pmb U_y\u0026amp;\\pmb U_z\u0026amp;0\\\\\n\\pmb D_x\u0026amp;\\pmb D_y\u0026amp;\\pmb D_z\u0026amp;0\\\\\n0\u0026amp;0\u0026amp;0\u0026amp;1\n\\end{bmatrix}\n\\ast\n\\begin{bmatrix}\n1\u0026amp;0\u0026amp;0\u0026amp;-\\pmb P_x\\\\\n0\u0026amp;1\u0026amp;0\u0026amp;-\\pmb P_y\\\\\n0\u0026amp;0\u0026amp;1\u0026amp;-\\pmb P_z\\\\\n0\u0026amp;0\u0026amp;0\u0026amp;1\n\\end{bmatrix}\n$$\n其中，$\\pmb R$为右向量，$\\pmb U$为上向量，$\\pmb D$为方向向量，$\\pmb P$为摄像机的位置向量。\n而投影矩阵则是实现将三维空间的物体投影到二维屏幕上，投影矩阵将相机空间中的顶点数据变换到裁剪空间中，最后通过透视除法变换到标准化设备坐标，这里以透视投影为例：\n从两个方向观察，由三角形近似可得：\n$$\n\\begin{align}\n\\dfrac{x_p}{x_e}\u0026amp;=\\dfrac{-n}{z_e}\\Rightarrow x_p=\\dfrac{-n\\cdot x_e}{z_e}=\\dfrac{n\\cdot x_e}{-z_e}\\\\\n\\dfrac{y_p}{y_e}\u0026amp;=\\dfrac{-n}{z_e}\\Rightarrow y_p=\\dfrac{-n\\cdot y_e}{z_e}=\\dfrac{n\\cdot y_e}{-z_e}\n\\end{align}\n$$\n注意到$x_p$和$y_p$的计算均需要除以一个$-z_e$，这与裁剪空间到NDC正则化的透视除法相对应：\n$$\n\\begin{align}\n\\begin{pmatrix}\nx_{clip}\\\\y_{clip}\\\\z_{clip}\\\\w_{clip}\n\\end{pmatrix}\n\u0026amp;=\\pmb M_{projection}\\cdot\n\\begin{pmatrix}\nx_{eye}\\\\y_{eye}\\\\z_{eye}\\\\w_{eye}\n\\end{pmatrix}\\\\\n\\begin{pmatrix}\nx_{ndc}\\\\y_{ndc}\\\\z_{ndc}\n\\end{pmatrix}\n\u0026amp;=\n\\begin{pmatrix}\nx_{clip}/w_{clip}\\\\\ny_{clip}/w_{clip}\\\\\nz_{clip}/w_{clip}\n\\end{pmatrix}\n\\end{align}\n$$\n这里的$w_{clip}$便是$-z_e$了，因此透视投影矩阵有如下形式：\n$$\n\\begin{align}\n\\begin{pmatrix}\nx_c\\\\\ny_c\\\\\nz_c\\\\\nw_c\n\\end{pmatrix}=\n\\begin{pmatrix}\n\\cdot \u0026amp; \\cdot \u0026amp; \\cdot \u0026amp; \\cdot\\\\\n\\cdot \u0026amp; \\cdot \u0026amp; \\cdot \u0026amp; \\cdot\\\\\n\\cdot \u0026amp; \\cdot \u0026amp; \\cdot \u0026amp; \\cdot\\\\\n0\u0026amp;0\u0026amp;-1\u0026amp;0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_e\\\\\ny_e\\\\\nz_e\\\\\nw_e\n\\end{pmatrix}\n\\end{align}\n$$\n下面我们需要把近平面坐标点$x_p$和$y_p$线性映射到NDC坐标$x_n$和$y_n$：$[l,r]\\Rightarrow [-1,1]$以及$[b,t]\\Rightarrow [-1,1]$\n$$\n\\begin{align}\n\\dfrac{x_n-(-1)}{1-(-1)}\u0026amp;=\\dfrac{x_p-l}{r-l}\\Rightarrow x_n=\\dfrac{2x_p}{r-l}-\\dfrac{r+l}{r-l}\\\\\n\\dfrac{y_n-(-1)}{1-(-1)}\u0026amp;=\\dfrac{y_p-b}{t-b}\\Rightarrow y_n=\\dfrac{2y_p}{t-b}-\\dfrac{t+b}{t-b}\n\\end{align}\n$$\n将$x_p$和$y_p$代入得\n$$\n\\begin{align}\nx_n=\\Big(\\dfrac{2n}{r-l}\\cdot x_e+\\dfrac{r+l}{r-l}\\cdot z_e \\Big)\\Big/-z_e\\\\\ny_n=\\Big(\\dfrac{2n}{t-b}\\cdot y_e+\\dfrac{t+b}{t-b}\\cdot z_e \\Big)\\Big/-z_e\n\\end{align}\n$$\n可填入透视投影矩阵：\n$$\n\\begin{align}\n\\begin{pmatrix}\nx_c\\\\\ny_c\\\\\nz_c\\\\\nw_c\n\\end{pmatrix}=\n\\begin{pmatrix}\n\\frac{2n}{r-l}\u0026amp;0\u0026amp;\\frac{r+l}{r-l}\u0026amp;0\\\\\n0\u0026amp;\\frac{2n}{t-b}\u0026amp;\\frac{t+b}{t-b}\u0026amp;0\\\\\n\\cdot \u0026amp; \\cdot \u0026amp; \\cdot \u0026amp; \\cdot\\\\\n0\u0026amp;0\u0026amp;-1\u0026amp;0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_e\\\\\ny_e\\\\\nz_e\\\\\nw_e\n\\end{pmatrix}\n\\end{align}\n$$\n由于$z_c$不依赖于$x_e$与$y_e$且与$z_e$和$w_e$成线性关系，设\n$$\n\\begin{align}\n\\begin{pmatrix}\nx_c\\\\\ny_c\\\\\nz_c\\\\\nw_c\n\\end{pmatrix}=\n\\begin{pmatrix}\n\\frac{2n}{r-l}\u0026amp;0\u0026amp;\\frac{r+l}{r-l}\u0026amp;0\\\\\n0\u0026amp;\\frac{2n}{t-b}\u0026amp;\\frac{t+b}{t-b}\u0026amp;0\\\\\n0\u0026amp;0\u0026amp;A\u0026amp;B\\\\\n0\u0026amp;0\u0026amp;-1\u0026amp;0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_e\\\\\ny_e\\\\\nz_e\\\\\nw_e\n\\end{pmatrix}\n\\end{align}\n$$\n即\n$$\nz_n=z_c/w_c=\\dfrac{Az_e+Bw_e}{-z_e}\n$$\n在视角空间中，$w_e=1$，因此$z_n=\\frac{Az_e+B}{-z_e}$，利用边界关系：\n$$\n\\begin{cases}\n\\dfrac{-An+B}{n}=-1\\\\\n\\dfrac{-Af+B}{f}=1\n\\end{cases}\n\\Rightarrow\n\\begin{cases}\nA=-\\dfrac{f+n}{f-n}\\\\\nB=-\\dfrac{2fn}{f-n}\n\\end{cases}\n$$\n因此完整的透视投影矩阵表示为：\n$$\n\\begin{pmatrix}\n\\frac{2n}{r-l}\u0026amp;0\u0026amp;\\frac{r+l}{r-l}\u0026amp;0\\\\\n0\u0026amp;\\frac{2n}{t-b}\u0026amp;\\frac{t+b}{t-b}\u0026amp;0\\\\\n0\u0026amp;0\u0026amp;\\frac{-(f+n)}{f-n}\u0026amp;\\frac{-2fn}{f-n}\\\\\n0\u0026amp;0\u0026amp;-1\u0026amp;0\n\\end{pmatrix}\n$$\n如果视锥体对称，即$t=-b$和$l=-r$，则可简化为：\n$$\n\\begin{pmatrix}\n\\frac{n}{r}\u0026amp;0\u0026amp;0\u0026amp;0\\\\\n0\u0026amp;\\frac{n}{t}\u0026amp;0\u0026amp;0\\\\\n0\u0026amp;0\u0026amp;\\frac{-(f+n)}{f-n}\u0026amp;\\frac{-2fn}{f-n}\\\\\n0\u0026amp;0\u0026amp;-1\u0026amp;0\n\\end{pmatrix}\n$$\n通常情况下我们会用参数$fovy$（$y$轴方向的视域角）、$aspect$（屏幕宽高比）、$near$（近平面）以及$far$（远平面）来构造透视投影矩阵，相关关系如下：\n$$\n\\begin{align}\nr-l\u0026amp;=width=2*near*aspect*tan(fovy/2)\\\\\nt-b\u0026amp;=height=2*near*tan(fovy/2)\n\\end{align}\n$$\n而对于正交投影，只需对各个方向作正则化即可：\n$$\n\\begin{cases}\n\\dfrac{x_n-(-1)}{1-(-1)}=\\dfrac{x_e-l}{r-l}\\\\\n\\dfrac{y_n-(-1)}{1-(-1)}=\\dfrac{y_e-b}{t-b}\\\\\n\\dfrac{z_n-(-1)}{1-(-1)}=\\dfrac{z_e-n}{f-n}\n\\end{cases}\n$$\n得到正交投影矩阵：\n$$\n\\begin{pmatrix}\n\\frac{2}{r-l}\u0026amp;0\u0026amp;0\u0026amp;-\\frac{r+l}{r-l}\\\\\n0\u0026amp;\\frac{2}{t-b}\u0026amp;0\u0026amp;-\\frac{t+b}{t-b}\\\\\n0\u0026amp;0\u0026amp;\\frac{-2}{f-n}\u0026amp;-\\frac{f+n}{f-n}\\\\\n0\u0026amp;0\u0026amp;0\u0026amp;1\n\\end{pmatrix}\n$$\n复习完视图矩阵和投影矩阵的相关概念后，我们正式计算世界空间中的包围球到屏幕空间AABB包围盒的投影。\n首先，需要将包围球投影到相机空间，这一步简单地乘上一个视图矩阵即可。\n求屏幕空间包围盒，即求包围球在投影面上的最大和最小坐标，从单方向看，如上图所示，若$\\hat a$表示$x$轴，欲求取点$T$的坐标，连线$OT$与圆$C$相切，在该二维平面上，有球心坐标$C(C_x,C_z)$，设$\\vec c=(C_x,C_y)$，$c=\\sqrt{C_x^2+C_y^2}$，则从相机到$T$的单位向量可由旋转得到：\n$$\n\\hat\\omega =\\begin{bmatrix}\\cos\\theta\u0026amp;\\sin\\theta\\\\-\\sin\\theta\u0026amp;\\cos\\theta\\end{bmatrix}\\frac{\\vec c}{|\\vec c|}\n$$\n而$T$到相机的距离也很容易求得：$d=\\sqrt{c^2-r^2}$，且$\\cos\\theta = \\frac{d}{c}$，$\\sin\\theta=\\frac{r}{c}$\n解得$T=O+\\hat \\omega d$，同理可求得点$B$的坐标，令$\\tilde \\theta = -\\theta$即可。\n点$B$和$T$即视图空间中，包围球在$x$轴方向上的最左点和最右点，我们还需要将其变换到裁剪空间中，进行归一化处理。\n由前述推导可知，透视投影过程可表示为如下形式：\n$$\n\\begin{pmatrix}\nP_{00}\u0026amp;0\u0026amp;0\u0026amp;0\\\\\n0\u0026amp;P_{11}\u0026amp;0\u0026amp;0\\\\\n0\u0026amp;0\u0026amp;P_{22}\u0026amp;P_{23}\\\\\n0\u0026amp;0\u0026amp;-1\u0026amp;0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_e\\\\y_e\\\\z_e\\\\1\n\\end{pmatrix}=\n\\begin{pmatrix}\nP_{00}x_e\\\\P_{11}y_e\\\\P_{22}z_e+P_{23}\\\\-z_e\n\\end{pmatrix}\n$$\n通过透视除法投影到NDC空间中：\n$$\n\\begin{aligned}\nx_n \u0026amp;= \\frac{Ax_e}{-z_e}\\\\\ny_n \u0026amp;= \\frac{By_e}{-z_e}\n\\end{aligned}\n$$\n最后通过简单的线性变换可以从NDC空间$[-1,1]$变换到UV空间$[0,1]$方便后续处理。\nGLSL实现如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  bool projectSphere(vec3 C, float r, float znear, float P00, float P11, out vec4 aabb) { if (-C.z \u0026lt; r + znear) return false; vec2 cx = C.xz; vec2 vx = vec2(sqrt(dot(cx, cx) - r * r), r); vec2 minx = mat2(vx.x, vx.y, -vx.y, vx.x) * cx; vec2 maxx = mat2(vx.x, -vx.y, vx.y, vx.x) * cx; vec2 cy = C.yz; vec2 vy = vec2(sqrt(dot(cy, cy) - r * r), r); vec2 miny = mat2(vy.x, vy.y, -vy.y, vy.x) * cy; vec2 maxy = mat2(vy.x, -vy.y, vy.y, vy.x) * cy; aabb = vec4(-minx.x / minx.y * P00, -miny.x / miny.y * P11, -maxx.x / maxx.y * P00, -maxy.x / maxy.y * P11); aabb = aabb.xwzy * vec4(0.5f, -0.5f, 0.5f, -0.5f) + vec4(0.5f); return true; }   深度值搜索与比较\n得到Hi-Z Buffer和屏幕空间包围盒后，便可开始最后的计算环节。首先通过屏幕空间包围盒的大小获得需要索引的MipLevel：\n1 2 3  float width = (aabb.z - aabb.x) * cullData.zbuffer_width; float height = (aabb.w - aabb.y) * cullData.zbuffer_height; float mip_level = floor(log2(max(width, height)));   通过包围盒中心位置来确定需要采样的UV坐标，为了保证不会误剔除，在其周围多采样几个像素：\n1 2 3 4 5  vec2 uv = (aabb.xy + aabb.zw) * 0.5; vec2 uv0 = aabb.xy; vec2 uv1 = aabb.zw; vec2 uv2 = aabb.xw; vec2 uv3 = aabb.zy;   选择深度最大的那个深度值，保守剔除策略：\n1 2 3 4 5  float depth = textureLod(hiz_buffer, uv, mip_level).r; depth = max(depth, textureLod(hiz_buffer, uv0, mip_level).r); depth = max(depth, textureLod(hiz_buffer, uv1, mip_level).r); depth = max(depth, textureLod(hiz_buffer, uv2, mip_level).r); depth = max(depth, textureLod(hiz_buffer, uv3, mip_level).r);   还需要将深度值通过逆透视除法还原到裁剪空间中，以避免浮点精度误差带来的剔除准确性降低，设采样得到的深度值为$z_n$，其裁剪空间对应的深度值为：\n$$\nz_c=\\frac{2f\\cdot n}{(f-n)\\cdot z_n-(f+n)}\n$$\n其中，$f$为远裁剪面距离，$n$为近裁剪面距离，注意这里得到的$z_c$为负，需要取反。\n在实际实验中，在摄像机远离物体的过程中可能会出现误剔除的情况，这是由于我们利用了上一帧的深度信息，而这一帧由于远离物体，深度变大，会导致自遮挡的现象出现，解决办法也很简单，我们需要保留上一帧的相机视图-投影矩阵，在计算用于比较的实际深度值时，应使用上一帧的相机参数，以解决自遮挡问题。\n完整的遮挡剔除代码如下所示：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40  float LinearizeDepth(float depth) { return -(2.0 * cullData.zfar * cullData.znear) / (cullData.zfar + cullData.znear - depth * (cullData.zfar - cullData.znear));\t} bool checkOcclusion(vec3 center, float radius) { vec3 dir = normalize(camera.position - center); vec4 sceen_space_center_last = camera.last_view_projection * vec4(center + dir*radius, 1.0); vec3 C = ((cullData.view) * vec4(center,1.0)).xyz; vec4 aabb; if(!projectSphere(C, radius, cullData.znear, cullData.P00, cullData.P11, aabb)) { return true; } float width = (aabb.z - aabb.x) * cullData.zbuffer_width; float height = (aabb.w - aabb.y) * cullData.zbuffer_height; float mip_level = floor(log2(max(width, height))); vec2 uv = (aabb.xy + aabb.zw) * 0.5; vec2 uv0 = aabb.xy; vec2 uv1 = aabb.zw; vec2 uv2 = aabb.xw; vec2 uv3 = aabb.zy; float depth = textureLod(hiz_buffer, uv, mip_level).r; depth = max(depth, textureLod(hiz_buffer, uv0, mip_level).r); depth = max(depth, textureLod(hiz_buffer, uv1, mip_level).r); depth = max(depth, textureLod(hiz_buffer, uv2, mip_level).r); depth = max(depth, textureLod(hiz_buffer, uv3, mip_level).r); float depthSphere = abs(sceen_space_center_last.z); return LinearizeDepth(depth) \u0026gt;= depthSphere; }   3.4. 基于Meshlet的渲染优化 前文中我们已经实现了基本的基于GPU的剔除优化技术，在多数场景下都能带来一定的性能增益，但目前我们还只是以子网格为单位进行的剔除与提交，当子网格较大时仍存在不少计算资源的浪费，由于我们已经使用了GPGPU技术来帮助我们完成剔除与绘制的操作，模型的数量和Drawcall已不再是我们的性能瓶颈，一个很自然的想法便是能否将一个大的网格切分为诸多一定规则的小网格，从而提高剔除的粒度，同时由于我们使用了GPU强大的并行处理能力进行剔除，对它们进行处理也不将成为问题，因此，我们引入了Mesh Shader中Meshlet的概念，只是我们为了平台灵活性，不打算用Mesh Shader进行处理，而是使用计算着色器来帮助我们完成相同的功能。\nMeshlet是网格划分为小块的单位，是Mesh Shader处理的基本单元，传统的顶点着色器是逐顶点处理模型的，而Mesh Shader则支持逐Meshlet处理模型。通常来讲，每个Meshlet具有相同的顶点数以及支持的最大三角形数，NVIDIA建议选取顶点数为64，三角形数为124的Meshlet进行处理，Meshlet的生成需要通过离线工具构建，IlumEngine中使用了开源库meshoptimizer进行处理，meshoptimizer使用非常方便，文档齐全，这里不多讲其使用。如下封面图所示，我们已经成功将整个场景分成了大量小网格块：\n加入了Meshlet支持的模型存储方式与之前也基本一致，区别在于每个Model对象需要维护其所有的Meshlet，每个子网格需要存储其拥有的Meshlet的偏移和数量，Meshlet结构体定义如下：\n1 2 3 4 5 6 7  struct Meshlet { meshopt_Bounds bounds; uint32_t indices_offset; uint32_t indices_count; uint32_t vertices_offset; };   meshopt_Bounds为meshoptimizer的包围体结构，使用的是包围球结构以及用于锥体背面剔除的相关参数。\n现在的Model和Submesh结构体定义如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  struct Model { public: std::vector\u0026lt;SubMesh\u0026gt; submeshes; uint32_t vertices_count = 0; uint32_t indices_count = 0; uint32_t vertices_offset = 0; uint32_t indices_offset = 0; // Raw geometry, original data  std::vector\u0026lt;Vertex\u0026gt; vertices; std::vector\u0026lt;uint32_t\u0026gt; indices; // Meshlet, for mesh shading \u0026amp; cluster culling  std::vector\u0026lt;Meshlet\u0026gt; meshlets; geometry::BoundingBox bounding_box; } struct SubMesh { public: uint32_t index = 0; glm::mat4 pre_transform = glm::mat4(1.f); uint32_t vertices_count = 0; uint32_t indices_count = 0; uint32_t vertices_offset = 0; uint32_t indices_offset = 0; uint32_t meshlet_offset = 0; uint32_t meshlet_count = 0; material::DisneyPBR material; geometry::BoundingBox bounding_box; }   现在我们已经将最小渲染单位从子网格换成了Meshlet，原先的间接绘制、GPU剔除等操作照样进行。\n3.4.1. Meshlet渲染性能调优1：压缩合批 由于我们将每个网格都分成了众多Meshlet，在提高剔除粒度的同时也增加了渲染物的数量，同时用于间接绘制所需要的Draw_Buffer也会相应的变大，在之前的实现中，我们通过设置VkDrawIndexedIndirectCommand中的instanceCount参数来决定是否进行绘制，设为0时则表示被剔除。但在直接从子网格处理转换到Meshlet处理时，却发现当场景规模很大时，渲染效率反而没有原来的高，经过Profile发现瓶颈出在了几何阶段的GPU开销上，尽管有了一系列CPU优化和GPU剔除，CPU已经基本不需要花什么时间了，但绘制过程中的GPU用时居高不下，经分析是由于Draw_Buffer过大造成，光是遍历里头每一条渲染指令就已经花了GPU很多时间了。这个问题的解决方法也很简单：只提交需要进行绘制的指令。\n在之前的实现中，我们是在CPU端事先设置好所有的VkDrawIndexedIndirectCommand数组，传到GPU中，通过GPU可见性判断来设置instanceCount参数决定是否绘制该物体，这种方法造成了最后进行提交的绘制缓冲中有效指令和无效指令相互混叠，容易产生流水线气泡，如果能够对绘制缓冲中的指令进行排序，将有效指令移至渲染队列的头部，将绘制指令数量设置为有效指令的数量，则可以大幅提升性能。但在着色器中实现排序操作似乎不是一件简单的操作，因此我们另辟蹊径，在剔除管线中不仅设置可见性，我们直接在里头设置整个渲染指令。\n为方便渲染队列的构建和有效指令数量的跟踪，我们使用了一个Count_Buffer来记录有效绘制指令的数量，在计算着色器中，通过原子加法操作，实现类似push_back的功能：\n1 2 3 4 5 6 7 8 9 10  if (visible) { uint dci = atomicAdd(visible_count, 1); indirectDraws[dci].indexCount = meshlet.index_count; indirectDraws[dci].instanceCount = 1; indirectDraws[dci].firstIndex = meshlet.index_offset; indirectDraws[dci].vertexOffset = int(meshlet.vertex_offset); indirectDraws[dci].firstInstance = 0; draw_data[dci] = meshlet.instance_id; }   然而，在开发过程中，又遇到一个问题，如何获取Count_Buffer中的计数数据？使用GPU-CPU回读？那样在 一个渲染循环中只能获取到上一帧的计数结果。通过查阅文档发现，Vulkan开发者已经考虑过这个问题了，提供了vkCmdDrawIndexedIndirectCount函数，让我们能够直接使用Count_Buffer作为参数向GPU指定渲染指令数量。\n至此，Meshlet渲染带来的合批过大问题已被完美解决。\n3.4.2. Meshlet渲染性能调优2：层次剔除 简单使用Meshlet进行渲染，除了有合批过大的问题外，还有剔除开销的问题，当场景面数一多，Meshlet数量一大，剔除阶段的计算量也是不可忽视的。这里IlumEngine采用的解决方案是分层次进行剔除，其实也就是先对实例（子网格）进行剔除，再进行Meshlet进行剔除。已经在实例剔除阶段剔除的实例，其包含的Meshlet也就不用再进行剔除了，减少了剔除用时。事实上，层次剔除还可以更进一步的，通过构建不同层级的Meshlet BVH，进行高效地索引需要剔除的层级，能够实现更加高效的剔除策略，和LOD方法相结合，也就是虚幻引擎5中Nanite虚拟几何体的处理方法了。\n4. 结果 场景总览\n渲染管线\n无优化\n仅视锥剔除\n仅背面剔除\n仅遮挡剔除\n使用所有剔除\n演示Demo\n\n可以看到由于使用了帧间连续性的原因，在某些地方仍会有部分闪烁，后续引入TAA等帧间累积方法可以一定程度上解决这个问题，但在帧率上确实有了实质性的提升。\n参考链接 [1] Haar U, Aaltonen S. Gpu-driven rendering pipelines. Ubisoft, Siggraph 2015: Advances in Real-Time Rendering in Games course, 2015\n[2] Gribb G, Hartmann K. Fast extraction of viewing frustum planes from the worldview-projection matrix. Online document, 2001. \n[3] Mara, Michael, and Morgan McGuire. \u0026ldquo;2D polyhedral bounds of a clipped, perspective-projected 3D sphere.\u0026rdquo; JCGT. in submission 5 (2012).\n","description":"IlumEngine的第一次渲染性能优化","id":6,"section":"posts","tags":["Real-time Rendering"],"title":"实时渲染优化技术(1)","uri":"https://chaphlagical.github.io/zh/posts/rendering/optimization1/"},{"content":"Lazy Snapping是一种基于图的图像分割方法[1]，能够将图像分割为前景和背景，通过用户交互挑选前景和背景种子，利用最大流最小分割的方法对图像进行分割。\n1. 基本方法 文献中利用分水岭算法对图像进行预处理，这里我们直接将图像下采样到原分辨率的八分之一进行处理，然后求解以下优化问题：\n$$\nE(X)=\\sum_{i\\in\\mathcal{V}}E_1(x_i)+\\lambda\\sum_{(i,j)\\in\\mathcal\\varepsilon}E_2(x_i, x_j)\n$$\n其中，$E_1(x_i)$为似然能量，编码结点$x_i$的代价，$E_2(x_i,x_j)$为先验能量，代表邻接结点$x_i$和$x_j$的代价。\n似然能量$E_1(x_i)$定义为：\n$$\n\\begin{cases}\n\\begin{matrix}\nE_1(x_i=1)=0\u0026amp;E_1(x_i=0)=\\infty\u0026amp;\\forall i\\in\\mathcal{F}\n\\end{matrix}\\\\\\\\\n\\begin{matrix}\nE_1(x_i=1)=\\infty\u0026amp;E_1(x_i=0)=0\u0026amp;\\forall i\\in\\mathcal{B}\n\\end{matrix}\\\\\\\\\n\\begin{matrix}\nE_1(x_i=1)=\\frac{d_i^\\mathcal{F}}{d_i^\\mathcal{F}+d_i^\\mathcal{B}}\u0026amp;E_1(x_i=0)=\\frac{d_i^\\mathcal{B}}{d_i^\\mathcal{F}+d_i^\\mathcal{B}}\u0026amp;\\forall i\\in\\mathcal{U}\n\\end{matrix}\n\\end{cases}\n$$\n其中，$\\mathcal{F}$表示前景种子集合，$\\mathcal{B}$为背景种子集合，均由用户输入，而$\\mathcal{U}$为补集。$d_i^\\mathcal{F}$和$d_i^\\mathcal{B}$分别表示当前颜色与前景种子平均值和背景种子平均值的距离平方\n先验能量$E_2(x_i,x_j)$定义为：\n$$\nE_2(x_i,x_j)=\\frac{1}{|C(i)-C(j)|^2+\\varepsilon}\n$$\n实验中，取$\\lambda=100$，$\\epsilon=0.01$。\n然后构建无向图，通过最大流最小割方法分离出前景和背景，提取轮廓得到结果\n实验结果    原图像 交互图像 轮廓                     参考文献 [1] Y. Li, J. Sun, C.-K. Tang, and H.-Y. Shum. Lazy snapping. ACM Transactions on Graphics (ToG), 23(3):303–308, 2004.\n","description":"复现Siggraph2004论文《Lazy snapping》","id":7,"section":"posts","tags":["Digital Image Process"],"title":"Lazy Snapping","uri":"https://chaphlagical.github.io/zh/posts/image_process/lazy_snapping/"},{"content":"Seam Carving算法是一种基于内容的图像缩放方法，在保证图像中“重要区域”不发生形变的前提下，对图像进行缩放。\n一种直观的想法便是找出图像中的“不重要区域”，并将其删除。文献[1]中便是采用这种思想，\n通过定义像素的能量函数，通过动态规划方法对某一方向的像素进行能量累积，最后回溯求出能量最低的一条路径，该路径便是我们要删除的“最不重要”路径。\n1. 基本方法 图像的能量简单地由图像梯度描述：\n$$\ne_1(\\pmb I)=\\left|\\frac{\\partial}{\\partial x}\\pmb I\\right|+\\left|\\frac{\\partial}{\\partial y}\\pmb I\\right|\n$$\n论文中也给出另外一种能量的变体实现：\n$$\ne_{HoG}(\\pmb I)=\\frac{\\left|\\frac{\\partial}{\\partial x}\\pmb I\\right|+\\left|\\frac{\\partial}{\\partial y}\\pmb I\\right|}{\\max(HoG(\\pmb I(x,y)))}\n$$\n实现中，采用Sobel算子提取图像梯度作为能量图：\n1 2 3 4 5 6 7 8 9 10 11  void SeamCarving::genEnergyMap(const cv::Mat\u0026amp; img, cv::Mat\u0026amp; energy) { cv::Mat sobel_x, sobel_y, gray_energy; cv::cvtColor(img, gray_energy, cv::COLOR_BGR2GRAY); cv::Sobel(gray_energy, sobel_x, CV_32F, 1, 0, 3); cv::convertScaleAbs(sobel_x, sobel_x); cv::Sobel(gray_energy, sobel_y, CV_32F, 0, 1, 3); cv::convertScaleAbs(sobel_y, sobel_y); cv::addWeighted(sobel_x, 0.5, sobel_y, 0.5, 0, energy); energy.convertTo(energy, CV_32FC1); }   对于$n\\times m$的图像$\\pmb I$，用于删除的接缝由如下定义：\n水平接缝：\n$$\n\\pmb{s^x}=\\{s_i^x\\}_{i=1}^n=\\{(x(i),i)\\}_{i=1}^n,\\ \\\ns.t.\\ \\ \\forall i,\\ \\ |x(i)-x(i-1)|\\leq 1\n$$\n竖直接缝：\n$$\n\\pmb{s^y}=\\{s_i^y\\}_{j=1}^m=\\{(j, y(j))\\}_{j=1}^m,\\ \\\ns.t.\\ \\ \\forall j,\\ \\ |y(j)-y(j-1)|\\leq 1\n$$\n其中，$x:[1,\\cdots,n]\\rightarrow[1,\\cdots,m]$，$y:[1,\\cdots,m]\\rightarrow[1,\\cdots,n]$\n而我们要寻找最低能量路径即求解最小优化问题：\n$$\ns^\\ast=\\min_{\\pmb s}E(\\pmb s)=\\min_{\\pmb s}\\sum_{i=1}^ne(\\pmb I(s_i))\n$$\n利用动态规划的思想可以很方便地求出上述优化问题，利用能量累积矩阵$M$，转移方程：\n$$\nM(i,j)=e(i,j)+\\min(M(i-1,j-1), M(i-1,j), M(i-1,j+1))\n$$\n从边缘出发按转移方程填充能量累积矩阵，最后寻找能量最低的终点$M(n,x)$（竖直搜索）或$M(x,m)$（水平搜索），回溯即可得到完整的最优路径。\n1.1. 图像缩小任务 对于单方向缩小任务，只需重复上述接缝搜索流程，每次删除一条接缝即可。\n对于多方向缩小任务，文献中也将横向竖向接缝的选择顺序其视为一个优化问题，假设现有$m\\times n$的图像$\\pmb I$欲缩小至$m'\\times n'$，其中$m\u0026gt;m',n\u0026gt;n'$，接缝顺序的选择等价于优化以下能量函数：\n$$\n\\min_{\\pmb{s^x},\\pmb{s^y},\\alpha}\\sum_{i=1}^kE(\\alpha_i\\pmb {s_i^x}+(1-\\alpha_i)\\pmb{s_i^y})\n$$\n其中，$r=m-m'$，$c=n-n'$， $\\alpha_i\\in{0,1}$描述了接缝选择的方向，因此有$\\sum_{i=1}^k\\alpha_i=r$，$\\sum_{i=1}^k(1-\\alpha_i)=c$成立。该问题同样可以利用动态规划的想法进行求解，取能量累积矩阵$\\pmb T$，满足$\\pmb T(0,0)=0$，转移方程：\n$$\n\\pmb T(r,c)=\\min(\\pmb T(r-1,c),E(\\pmb s^x(\\pmb {I_{n-r-1\\times m-c}})),\\pmb T(r,c-1),E(\\pmb s^y(\\pmb {I_{n-r\\times m-c-1}})))\n$$\n$\\pmb {I_{n-r-1\\times m-c}}$表示大小为$n-r-1\\times m-c$的图像（中间量），$E(\\pmb{s^x}(\\pmb I))$和$E(\\pmb{s^y}(\\pmb I))$为相应的方向接缝删除后的能量。\n但上述方法实测速度很慢，因此在实现中选择简单的贪婪策略选择带来当前最低能量的方法。\n1.2. 图像拉伸任务 对于图像拉伸任务，同样可以采用缩小任务相似的处理方法，只是将最优接缝的删除修改为最优接缝邻域的插值，但和缩小任务不同的是，每次对单条最优接缝进行插值，容易导致后续的最优接缝\n搜索会集中在同一区域，因此在图像拉伸任务中，建议一次性选择多条低能量接缝进行插值。\n2. 实验结果 2.1. 图像缩小    原图像 图像能量 结果图像                     2.2. 图像拉伸    原图像 图像能量 最优接缝 结果图像                        3. 总结 从上图中可以看出 Seam Carving 的一些局限性：\n 进行图像拉伸任务时容易造成图像区块重复，可以考虑手动排除部分区域进行优化 在梯度变化不明显的“重要区域”容易造成误处理，比如：《蒙娜丽莎》大片的头发。可以考虑手动划分“重要区域”进行处理  参考文献 [1] S. Avidan and A. Shamir. Seam carving for content-aware image resizing. In ACM SIGGRAPH 2007 papers, pages 10–es. 2007\n","description":"复现Siggraph2007论文《Seam Carving for Content-Aware Image Resizing》","id":8,"section":"posts","tags":["Digital Image Process"],"title":"Seam Carving","uri":"https://chaphlagical.github.io/zh/posts/image_process/seam_carving/"},{"content":"1. 问题描述 给定以下两幅图：\n现我们需要将第一幅图中的女孩搬到第二幅图的海水中，为使得复制粘贴更加逼真自然，我们需要设计算法来满足我们两幅图像融合的需要\n2. 算法描述 Poisson Image Editing算法[1]的基本思想是在尽可能保持原图像内部梯度的前提下，让粘贴后图像的边界值与新的背景图相同，以实现无缝粘贴的效果。从数学上讲，对于原图像$f(x,y)$，新背景$f^(x,y)$和嵌入新背景后的新图像$v(x,y)$，等价于解最优化问题：\n$$\n\\min\\limits_f \\iint \\Omega |\\nabla f-\\boldsymbol v |^2 \\ \\ \\mathrm{with}\\ f|{\\partial \\Omega}=f^|_{\\partial \\Omega}\n$$\n利用变分法，令$F=|\\nabla f-\\boldsymbol v |^2=(\\nabla f_x-\\boldsymbol v_x)^2+(\\nabla f_y-\\boldsymbol v_y)^2$\n代入欧拉-拉格朗日方程：\n$$\nF_f-\\frac{\\mathrm d}{\\mathrm d x}F_{f_x}-\\frac{\\mathrm d}{\\mathrm d y}F_{f_y}=0\n$$\n由于$F$是关于$\\nabla f$的函数，因此$F_f=0$\n所以有：\n$$\n\\begin{align}\n\u0026amp;\\frac{\\partial F}{\\partial f}=\\frac{\\mathrm d}{\\mathrm dx}\\left[\\frac{\\partial F}{\\partial(\\nabla f_x-\\pmb v_x)^2}\\right]+\\frac{\\mathrm d}{\\mathrm dy}\\left[\\frac{\\partial F}{\\partial(\\nabla f_y-\\pmb v_y)^2}\\right]\\\\\\\\\n\u0026amp;\\Rightarrow 0=\\frac{\\mathrm d}{\\mathrm dx}[2(\\nabla f_x-\\pmb v_x)]+\n\\frac{\\mathrm d}{\\mathrm dy}[2(\\nabla f_y-\\pmb v_y)]\\\\\\\\\n\u0026amp;\\Rightarrow 0=\\left(\\frac{\\partial ^2f}{\\partial x^2}-\\frac{\\partial \\pmb v}{\\partial x}\\right)+\\left(\\frac{\\partial ^2f}{\\partial y^2}-\\frac{\\partial \\pmb v}{\\partial y}\\right)\\\\\\\\\n\u0026amp;\\Rightarrow \\Delta f=\\mathrm{div}\\pmb v\n\\end{align}\n$$\n可转化为具有Dirichlet边界条件的Poisson方程：\n$$\n\\Delta f= \\mathrm{div} \\boldsymbol v\\ \\ \\mathrm{over}\\ \\Omega\\ \\ \\mathrm{with} \\ \\ f|_{\\partial \\Omega}=f^\\ast|_{\\partial\\Omega}\n$$\n以第一幅图和第二幅图为例，将图1中需要复制的区域设为$S$，定义$N_p$为$S$中的每一个像素$p$四个方向连接邻域，令$\u0026lt;p,q\u0026gt;$为满足$q\\in N_p$的像素对。边界$\\Omega$定义为$\\partial \\Omega ={p\\in S\\setminus \\Omega: N_p \\cap \\Omega \\neq \\emptyset }$，设$f_p$为$p$处的像素值$f$，目标即求解像素值集$f|_\\Omega ={f_p,p\\in \\Omega}$\n利用Poisson Image Editing算法的基本原理，上述问题转化为求解最优化问题：\n$$\n\\min\\limits_{f|_\\Omega}\\sum\\limits_{\u0026lt;p,q\u0026gt;\\cap \\Omega\\neq \\emptyset}(f_p-f_q-v_{pq})^2,\\mathrm{with}\\ f_p=f_p^*,\\forall\\ p\\in \\partial\\Omega\r$$\n化为求解线性方程组：\n$$\n\\forall\\ p\\in \\Omega,\\ |N_p|f_p-\\sum\\limits_{q\\in N_p\\cap \\Omega} f_q=\\sum\\limits_{q\\in N_p\\cap \\partial \\Omega}f_p^*+\\sum\\limits_{q\\in N_p}v_{pq}\n$$\n对于梯度场$\\boldsymbol{v}(\\boldsymbol{x})$的选择，文献[1]给出两种方法，一种是完全使用前景图像的内部梯度，即：\n$$\n\\forall\\ \u0026lt;p,q\u0026gt;,v_{pq}=g_p-g_q\n$$\n另一种是使用混合梯度：\n$$\n\\forall\\ \\boldsymbol{x}\\in \\Omega,\\ \\boldsymbol{v}(\\boldsymbol{x})=\\begin{cases}\n\\nabla f^*(\\boldsymbol{x})\u0026amp;\\mathrm{if}\\ |\\nabla f^*(\\boldsymbol{x})\u0026gt;|\\nabla g(\\boldsymbol{x})|,\\\\\\\\\n\\nabla g(\\boldsymbol{x})\u0026amp;\\mathrm{otherwise}\n\\end{cases} $$\n**扫描线算法**\n为实现多边形和自由绘制闭合图形区域的Poisson Image Editing算法，需通过扫描线算法获取多边形内部掩膜。这里从网上资料了解到一种有序边表法，其基本思想是定义边表ET和活动边表AET，ET记录当前扫描线与边的交点坐标、从当前扫描线到下一条扫描线间x的增量、该边所交的最高扫描线，AET记录只与当前扫描线相交的边的链表，通过迭代得到当前扫描线与待求多边形各边的交点，再利用奇偶检测法判断该点是否在多边形内部进行填充。\n3. 实验结果 3.1. 标准图像测试 原图像：\n新背景图像：\n   边界形式 图1选择区域 图2选择区域 结果图像     矩形边界      多边形边界      自由绘制边界       3.2. Poisson vs. Mix Poisson 背景图像：\n前景图像：\n混合结果：\n如图，左上为直接复制粘贴，保留前景全部颜色梯度信息；左下为普通Poisson编辑，保留前景全部梯度信息，前景像素颜色与背景作融合；右上为应用混合梯度的Poisson编辑，前景梯度部分保留，效果上比普通Poisson编辑更加“透明”，适合用在水印等场景。\n3.3. 其他应用 3.3.1. 遮盖不必要的信息（如去皱纹） 原图像：\n处理效果：\n3.3.2. 恐怖片特效 原图：\n掩盖镜子中人物：\n使用电影《修女》中的角色：\n处理效果：\n3.3.3. 生成表情包 原图：\n处理效果：\n参考文献 [1] Patrick Pérez, Michel Gangnet, Andrew Blake. Poisson image editing. Siggraph 2003.\n","description":"复现Siggraph2003论文《Poisson Image Editing》","id":9,"section":"posts","tags":["Digital Image Process"],"title":"Poisson Image Editing","uri":"https://chaphlagical.github.io/zh/posts/image_process/possion/"},{"content":"1. 算法原理 1.1. 基本原理  输入：$n$对控制点对$(\\pmb p_i,\\pmb q_i)$，$i=1,2,\\cdots,n$，其中$\\pmb p_i\\in\\mathbb R^2$为控制起始点，$\\pmb q_i\\in\\mathbb{R}^2$为控制目标点 目标：找到一个映射$f:\\mathbb R^2\\rightarrow \\mathbb{R}^2$，满足$f(\\pmb p_i)=\\pmb q_i$，$i=1,2,\\cdots,n$  1.2. Inverse distance-weighted interpolation methods(IDW)[1] IDW 算法基本原理是根据给定的控制点对和控制点对的位移矢量，计算控制点对周围像素的反距离加权权重影响，实现图像每一个像素点的位移。\n选择$n$对控制点对$(\\pmb p_i,\\pmb q_i)$，$i=1,2,\\cdots,n$，目标映射$f:\\mathbb R^2\\rightarrow \\mathbb{R}^2$可表示成\n以下形式：\n$$\nf(\\pmb p)=\\sum_{i=1}^n\\omega_i(\\pmb p)f_i(\\pmb p)\n$$\n其中，权重$\\omega_i(\\pmb p)$满足：\n$$\nw_i(\\pmb p)=\\frac{\\sigma_i(\\pmb p)}{\\sum_{j=1}^n\\sigma_j(\\pmb p)}\n$$\n$\\sigma_i(\\pmb p)$反映第$i$对控制点对像素$\\pmb p$得反距离加权权重影响程度，可以直接取：\n$$\n\\sigma_i(\\pmb p)=\\frac{1}{|\\pmb p-\\pmb p_i|^\\mu}\n$$\n其中$\\mu\u0026gt;1$，也可以取locally bounded weight：\n$$\n\\sigma_i(\\pmb p)=\\left[\\frac{R_i-d(\\pmb p,\\pmb p_i)}{R_id(\\pmb p,\\pmb p_i)}\\right]^\\mu\n$$\n$f_i$为线性函数，满足：\n$$\nf_i(\\pmb p)=\\pmb q_i+\\pmb T_i(\\pmb p-\\pmb p_i)\n$$\n其中$\\pmb T_i$为二阶矩阵：\n$$\n\\pmb T_i=\\begin{bmatrix}\nt_{11}^{(i)}\u0026amp;t_{12}^{(i)}\\\\\nt_{21}^{(i)}\u0026amp;t_{22}^{(i)}\n\\end{bmatrix}\n$$\n矩阵$\\pmb T$得确定，可以通过求解如下最优化问题：\n$$\n\\arg\\min_{\\pmb T_i} E(\\pmb T_i)=\\sum_{j=1,j\\neq i}^n\\sigma_i(\\pmb p_j)|\\pmb q_j-f_i(\\pmb p_j)|^2\n$$\n上式对$\\pmb T_i$求导，令方程为0得：\n$$\n\\pmb T_i\\sum_{j=1,j\\neq i}\\sigma_i(\\pmb p_j)\\pmb p\\pmb p^T=\\sum_{j=1,j\\neq i}\\sigma_i(\\pmb p_j)\\pmb q\\pmb p^T\n$$\n其中$\\pmb p=\\pmb p_j-\\pmb p_i$，$\\pmb q=\\pmb q_j-\\pmb q_i$\n又$\\sigma_i(\\pmb p_j)\\pmb p\\pmb p^T$非奇异，因此可以直接解出$\\pmb T_i$的值：\n$$\n\\pmb T_i=\\left(\\sum_{j=1,j\\neq i}\\sigma_i(\\pmb p_j)\\pmb q\\pmb p^T\\right)\\left(\\sum_{j=1,j\\neq i}\\sigma_i(\\pmb p_j)\\pmb p\\pmb p^T\\right)^{-1}\n$$\n求出$\\pmb T_i(i=1,\\cdots,n)$后，映射$f$也就相应确定\n1.3. Radial basis functions interpolation method(RBF)[2] 选择$n$对控制点对$(\\pmb p_i,\\pmb q_i)$，$i=1,2,\\cdots,n$，目标映射$f:\\mathbb R^2\\rightarrow \\mathbb{R}^2$可表示为以下形式：\n$$\nf(\\pmb p)=\\sum_{i=1}^n\\alpha_ig_i(|\\pmb p-\\pmb p_i|)+\\pmb{Ap}+\\pmb B\n$$\n其中，$g_i$为径向基函数，通常可以取Hardy multiquadrics：$g(t)=(t^2+c^2)^{\\pm \\frac{1}{2}}$或高斯函数$g_\\sigma(t)=e^{-t^2/\\sigma^2}$，\n为了计算方便，这里取Hardy multiquadrics：\n$$\n\\begin{aligned}\ng_i(d)\u0026amp;=(d+r_i)^{\\pm\\frac{1}{2}}\\\nr_i\u0026amp;=\\min_{j\\neq i}d(\\pmb p_i,\\pmb p_j)\n\\end{aligned}\n$$\n对于线性部分分量$\\pmb {Ap}+\\pmb B$，本例简单地取$\\pmb A=\\pmb I$和$\\pmb B=\\pmb 0$\n2. 实验结果 2.1. 标准图像测试 如下图所示，固定四角，蓝色为控制起始点，绿色为控制终止。\n2.2. IDW算法    $\\mu$取值 修复前 修复后     $\\mu=-1$     $\\mu=-2$      2.3. RBF算法    $\\mu$取值 修复前 修复后     $\\mu=-0.5$     $\\mu=0.5$      2.4. 相关应用 2.4.1. 柴犬表情包 原图像：\n开心：\nEmmm……：\n2.4.2. 藏狐的笑容 原图像：\n处理后：\n参考文献 [1] D. Ruprecht and H. Muller. Image warping with scattered data interpolation. IEEE Computer Graphics and Applications, 15(2):37–43, 1995. 10\n[2] N. Arad and D. Reisfeld. Image warping using few anchor points and radial functions. In Computer graphics forum, volume 14, pages 35–46. Wiley Online Library, 1995.\n","description":"基于IDW与RBF方法的图像扭曲算法实现","id":10,"section":"posts","tags":["Digital Image Process"],"title":"Image Warping","uri":"https://chaphlagical.github.io/zh/posts/image_process/image_warping/"},{"content":"1. 问题描述 给定一张原图像：\n和一张参考图像：\n我们希望通过设计一个颜色转换算法，使得源图像具有目标图像的颜色风格，如下图：\n2. 算法描述 衡量一幅图像的颜色分布最基础的统计特征就是均值和标准差，文献[1]中便通过这两个特征对图像进行简单的变换并取得不错的效果。\n首先，需要将图像的颜色空间从$RGB$空间变换到$l\\alpha\\beta$空间：\n  $RGB$空间$\\rightarrow$ $XYZ$空间\n$$\n\\begin{pmatrix}\nX\\\\Y\\\\Z\n\\end{pmatrix}=\n\\begin{pmatrix}\n0.5141\u0026amp;0.3239\u0026amp;0.1604\\\\\n0.2651\u0026amp;0.6702\u0026amp;0.0641\\\\\n0.0241\u0026amp;0.1228\u0026amp;0.8444\n\\end{pmatrix}\n\\begin{pmatrix}\nR\\\\G\\\\B\n\\end{pmatrix}\n$$\n  $XYZ$空间$\\rightarrow$ $LMS$空间\n$$\n\\begin{pmatrix}\nL\\\\M\\\\S\n\\end{pmatrix}=\n\\begin{pmatrix}\n0.3897\u0026amp;0.6890\u0026amp;-0.0787\\\\\n-0.2298\u0026amp;1.1834\u0026amp;0.0464\\\\\n0.0000\u0026amp;0.0000\u0026amp;1.0000\n\\end{pmatrix}\n\\begin{pmatrix}\nX\\\\Y\\\\Z\n\\end{pmatrix}\n$$\n  变换到对数空间\n$$\n\\begin{aligned}\n\\pmb L\u0026amp;=\\lg L\\\\\n\\pmb M\u0026amp;=\\lg M\\\\\n\\pmb S\u0026amp;=\\lg S\n\\end{aligned}\n$$\n  $\\pmb{LMS}$空间$\\rightarrow$ $l\\alpha\\beta$空间\n$$\n\\begin{pmatrix}\nl\\\\\\alpha\\\\\\beta\n\\end{pmatrix}=\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{3}}\u0026amp;0\u0026amp;0\\\\\n0\u0026amp;\\frac{1}{\\sqrt{6}}\u0026amp;0\\\\\n0\u0026amp;0\u0026amp;\\frac{1}{\\sqrt{2}}\n\\end{pmatrix}\n\\begin{pmatrix}\n1\u0026amp;1\u0026amp;1\\\\\n1\u0026amp;1\u0026amp;-2\\\\\n1\u0026amp;-1\u0026amp;0\n\\end{pmatrix}\n\\begin{pmatrix}\n\\pmb L\\\\\\pmb M\\\\\\pmb S\n\\end{pmatrix}\n$$\n  同理，将图像的颜色空间从 lαβ 空间变换到 RGB 空间只需将相应的变换矩阵依次求逆即可。\n设变换到$l\\alpha\\beta$颜色空间的源图像的均值和标准差分别为$\\mu_s$、$\\sigma_s$，目标图像的均值和标准差分别为$\\mu_t$、$\\sigma_t$，只需通过如下简单处理，即可将源图像的颜色分布变换到接近与目标图像\n$$\n\\begin{aligned}\nl'\u0026amp;=\\frac{\\sigma_t^l}{\\sigma_s^l}(l-\\mu^l_s)+\\mu^l_t\\\\\n\\alpha'\u0026amp;=\\frac{\\sigma_t^\\alpha}{\\sigma_s^\\alpha}(\\alpha-mu^\\alpha_s)+\\mu^\\alpha_t\\\\\n\\beta'\u0026amp;=\\frac{\\sigma_t^\\beta}{\\sigma_s^\\beta}(\\beta-\\mu^\\beta_s)+\\mu^\\beta_t\\\n\\end{aligned}\n$$\n并将图像$(l',\\alpha',\\beta')$变换回$RGB$空间即可得到处理后的结果。\n3. 实验结果    原图像 参考图像 结果图像                               参考文献 [1] E. Reinhard, M. Adhikhmin, B. Gooch, and P. Shirley. Color transfer between images. IEEE Computer graphics and applications, 21(5):34–41, 2001.10\n","description":"复现论文《Color Transfer between Images》","id":11,"section":"posts","tags":["Digital Image Process"],"title":"Color Transfer","uri":"https://chaphlagical.github.io/zh/posts/image_process/color_transfer/"},{"content":"1. 问题描述 给定一张原图像：\n我们希望通过设计一个灰度图着色算法，使得下述灰度目标图像能够利用彩色原图像的颜色信息进行合理的上色：\n以得到类似下图的效果：\n2. 算法描述 实验参考了论文[1]，并使用全局匹配的方法进行求解。我们在图像的$l\\alpha\\beta$颜色空间上进行求解，这是由于$l\\alpha\\beta$颜色空间的$l$即代表着亮度，通过查找灰度源图像上的像素与彩色目标图像的$l$分量最佳匹配像素，再将目标图像对应匹配像素的$\\alpha$和$\\beta$分量赋予源图像即可\n进行灰度图像上色。算法流程如下：\n  将源图像$img_{src}$变换到$l\\alpha\\beta$颜色空间得到$img_{src}^{(l\\alpha\\beta)}$，同时将源图像$img_{src}$转换为灰度图$img_{src}^{(grey)}$\n  利用如下均值标准差变换，将$img_{src}^{(l\\alpha\\beta)}$的$l$通道和$img_{src}^{(grey)}$映射到具有灰度目标图像$img_{tar}$的像素值分布，得到$luminance_{src}$和$luminance_{src}^{grey}$\n$$\n\\begin{aligned}\nl'\u0026amp;=\\frac{\\sigma_t^l}{\\sigma_s^l}(l-\\mu^l_s)+\\mu^l_t\\\\\n\\alpha'\u0026amp;=\\frac{\\sigma_t^\\alpha}{\\sigma_s^\\alpha}(\\alpha-\\mu^\\alpha_s)+\\mu^\\alpha_t\\\\\n\\beta'\u0026amp;=\\frac{\\sigma_t^\\beta}{\\sigma_s^\\beta}(\\beta-\\mu^\\beta_s)+\\mu^\\beta_t\\\\\n\\end{aligned}\n$$\n  近邻域标准差计算。利用一个$n\\times n$（本实验中取$n=5)$）滑动窗口遍历图像（类似于卷积操作，边界用0填充），对窗口内的所有像素值计算它们的标准差并赋值到一幅标准差图像上。对$luminance_{src}^{grey}$和$img_{tar}$进行该计算，得到$stddev_{src}^{grey}$和$stddev_{tar}$\n  最佳匹配查找。遍历$img_{tar}$，已知$img_{tar}$上的某一像素值$p_{tar}$和其标准差$stddev_{tar}$对应的值$\\sigma_{tar}$，在源图像上寻找最佳匹配的像素点，这里简单地采用加权的$\\mathcal{L}_2$范数平方去算：\n$$\n\\arg\\min_{x,y} \\Big(w_1\\ast |luminance_{src}(x,y)-p_{tar}|^2+w_2\\ast|stddev_{src}^{grey}(x,y)-\\sigma_{tar}|^2\\Big)\n$$\n其中$w_1$和$w_2$分别表示亮度值和标准差的对匹配结果的贡献，这里简单地取$w_1=w_2=0.5$\n  灰度图上色。将查找到的最佳匹配像素$(\\tilde x. \\tilde y)$在$img_{src}^{(l\\alpha\\beta)}$所在像素值的$\\alpha$和$\\beta$分量赋值给源图像作为其$\\alpha$和$\\beta$通道的值，而原来的灰度值则作为$l$分量继续使用，并将源图像重新变换回$RGB$颜色空间得到最终的结果\n  3. 实验结果    原图像 参考图像 结果图像                               参考文献 [1] Welsh, M. Ashikhmin, and K. Mueller. Transferring color to greyscale im-ages. In Proceedings of the 29th annual conference on Computer graphics and interactive techniques, pages 277–280, 2002.11\n","description":"复现Siggraph2002论文《Transferring Color To Greyscale Image》","id":12,"section":"posts","tags":["Digital Image Process"],"title":"Colorization","uri":"https://chaphlagical.github.io/zh/posts/image_process/colorization/"},{"content":"我将在这里分享有趣的知识与技术\n","description":"My Blog","id":13,"section":"","tags":null,"title":"简介","uri":"https://chaphlagical.github.io/zh/about/"}]