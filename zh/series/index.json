[{"content":"前段时间初步完成了个人图形引擎IlumEngine的第一次性能优化。此次优化主要集中在几何渲染与纹理系统上，主要内容大致有：\n Vertex/Index Buffer Packing GPU Driven Rendering  GPU Based Culling  Frustum Culling Hierarchy Z Buffer Occlusion Culling Cone Back Face Culling   Multi Draw Indirect Bindless Texture System    利用现代API灵活的可操作性，解决了几何阶段大量DrawCalls带来的CPU压力，以及提高顶点、索引数据的利用率，并且该架构也有利于后续集成实时光线追踪等功能\n1. IlumEngine简介 IlumEngine是我目前正在开发的一个玩具图形引擎，名字取自*《星球大战》*中凯伯水晶的产地伊冷（后传中被第一军团改造为弑星者基地），引擎使用Vulkan作为图形API（后续或重构为RHI层以支持DX12甚至向下兼容OpenGL），目的是锻炼软件系统工程能力和作为学习图形学经典技术复现和前沿技术的实验平台，预期功能：\n  Render Graph高灵活度渲染管线架构\n  基于ImGui的交互友好的编辑器\n  异步资源加载系统（或进化至流式加载系统）\n  集成基本几何造型算法\n Bezier曲线 三次样条曲线 B样条曲线 有理样条曲线 有理样条曲面等    集成基本数字几何处理算法\n 网格参数化 网格简化与细分 网格变形等    集成基本物理模拟算法\n 刚体模拟 布料模拟 柔性体模拟 流体模拟    集成基本光栅渲染算法\n Forward/Deferred/Tile Based渲染管线 实时阴影  PCF、PCSS VSM CSM   环境光照：IBL、PRT 全局光照  DDGI VXGI等   屏幕空间后处理  Blooming SSGI SSR等      集成基本离线渲染算法\n PT PM BDPT等    目前已将基础的架构部分搭建完成，能够支持Disney PBR材质的延迟渲染管线：\n接下来几节将从存储优化、CPU负载优化、GPU负载优化方面来介绍此次引擎优化的主要内容。\n2. 几何缓存优化 在介绍引擎的几何缓存优化之前，先介绍一下目前引擎使用的场景图和几何模型存储结构。\n2.1. 场景图 场景图是渲染引擎中重要的一个部分，通常采用树状结构（有向无环图）进行组织，IlumEngine中使用基于entt的实体组件系统ECS来实现场景图：\n 每个实体（Entity）作为场景图中的一个结点 每个实体可以挂上若干个组件（Component） 实体只是拓扑关系的结点，不存储实际数据 组件仅存储数据，而不存储逻辑（函数、方法） 组件中的数据由系统（System）使用，实现场景图的更新  想让一个实体拥有几何数据，则将该实体挂载上MeshRenderer组件，在渲染循环系统中，将从所有实体的MeshRenderer中获取渲染所需的几何数据，以完成几何阶段的渲染。\n2.2. 几何模型存储结构 组件MeshRenderer定义如下：\n1 2 3 4 5 6  struct MeshRenderer { std::string model; std::vector\u0026lt;scope\u0026lt;IMaterial\u0026gt;\u0026gt; materials; inline static bool update = false; };   其中，\n model为模型数据的索引，这里使用模型文件所在位置的相对路径表示 materials为模型的材质，初始化时将拷贝为模型的默认材质，在编辑器中也可对某个实体的材质进行修改 update为全局静态更新变量，表示在某循环中与MeshRenderer相关的更新  为得到实际的几何数据，我们还需要利用索引model在资源管理器ResourceCache中查询几何模型，ResourceCache实现了模型与贴图的多线程异步加载和缓存查询等功能，这里不作展开。通过查询，将得到实际模型对象的引用ModelReference：\n1  using ModelReference = std::reference_wrapper\u0026lt;Model\u0026gt;;   而Model便是我们实际存储几何数据的对象了，Model中又有如下数据：\n1 2 3 4 5 6 7  struct Model { std::vector\u0026lt;SubMesh\u0026gt; submeshes; geometry::BoundingBox bounding_box; VertexInfo vertex_info; IndexInfo index_info; }   其中，Submesh为模型的子网格，为导入方便以及支持单个模型不同部分使用不同材质，IlumEngine采用了子网格的形式来组织大型模型，每个子网格拥有以下信息：\n1 2 3 4 5 6 7 8 9  struct Submesh { uint32_t index = 0; glm::mat4 pre_transform = glm::mat4(1.f); material::DisneyPBR material; geometry::BoundingBox bounding_box; VertexInfo vertex_info; IndexInfo index_info; }   其实和Model是差不多的，Submesh是目前几何数据渲染的最小独立单位。\n2.3. 几何存储方案 上文已介绍了几何模型的一个存储结构，但是并未涉及具体的几何数据存储方案，所谓的几何数据存储方案，一个是CPU端的存储，即顶点和索引数据的存储；一个是GPU端的存储，即Vertex Buffer与Index Buffer的存储。对于静态网格模型而已，完全可以将几何数据送入GPU后删除CPU端的数据，但由于本引擎后续需要加入几何处理的功能，为了方便起见依旧全部保留CPU端的几何信息。\n2.3.1. 极简方案 最简单的一种也是最直观的一种策略，便是每个子网格存一份位置的几何信息，即：\n1 2 3 4 5 6 7 8 9 10 11  struct Submesh { uint32_t index = 0; glm::mat4 pre_transform = glm::mat4(1.f); material::DisneyPBR material; geometry::BoundingBox bounding_box; std::vector\u0026lt;Vertex\u0026gt; vertices; std::vector\u0026lt;uint32_t\u0026gt; indices; Buffer vertex_buffer; Buffer index_buffer; }   这种方法最为简单直观，也方便编程，但在实际渲染过程中会有渲染状态频繁切换的问题。假设有$N$个模型，第$i$个模型具有$M_i$个子网格，那么在一个渲染循环内需要做以下操作：\n  遍历$N$个模型\n  遍历模型$i$中的$M_i$个子网格\n  绑定子网格对应的GPU资源\n1 2  vkCmdBindVertexBuffers(cmd_buffer, 0, 1, \u0026amp;vertex_buffer, 0); vkCmdBindIndexBuffer(cmd_buffer, index_buffer, 0, VK_INDEX_TYPE_UINT32);     执行渲染绘制操作\n1  vkCmdDrawIndexed(cmd_buffer, index_count, 1, 0, 0, 0);     因此在一个渲染循环中需要切换绑定$N*M$次顶点/索引缓冲，当模型数量增加时会明显影响效率，而且多块小存储空间也不是一种好的存储分配策略，容易带来内存碎片等问题，同时，当模型具有多个重复的子网格时，这种存储策略将造成数据冗余，降低存储资源的利用率\n2.3.2. 基于模型的优化方案 既然子网格存储所有的几何数据不太好，那我就每个模型存储一份几何数据，然后子网格只存偏移和长度咯。基于模型的优化方案也确实是这样的设计思路：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  struct Model { std::vector\u0026lt;SubMesh\u0026gt; submeshes; geometry::BoundingBox bounding_box; std::vector\u0026lt;Vertex\u0026gt; vertices; std::vector\u0026lt;uint32_t\u0026gt; indices; Buffer vertex_buffer; Buffer index_buffer; } struct Submesh { uint32_t index = 0; glm::mat4 pre_transform = glm::mat4(1.f); material::DisneyPBR material; geometry::BoundingBox bounding_box; uint32_t indices_offset; uint32_t indices_count; }   模型中存储了所有子网格的几何数据，通过顶点索引的偏移offset和顶点索引数量count即可绘制出相应的子网格。由于目前的索引均从顶点缓冲的开头开始，因此暂不需要vertex_offset的参与。\n假设有$N$个模型，第$i$个模型具有$M_i$个子网格，那么在一个渲染循环内需要做以下操作：\n  遍历$N$个模型\n  绑定模型对应的GPU资源\n1 2  vkCmdBindVertexBuffers(cmd_buffer, 0, 1, \u0026amp;vertex_buffer, 0); vkCmdBindIndexBuffer(cmd_buffer, index_buffer, 0, VK_INDEX_TYPE_UINT32);     遍历模型$i$中的$M_i$个子网格\n  执行渲染绘制操作\n1  vkCmdDrawIndexed(cmd_buffer, index_count, 1, index_offset, 0, 0);     因此在一个渲染循环中需要切换绑定$N$次顶点/索引缓冲，比前述的极简方案要好不少，同时模型存储也避免了多个重复子网格冗余的问题，相同的子网格只要有相同的索引偏移和数量即可。\n2.3.3. 统一存储的优化方案 基于模型的方案在渲染每个模型时依然需要切换绑定顶点索引缓冲，在模型数量很多时同样可能带来瓶颈，同时也不利于我们后面进行GPU Driven Rendering的single drawcall设计。所以这次一劳永逸，分配一个大块的GPU显存资源来存储所有的顶点和索引缓冲，而CPU端的几何数据则仍按基于模型的方案设计。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  struct Model { std::vector\u0026lt;SubMesh\u0026gt; submeshes; geometry::BoundingBox bounding_box; std::vector\u0026lt;Vertex\u0026gt; vertices; std::vector\u0026lt;uint32_t\u0026gt; indices; uint32_t indices_offset; uint32_t vertices_offset; } struct Submesh { uint32_t index = 0; glm::mat4 pre_transform = glm::mat4(1.f); material::DisneyPBR material; geometry::BoundingBox bounding_box; uint32_t vertex_offset; uint32_t indices_offset; uint32_t indices_count; } class Renderer { Buffer vertex_buffer; Buffer index_buffer; ... }   该方案的麻烦之处在于，每当有模型添加、修改或删除时需要对全局缓冲进行更新，同时也需要更新每个模型的偏移。下图为各个存储索引关系示例：\n现在，假设有$N$个模型，第$i$个模型具有$M_i$个子网格，那么在一个渲染循环内需要做以下操作：\n  绑定几何数据对应的GPU资源\n1 2  vkCmdBindVertexBuffers(cmd_buffer, 0, 1, \u0026amp;vertex_buffer, 0); vkCmdBindIndexBuffer(cmd_buffer, index_buffer, 0, VK_INDEX_TYPE_UINT32);     遍历$N$个模型\n  遍历模型$i$中的$M_i$个子网格\n  执行渲染绘制操作\n1  vkCmdDrawIndexed(cmd_buffer, index_count, 1, index_offset, vertex_offset, 0);     现在，我们彻底地将几何资源绑定次数降低至single bind，无论场景多大，模型数量多少我们均只需单次绑定开销，而在后续的GPU Driven Rendering的管线设计中，我们也会看到这种Vertex/Index Buffer packing的方法具有的巨大优势。\n3. GPU驱动渲染管线 GPU Driven Rendering Pipelien的概念最早在Siggraph2015上由育碧Ubisoft提出，其相应技术也已在《刺客信条：大革命》中得以落地，在当时可以说是相当前卫的一种设计，但由于当年硬件条件所限，《刺客信条：大革命》却因为层出不穷的Bug被当时的玩家所诟病，一度将育碧和刺客信条系列推向低谷，不过回过头看，《刺客信条：大革命》确实在大型场景和复杂建筑、海量NPC、真实感渲染等方面都是前作所不能比拟的，可以算是3A大作进入画质内卷的一个分界线。\n在IlumEngine中，我也尝试了使用GPU Driven Rendering Pipeline的思想，来对引擎进行性能调优。\n3.1. 无绑定纹理 Bindless方法指不通过传统方法将资源通过bindTexture/bindBuffer的方式进行绑定，而是直接将Texture/Buffer等GPU资源的虚拟地址直接存储在Bindless Buffer中，在着色器中可以直接使用索引进行访问。Bindless技术最早来源于Nvidia提出的 AZDO（Approaching Zero Driver Overhead）技术框架，2008年Nvidia的Tesla架构就已经实现了Bindless Buffer，而在2012年的Kepler架构正式加入了Bindless Texture特性。\n对于传统的绑定模型，我们往往需要在着色器中声明所需要的纹理/缓冲资源，并且分配相应的槽位（slot）：\n1 2 3  layout (binding = 0) uniform sampler2D tex0; layout (binding = 1) uniform sampler2D tex1; layout (binding = 2) uniform sampler2D tex2;   在CPU端，需要显式绑定所有纹理资源：\n而使用Bindless绑定模型，在着色器中，我们相当于使用了一个无穷大的纹理数组：\n1  layout (binding = 0) uniform sampler2D textureArray[];   所有的纹理数据可以一次性全部灌入其中，需要用到时，我们只需要一个下标索引即可进行访问，而对于材质而言，也不再像下图那样的贴图绑定：\n1 2 3  layout (binding = 0) uniform sampler2D Albedo; layout (binding = 1) uniform sampler2D Metallic; layout (binding = 2) uniform sampler2D Roughness;   而是使用一个结构体，存储所有的材质贴图索引：\n1 2 3 4 5 6  struct Material { uint Albedo; uint Metallic; uint Roughness; }   访问时只需：\n1  vec4 albedo = texture(textureArray[nonuniformEXT(material.Albedo)], inUV);   即可。对于GLSL，记得开启扩展GL_EXT_nonuniform_qualifier\nBindless访问模型如下：\nBindless对GPU Driven Rendering Pipeline有至关重要的作用，它主要解决了传统API下绑定资源到管线的开销问题，同时突破了着色器的硬件访问限制，进一步降低CPU-GPU的交互，我们不需要在CPU端设置Bindless资源的绑定状态，是之后实现single drawcall for everything的基础。\nVulkan中与Bindless相关的技术叫descriptor_indexing，在Vulkan 1.0属于EXT特性，但在Vulkan 1.2中已升为Core特性。在Logical Device的创建时指定：\n1 2 3 4 5 6 7  VkPhysicalDeviceVulkan12Features vulkan12_features = {}; vulkan12_features.sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VULKAN_1_2_FEATURES; vulkan12_features.shaderSampledImageArrayNonUniformIndexing = VK_TRUE; vulkan12_features.runtimeDescriptorArray = VK_TRUE; vulkan12_features.descriptorBindingVariableDescriptorCount = VK_TRUE; vulkan12_features.descriptorBindingPartiallyBound = VK_TRUE;   其中，shaderSampledImageArrayNonUniformIndexing、runtimeDescriptorArray、descriptorBindingVariableDescriptorCount指定开启descriptor_indexing特性，descriptorBindingPartiallyBound解决了缺省绑定的问题。\n在创建Bindless Texture过程中，需要指定Bindless Texture的数组支持的最大容量，通常会指定为一个较大的数（如1024）以避免反复扩容，而大部分情况下场景中的纹理都不会填满最大容量，此时需要开启descriptorBindingPartiallyBound支持缺省绑定，以防止出错。\nBindless Texture可视化：\n至此，我们又将一个费时的操作从CPU端移走了。\n3.2. 多重间接绘制 此前的一章一节中，我们将几何数据资源绑定的CPU开销降至最低，将纹理资源绑定的CPU开销给完全移走了，在本节中，我们将要把绘制开销降至最低，实现心心念念的single drawcall for everything。\n在最开始的设计中，我以一种非常低效的方式进行几何阶段的渲染，流程如下：\n 绑定Pipeline、DescriptorSet、Vertex/Index Buffer 遍历模型 遍历子网格 收集材质信息，使用Push Constant操作将材质数据送往着色器 调用绘制命令  可以看到，每一个子网格都将贡献一次Push Constant开销和一次Drawcall的开销，更不用说其他的逻辑判断操作，结果可想而知，场景复杂度一上去，CPU开销裂开，非常不贴合现代图形API的设计初衷，我们需要更多类似Bindless Texture的低CPU开销设计。\n好在现代图形API已经帮我们考虑好了，多重间接绘制Multi Draw Indirect能够完美地满足我们的需求。不同于显式调用绘制命令，Multi Draw Indirect允许我们实现将绘制命令预存在GPU的显存中，在需要绘制时调用：\n1  vkCmdDrawIndexedIndirect(cmd_buffer, draw_buffer, buffer_size, draw_count, sizeof(VkDrawIndexedIndirectCommand));   一个Drawcall即可完成所有的绘制指令提交。\n下面介绍多重间接绘制相关的技术细节：\n3.2.1. 指令缓冲 前述中，Multi Draw Indirect使用我们预存在GPU显存中的绘制命令进行提交，这些绘制命令存储在指令缓冲。在Vulkan中，有结构体VkDrawIndexedIndirectCommand或VkDrawIndirectCommand帮助我们指定指令缓冲中需要存哪些信息，一般来讲我们使用索引进行绘制，因此用的是VkDrawIndexedIndirectCommand，其数据结构定义为：\n1 2 3 4 5 6 7  typedef struct VkDrawIndexedIndirectCommand { uint32_t indexCount; uint32_t instanceCount; uint32_t firstIndex; int32_t vertexOffset; uint32_t firstInstance; } VkDrawIndexedIndirectCommand;   是不是和我们显式绘制指令的参数不能说很像，只能说一模一样？\n1 2 3 4 5 6 7  void vkCmdDrawIndexed( VkCommandBuffer commandBuffer, uint32_t indexCount, uint32_t instanceCount, uint32_t firstIndex, int32_t vertexOffset, uint32_t firstInstance);   当然用法也一样，就是把之前要在渲染循环里指定的参数一一写入一个std::vector\u0026lt;VkDrawIndexedIndirectCommand\u0026gt;容器里，然后在把里面的数据传到GPU显存中，使用其缓冲句柄即可调用vkCmdDrawIndexedIndirect了。\n","description":"IlumEngine的第一次渲染性能优化","id":5,"section":"posts","tags":["Real-time Rendering"],"title":"实时渲染优化技术(1)","uri":"https://chaphlagical.github.io/zh/posts/rendering/optimization1/"},{"content":"Lazy Snapping是一种基于图的图像分割方法[1]，能够将图像分割为前景和背景，通过用户交互挑选前景和背景种子，利用最大流最小分割的方法对图像进行分割。\n1. 基本方法 文献中利用分水岭算法对图像进行预处理，这里我们直接将图像下采样到原分辨率的八分之一进行处理，然后求解以下优化问题：\n$$\nE(X)=\\sum_{i\\in\\mathcal{V}}E_1(x_i)+\\lambda\\sum_{(i,j)\\in\\mathcal\\varepsilon}E_2(x_i, x_j)\n$$\n其中，$E_1(x_i)$为似然能量，编码结点$x_i$的代价，$E_2(x_i,x_j)$为先验能量，代表邻接结点$x_i$和$x_j$的代价。\n似然能量$E_1(x_i)$定义为：\n$$\n\\begin{cases}\n\\begin{matrix}\nE_1(x_i=1)=0\u0026amp;E_1(x_i=0)=\\infty\u0026amp;\\forall i\\in\\mathcal{F}\n\\end{matrix}\\\\\\\\\n\\begin{matrix}\nE_1(x_i=1)=\\infty\u0026amp;E_1(x_i=0)=0\u0026amp;\\forall i\\in\\mathcal{B}\n\\end{matrix}\\\\\\\\\n\\begin{matrix}\nE_1(x_i=1)=\\frac{d_i^\\mathcal{F}}{d_i^\\mathcal{F}+d_i^\\mathcal{B}}\u0026amp;E_1(x_i=0)=\\frac{d_i^\\mathcal{B}}{d_i^\\mathcal{F}+d_i^\\mathcal{B}}\u0026amp;\\forall i\\in\\mathcal{U}\n\\end{matrix}\n\\end{cases}\n$$\n其中，$\\mathcal{F}$表示前景种子集合，$\\mathcal{B}$为背景种子集合，均由用户输入，而$\\mathcal{U}$为补集。$d_i^\\mathcal{F}$和$d_i^\\mathcal{B}$分别表示当前颜色与前景种子平均值和背景种子平均值的距离平方\n先验能量$E_2(x_i,x_j)$定义为：\n$$\nE_2(x_i,x_j)=\\frac{1}{|C(i)-C(j)|^2+\\varepsilon}\n$$\n实验中，取$\\lambda=100$，$\\epsilon=0.01$。\n然后构建无向图，通过最大流最小割方法分离出前景和背景，提取轮廓得到结果\n实验结果    原图像 交互图像 轮廓                     参考文献 [1] Y. Li, J. Sun, C.-K. Tang, and H.-Y. Shum. Lazy snapping. ACM Transactions on Graphics (ToG), 23(3):303–308, 2004.\n","description":"复现Siggraph2004论文《Lazy snapping》","id":6,"section":"posts","tags":["Digital Image Process"],"title":"Lazy Snapping","uri":"https://chaphlagical.github.io/zh/posts/image_process/lazy_snapping/"},{"content":"Seam Carving算法是一种基于内容的图像缩放方法，在保证图像中“重要区域”不发生形变的前提下，对图像进行缩放。\n一种直观的想法便是找出图像中的“不重要区域”，并将其删除。文献[1]中便是采用这种思想，\n通过定义像素的能量函数，通过动态规划方法对某一方向的像素进行能量累积，最后回溯求出能量最低的一条路径，该路径便是我们要删除的“最不重要”路径。\n1. 基本方法 图像的能量简单地由图像梯度描述：\n$$\ne_1(\\pmb I)=\\left|\\frac{\\partial}{\\partial x}\\pmb I\\right|+\\left|\\frac{\\partial}{\\partial y}\\pmb I\\right|\n$$\n论文中也给出另外一种能量的变体实现：\n$$\ne_{HoG}(\\pmb I)=\\frac{\\left|\\frac{\\partial}{\\partial x}\\pmb I\\right|+\\left|\\frac{\\partial}{\\partial y}\\pmb I\\right|}{\\max(HoG(\\pmb I(x,y)))}\n$$\n实现中，采用Sobel算子提取图像梯度作为能量图：\n1 2 3 4 5 6 7 8 9 10 11  void SeamCarving::genEnergyMap(const cv::Mat\u0026amp; img, cv::Mat\u0026amp; energy) { cv::Mat sobel_x, sobel_y, gray_energy; cv::cvtColor(img, gray_energy, cv::COLOR_BGR2GRAY); cv::Sobel(gray_energy, sobel_x, CV_32F, 1, 0, 3); cv::convertScaleAbs(sobel_x, sobel_x); cv::Sobel(gray_energy, sobel_y, CV_32F, 0, 1, 3); cv::convertScaleAbs(sobel_y, sobel_y); cv::addWeighted(sobel_x, 0.5, sobel_y, 0.5, 0, energy); energy.convertTo(energy, CV_32FC1); }   对于$n\\times m$的图像$\\pmb I$，用于删除的接缝由如下定义：\n水平接缝：\n$$\n\\pmb{s^x}=\\{s_i^x\\}_{i=1}^n=\\{(x(i),i)\\}_{i=1}^n,\\ \\\ns.t.\\ \\ \\forall i,\\ \\ |x(i)-x(i-1)|\\leq 1\n$$\n竖直接缝：\n$$\n\\pmb{s^y}=\\{s_i^y\\}_{j=1}^m=\\{(j, y(j))\\}_{j=1}^m,\\ \\\ns.t.\\ \\ \\forall j,\\ \\ |y(j)-y(j-1)|\\leq 1\n$$\n其中，$x:[1,\\cdots,n]\\rightarrow[1,\\cdots,m]$，$y:[1,\\cdots,m]\\rightarrow[1,\\cdots,n]$\n而我们要寻找最低能量路径即求解最小优化问题：\n$$\ns^\\ast=\\min_{\\pmb s}E(\\pmb s)=\\min_{\\pmb s}\\sum_{i=1}^ne(\\pmb I(s_i))\n$$\n利用动态规划的思想可以很方便地求出上述优化问题，利用能量累积矩阵$M$，转移方程：\n$$\nM(i,j)=e(i,j)+\\min(M(i-1,j-1), M(i-1,j), M(i-1,j+1))\n$$\n从边缘出发按转移方程填充能量累积矩阵，最后寻找能量最低的终点$M(n,x)$（竖直搜索）或$M(x,m)$（水平搜索），回溯即可得到完整的最优路径。\n1.1. 图像缩小任务 对于单方向缩小任务，只需重复上述接缝搜索流程，每次删除一条接缝即可。\n对于多方向缩小任务，文献中也将横向竖向接缝的选择顺序其视为一个优化问题，假设现有$m\\times n$的图像$\\pmb I$欲缩小至$m'\\times n'$，其中$m\u0026gt;m',n\u0026gt;n'$，接缝顺序的选择等价于优化以下能量函数：\n$$\n\\min_{\\pmb{s^x},\\pmb{s^y},\\alpha}\\sum_{i=1}^kE(\\alpha_i\\pmb {s_i^x}+(1-\\alpha_i)\\pmb{s_i^y})\n$$\n其中，$r=m-m'$，$c=n-n'$， $\\alpha_i\\in{0,1}$描述了接缝选择的方向，因此有$\\sum_{i=1}^k\\alpha_i=r$，$\\sum_{i=1}^k(1-\\alpha_i)=c$成立。该问题同样可以利用动态规划的想法进行求解，取能量累积矩阵$\\pmb T$，满足$\\pmb T(0,0)=0$，转移方程：\n$$\n\\pmb T(r,c)=\\min(\\pmb T(r-1,c),E(\\pmb s^x(\\pmb {I_{n-r-1\\times m-c}})),\\pmb T(r,c-1),E(\\pmb s^y(\\pmb {I_{n-r\\times m-c-1}})))\n$$\n$\\pmb {I_{n-r-1\\times m-c}}$表示大小为$n-r-1\\times m-c$的图像（中间量），$E(\\pmb{s^x}(\\pmb I))$和$E(\\pmb{s^y}(\\pmb I))$为相应的方向接缝删除后的能量。\n但上述方法实测速度很慢，因此在实现中选择简单的贪婪策略选择带来当前最低能量的方法。\n1.2. 图像拉伸任务 对于图像拉伸任务，同样可以采用缩小任务相似的处理方法，只是将最优接缝的删除修改为最优接缝邻域的插值，但和缩小任务不同的是，每次对单条最优接缝进行插值，容易导致后续的最优接缝\n搜索会集中在同一区域，因此在图像拉伸任务中，建议一次性选择多条低能量接缝进行插值。\n2. 实验结果 2.1. 图像缩小    原图像 图像能量 结果图像                     2.2. 图像拉伸    原图像 图像能量 最优接缝 结果图像                        3. 总结 从上图中可以看出 Seam Carving 的一些局限性：\n 进行图像拉伸任务时容易造成图像区块重复，可以考虑手动排除部分区域进行优化 在梯度变化不明显的“重要区域”容易造成误处理，比如：《蒙娜丽莎》大片的头发。可以考虑手动划分“重要区域”进行处理  参考文献 [1] S. Avidan and A. Shamir. Seam carving for content-aware image resizing. In ACM SIGGRAPH 2007 papers, pages 10–es. 2007\n","description":"复现Siggraph2007论文《Seam Carving for Content-Aware Image Resizing》","id":7,"section":"posts","tags":["Digital Image Process"],"title":"Seam Carving","uri":"https://chaphlagical.github.io/zh/posts/image_process/seam_carving/"},{"content":"1. 问题描述 给定以下两幅图：\n现我们需要将第一幅图中的女孩搬到第二幅图的海水中，为使得复制粘贴更加逼真自然，我们需要设计算法来满足我们两幅图像融合的需要\n2. 算法描述 Poisson Image Editing算法[1]的基本思想是在尽可能保持原图像内部梯度的前提下，让粘贴后图像的边界值与新的背景图相同，以实现无缝粘贴的效果。从数学上讲，对于原图像$f(x,y)$，新背景$f^(x,y)$和嵌入新背景后的新图像$v(x,y)$，等价于解最优化问题：\n$$\n\\min\\limits_f \\iint \\Omega |\\nabla f-\\boldsymbol v |^2 \\ \\ \\mathrm{with}\\ f|{\\partial \\Omega}=f^|_{\\partial \\Omega}\n$$\n利用变分法，令$F=|\\nabla f-\\boldsymbol v |^2=(\\nabla f_x-\\boldsymbol v_x)^2+(\\nabla f_y-\\boldsymbol v_y)^2$\n代入欧拉-拉格朗日方程：\n$$\nF_f-\\frac{\\mathrm d}{\\mathrm d x}F_{f_x}-\\frac{\\mathrm d}{\\mathrm d y}F_{f_y}=0\n$$\n由于$F$是关于$\\nabla f$的函数，因此$F_f=0$\n所以有：\n$$\n\\begin{align}\n\u0026amp;\\frac{\\partial F}{\\partial f}=\\frac{\\mathrm d}{\\mathrm dx}\\left[\\frac{\\partial F}{\\partial(\\nabla f_x-\\pmb v_x)^2}\\right]+\\frac{\\mathrm d}{\\mathrm dy}\\left[\\frac{\\partial F}{\\partial(\\nabla f_y-\\pmb v_y)^2}\\right]\\\\\\\\\n\u0026amp;\\Rightarrow 0=\\frac{\\mathrm d}{\\mathrm dx}[2(\\nabla f_x-\\pmb v_x)]+\n\\frac{\\mathrm d}{\\mathrm dy}[2(\\nabla f_y-\\pmb v_y)]\\\\\\\\\n\u0026amp;\\Rightarrow 0=\\left(\\frac{\\partial ^2f}{\\partial x^2}-\\frac{\\partial \\pmb v}{\\partial x}\\right)+\\left(\\frac{\\partial ^2f}{\\partial y^2}-\\frac{\\partial \\pmb v}{\\partial y}\\right)\\\\\\\\\n\u0026amp;\\Rightarrow \\Delta f=\\mathrm{div}\\pmb v\n\\end{align}\n$$\n可转化为具有Dirichlet边界条件的Poisson方程：\n$$\n\\Delta f= \\mathrm{div} \\boldsymbol v\\ \\ \\mathrm{over}\\ \\Omega\\ \\ \\mathrm{with} \\ \\ f|_{\\partial \\Omega}=f^\\ast|_{\\partial\\Omega}\n$$\n以第一幅图和第二幅图为例，将图1中需要复制的区域设为$S$，定义$N_p$为$S$中的每一个像素$p$四个方向连接邻域，令$\u0026lt;p,q\u0026gt;$为满足$q\\in N_p$的像素对。边界$\\Omega$定义为$\\partial \\Omega ={p\\in S\\setminus \\Omega: N_p \\cap \\Omega \\neq \\emptyset }$，设$f_p$为$p$处的像素值$f$，目标即求解像素值集$f|_\\Omega ={f_p,p\\in \\Omega}$\n利用Poisson Image Editing算法的基本原理，上述问题转化为求解最优化问题：\n$$\n\\min\\limits_{f|_\\Omega}\\sum\\limits_{\u0026lt;p,q\u0026gt;\\cap \\Omega\\neq \\emptyset}(f_p-f_q-v_{pq})^2,\\mathrm{with}\\ f_p=f_p^*,\\forall\\ p\\in \\partial\\Omega\r$$\n化为求解线性方程组：\n$$\n\\forall\\ p\\in \\Omega,\\ |N_p|f_p-\\sum\\limits_{q\\in N_p\\cap \\Omega} f_q=\\sum\\limits_{q\\in N_p\\cap \\partial \\Omega}f_p^*+\\sum\\limits_{q\\in N_p}v_{pq}\n$$\n对于梯度场$\\boldsymbol{v}(\\boldsymbol{x})$的选择，文献[1]给出两种方法，一种是完全使用前景图像的内部梯度，即：\n$$\n\\forall\\ \u0026lt;p,q\u0026gt;,v_{pq}=g_p-g_q\n$$\n另一种是使用混合梯度：\n$$\n\\forall\\ \\boldsymbol{x}\\in \\Omega,\\ \\boldsymbol{v}(\\boldsymbol{x})=\\begin{cases}\n\\nabla f^*(\\boldsymbol{x})\u0026amp;\\mathrm{if}\\ |\\nabla f^*(\\boldsymbol{x})\u0026gt;|\\nabla g(\\boldsymbol{x})|,\\\\\\\\\n\\nabla g(\\boldsymbol{x})\u0026amp;\\mathrm{otherwise}\n\\end{cases} $$\n**扫描线算法**\n为实现多边形和自由绘制闭合图形区域的Poisson Image Editing算法，需通过扫描线算法获取多边形内部掩膜。这里从网上资料了解到一种有序边表法，其基本思想是定义边表ET和活动边表AET，ET记录当前扫描线与边的交点坐标、从当前扫描线到下一条扫描线间x的增量、该边所交的最高扫描线，AET记录只与当前扫描线相交的边的链表，通过迭代得到当前扫描线与待求多边形各边的交点，再利用奇偶检测法判断该点是否在多边形内部进行填充。\n3. 实验结果 3.1. 标准图像测试 原图像：\n新背景图像：\n   边界形式 图1选择区域 图2选择区域 结果图像     矩形边界      多边形边界      自由绘制边界       3.2. Poisson vs. Mix Poisson 背景图像：\n前景图像：\n混合结果：\n如图，左上为直接复制粘贴，保留前景全部颜色梯度信息；左下为普通Poisson编辑，保留前景全部梯度信息，前景像素颜色与背景作融合；右上为应用混合梯度的Poisson编辑，前景梯度部分保留，效果上比普通Poisson编辑更加“透明”，适合用在水印等场景。\n3.3. 其他应用 3.3.1. 遮盖不必要的信息（如去皱纹） 原图像：\n处理效果：\n3.3.2. 恐怖片特效 原图：\n掩盖镜子中人物：\n使用电影《修女》中的角色：\n处理效果：\n3.3.3. 生成表情包 原图：\n处理效果：\n参考文献 [1] Patrick Pérez, Michel Gangnet, Andrew Blake. Poisson image editing. Siggraph 2003.\n","description":"复现Siggraph2003论文《Poisson Image Editing》","id":8,"section":"posts","tags":["Digital Image Process"],"title":"Poisson Image Editing","uri":"https://chaphlagical.github.io/zh/posts/image_process/possion/"},{"content":"1. 算法原理 1.1. 基本原理  输入：$n$对控制点对$(\\pmb p_i,\\pmb q_i)$，$i=1,2,\\cdots,n$，其中$\\pmb p_i\\in\\mathbb R^2$为控制起始点，$\\pmb q_i\\in\\mathbb{R}^2$为控制目标点 目标：找到一个映射$f:\\mathbb R^2\\rightarrow \\mathbb{R}^2$，满足$f(\\pmb p_i)=\\pmb q_i$，$i=1,2,\\cdots,n$  1.2. Inverse distance-weighted interpolation methods(IDW)[1] IDW 算法基本原理是根据给定的控制点对和控制点对的位移矢量，计算控制点对周围像素的反距离加权权重影响，实现图像每一个像素点的位移。\n选择$n$对控制点对$(\\pmb p_i,\\pmb q_i)$，$i=1,2,\\cdots,n$，目标映射$f:\\mathbb R^2\\rightarrow \\mathbb{R}^2$可表示成\n以下形式：\n$$\nf(\\pmb p)=\\sum_{i=1}^n\\omega_i(\\pmb p)f_i(\\pmb p)\n$$\n其中，权重$\\omega_i(\\pmb p)$满足：\n$$\nw_i(\\pmb p)=\\frac{\\sigma_i(\\pmb p)}{\\sum_{j=1}^n\\sigma_j(\\pmb p)}\n$$\n$\\sigma_i(\\pmb p)$反映第$i$对控制点对像素$\\pmb p$得反距离加权权重影响程度，可以直接取：\n$$\n\\sigma_i(\\pmb p)=\\frac{1}{|\\pmb p-\\pmb p_i|^\\mu}\n$$\n其中$\\mu\u0026gt;1$，也可以取locally bounded weight：\n$$\n\\sigma_i(\\pmb p)=\\left[\\frac{R_i-d(\\pmb p,\\pmb p_i)}{R_id(\\pmb p,\\pmb p_i)}\\right]^\\mu\n$$\n$f_i$为线性函数，满足：\n$$\nf_i(\\pmb p)=\\pmb q_i+\\pmb T_i(\\pmb p-\\pmb p_i)\n$$\n其中$\\pmb T_i$为二阶矩阵：\n$$\n\\pmb T_i=\\begin{bmatrix}\nt_{11}^{(i)}\u0026amp;t_{12}^{(i)}\\\\\nt_{21}^{(i)}\u0026amp;t_{22}^{(i)}\n\\end{bmatrix}\n$$\n矩阵$\\pmb T$得确定，可以通过求解如下最优化问题：\n$$\n\\arg\\min_{\\pmb T_i} E(\\pmb T_i)=\\sum_{j=1,j\\neq i}^n\\sigma_i(\\pmb p_j)|\\pmb q_j-f_i(\\pmb p_j)|^2\n$$\n上式对$\\pmb T_i$求导，令方程为0得：\n$$\n\\pmb T_i\\sum_{j=1,j\\neq i}\\sigma_i(\\pmb p_j)\\pmb p\\pmb p^T=\\sum_{j=1,j\\neq i}\\sigma_i(\\pmb p_j)\\pmb q\\pmb p^T\n$$\n其中$\\pmb p=\\pmb p_j-\\pmb p_i$，$\\pmb q=\\pmb q_j-\\pmb q_i$\n又$\\sigma_i(\\pmb p_j)\\pmb p\\pmb p^T$非奇异，因此可以直接解出$\\pmb T_i$的值：\n$$\n\\pmb T_i=\\left(\\sum_{j=1,j\\neq i}\\sigma_i(\\pmb p_j)\\pmb q\\pmb p^T\\right)\\left(\\sum_{j=1,j\\neq i}\\sigma_i(\\pmb p_j)\\pmb p\\pmb p^T\\right)^{-1}\n$$\n求出$\\pmb T_i(i=1,\\cdots,n)$后，映射$f$也就相应确定\n1.3. Radial basis functions interpolation method(RBF)[2] 选择$n$对控制点对$(\\pmb p_i,\\pmb q_i)$，$i=1,2,\\cdots,n$，目标映射$f:\\mathbb R^2\\rightarrow \\mathbb{R}^2$可表示为以下形式：\n$$\nf(\\pmb p)=\\sum_{i=1}^n\\alpha_ig_i(|\\pmb p-\\pmb p_i|)+\\pmb{Ap}+\\pmb B\n$$\n其中，$g_i$为径向基函数，通常可以取Hardy multiquadrics：$g(t)=(t^2+c^2)^{\\pm \\frac{1}{2}}$或高斯函数$g_\\sigma(t)=e^{-t^2/\\sigma^2}$，\n为了计算方便，这里取Hardy multiquadrics：\n$$\n\\begin{aligned}\ng_i(d)\u0026amp;=(d+r_i)^{\\pm\\frac{1}{2}}\\\nr_i\u0026amp;=\\min_{j\\neq i}d(\\pmb p_i,\\pmb p_j)\n\\end{aligned}\n$$\n对于线性部分分量$\\pmb {Ap}+\\pmb B$，本例简单地取$\\pmb A=\\pmb I$和$\\pmb B=\\pmb 0$\n2. 实验结果 2.1. 标准图像测试 如下图所示，固定四角，蓝色为控制起始点，绿色为控制终止。\n2.2. IDW算法    $\\mu$取值 修复前 修复后     $\\mu=-1$     $\\mu=-2$      2.3. RBF算法    $\\mu$取值 修复前 修复后     $\\mu=-0.5$     $\\mu=0.5$      2.4. 相关应用 2.4.1. 柴犬表情包 原图像：\n开心：\nEmmm……：\n2.4.2. 藏狐的笑容 原图像：\n处理后：\n参考文献 [1] D. Ruprecht and H. Muller. Image warping with scattered data interpolation. IEEE Computer Graphics and Applications, 15(2):37–43, 1995. 10\n[2] N. Arad and D. Reisfeld. Image warping using few anchor points and radial functions. In Computer graphics forum, volume 14, pages 35–46. Wiley Online Library, 1995.\n","description":"基于IDW与RBF方法的图像扭曲算法实现","id":9,"section":"posts","tags":["Digital Image Process"],"title":"Image Warping","uri":"https://chaphlagical.github.io/zh/posts/image_process/image_warping/"},{"content":"1. 问题描述 给定一张原图像：\n和一张参考图像：\n我们希望通过设计一个颜色转换算法，使得源图像具有目标图像的颜色风格，如下图：\n2. 算法描述 衡量一幅图像的颜色分布最基础的统计特征就是均值和标准差，文献[1]中便通过这两个特征对图像进行简单的变换并取得不错的效果。\n首先，需要将图像的颜色空间从$RGB$空间变换到$l\\alpha\\beta$空间：\n  $RGB$空间$\\rightarrow$ $XYZ$空间\n$$\n\\begin{pmatrix}\nX\\\\Y\\\\Z\n\\end{pmatrix}=\n\\begin{pmatrix}\n0.5141\u0026amp;0.3239\u0026amp;0.1604\\\\\n0.2651\u0026amp;0.6702\u0026amp;0.0641\\\\\n0.0241\u0026amp;0.1228\u0026amp;0.8444\n\\end{pmatrix}\n\\begin{pmatrix}\nR\\\\G\\\\B\n\\end{pmatrix}\n$$\n  $XYZ$空间$\\rightarrow$ $LMS$空间\n$$\n\\begin{pmatrix}\nL\\\\M\\\\S\n\\end{pmatrix}=\n\\begin{pmatrix}\n0.3897\u0026amp;0.6890\u0026amp;-0.0787\\\\\n-0.2298\u0026amp;1.1834\u0026amp;0.0464\\\\\n0.0000\u0026amp;0.0000\u0026amp;1.0000\n\\end{pmatrix}\n\\begin{pmatrix}\nX\\\\Y\\\\Z\n\\end{pmatrix}\n$$\n  变换到对数空间\n$$\n\\begin{aligned}\n\\pmb L\u0026amp;=\\lg L\\\\\n\\pmb M\u0026amp;=\\lg M\\\\\n\\pmb S\u0026amp;=\\lg S\n\\end{aligned}\n$$\n  $\\pmb{LMS}$空间$\\rightarrow$ $l\\alpha\\beta$空间\n$$\n\\begin{pmatrix}\nl\\\\\\alpha\\\\\\beta\n\\end{pmatrix}=\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{3}}\u0026amp;0\u0026amp;0\\\\\n0\u0026amp;\\frac{1}{\\sqrt{6}}\u0026amp;0\\\\\n0\u0026amp;0\u0026amp;\\frac{1}{\\sqrt{2}}\n\\end{pmatrix}\n\\begin{pmatrix}\n1\u0026amp;1\u0026amp;1\\\\\n1\u0026amp;1\u0026amp;-2\\\\\n1\u0026amp;-1\u0026amp;0\n\\end{pmatrix}\n\\begin{pmatrix}\n\\pmb L\\\\\\pmb M\\\\\\pmb S\n\\end{pmatrix}\n$$\n  同理，将图像的颜色空间从 lαβ 空间变换到 RGB 空间只需将相应的变换矩阵依次求逆即可。\n设变换到$l\\alpha\\beta$颜色空间的源图像的均值和标准差分别为$\\mu_s$、$\\sigma_s$，目标图像的均值和标准差分别为$\\mu_t$、$\\sigma_t$，只需通过如下简单处理，即可将源图像的颜色分布变换到接近与目标图像\n$$\n\\begin{aligned}\nl'\u0026amp;=\\frac{\\sigma_t^l}{\\sigma_s^l}(l-\\mu^l_s)+\\mu^l_t\\\\\n\\alpha'\u0026amp;=\\frac{\\sigma_t^\\alpha}{\\sigma_s^\\alpha}(\\alpha-mu^\\alpha_s)+\\mu^\\alpha_t\\\\\n\\beta'\u0026amp;=\\frac{\\sigma_t^\\beta}{\\sigma_s^\\beta}(\\beta-\\mu^\\beta_s)+\\mu^\\beta_t\\\n\\end{aligned}\n$$\n并将图像$(l',\\alpha',\\beta')$变换回$RGB$空间即可得到处理后的结果。\n3. 实验结果    原图像 参考图像 结果图像                               参考文献 [1] E. Reinhard, M. Adhikhmin, B. Gooch, and P. Shirley. Color transfer between images. IEEE Computer graphics and applications, 21(5):34–41, 2001.10\n","description":"复现论文《Color Transfer between Images》","id":10,"section":"posts","tags":["Digital Image Process"],"title":"Color Transfer","uri":"https://chaphlagical.github.io/zh/posts/image_process/color_transfer/"},{"content":"1. 问题描述 给定一张原图像：\n我们希望通过设计一个灰度图着色算法，使得下述灰度目标图像能够利用彩色原图像的颜色信息进行合理的上色：\n以得到类似下图的效果：\n2. 算法描述 实验参考了论文[1]，并使用全局匹配的方法进行求解。我们在图像的$l\\alpha\\beta$颜色空间上进行求解，这是由于$l\\alpha\\beta$颜色空间的$l$即代表着亮度，通过查找灰度源图像上的像素与彩色目标图像的$l$分量最佳匹配像素，再将目标图像对应匹配像素的$\\alpha$和$\\beta$分量赋予源图像即可\n进行灰度图像上色。算法流程如下：\n  将源图像$img_{src}$变换到$l\\alpha\\beta$颜色空间得到$img_{src}^{(l\\alpha\\beta)}$，同时将源图像$img_{src}$转换为灰度图$img_{src}^{(grey)}$\n  利用如下均值标准差变换，将$img_{src}^{(l\\alpha\\beta)}$的$l$通道和$img_{src}^{(grey)}$映射到具有灰度目标图像$img_{tar}$的像素值分布，得到$luminance_{src}$和$luminance_{src}^{grey}$\n$$\n\\begin{aligned}\nl'\u0026amp;=\\frac{\\sigma_t^l}{\\sigma_s^l}(l-\\mu^l_s)+\\mu^l_t\\\\\n\\alpha'\u0026amp;=\\frac{\\sigma_t^\\alpha}{\\sigma_s^\\alpha}(\\alpha-\\mu^\\alpha_s)+\\mu^\\alpha_t\\\\\n\\beta'\u0026amp;=\\frac{\\sigma_t^\\beta}{\\sigma_s^\\beta}(\\beta-\\mu^\\beta_s)+\\mu^\\beta_t\\\\\n\\end{aligned}\n$$\n  近邻域标准差计算。利用一个$n\\times n$（本实验中取$n=5)$）滑动窗口遍历图像（类似于卷积操作，边界用0填充），对窗口内的所有像素值计算它们的标准差并赋值到一幅标准差图像上。对$luminance_{src}^{grey}$和$img_{tar}$进行该计算，得到$stddev_{src}^{grey}$和$stddev_{tar}$\n  最佳匹配查找。遍历$img_{tar}$，已知$img_{tar}$上的某一像素值$p_{tar}$和其标准差$stddev_{tar}$对应的值$\\sigma_{tar}$，在源图像上寻找最佳匹配的像素点，这里简单地采用加权的$\\mathcal{L}_2$范数平方去算：\n$$\n\\arg\\min_{x,y} \\Big(w_1\\ast |luminance_{src}(x,y)-p_{tar}|^2+w_2\\ast|stddev_{src}^{grey}(x,y)-\\sigma_{tar}|^2\\Big)\n$$\n其中$w_1$和$w_2$分别表示亮度值和标准差的对匹配结果的贡献，这里简单地取$w_1=w_2=0.5$\n  灰度图上色。将查找到的最佳匹配像素$(\\tilde x. \\tilde y)$在$img_{src}^{(l\\alpha\\beta)}$所在像素值的$\\alpha$和$\\beta$分量赋值给源图像作为其$\\alpha$和$\\beta$通道的值，而原来的灰度值则作为$l$分量继续使用，并将源图像重新变换回$RGB$颜色空间得到最终的结果\n  3. 实验结果    原图像 参考图像 结果图像                               参考文献 [1] Welsh, M. Ashikhmin, and K. Mueller. Transferring color to greyscale im-ages. In Proceedings of the 29th annual conference on Computer graphics and interactive techniques, pages 277–280, 2002.11\n","description":"复现Siggraph2002论文《Transferring Color To Greyscale Image》","id":11,"section":"posts","tags":["Digital Image Process"],"title":"Colorization","uri":"https://chaphlagical.github.io/zh/posts/image_process/colorization/"},{"content":"我将在这里分享有趣的知识与技术\n","description":"My Blog","id":12,"section":"","tags":null,"title":"简介","uri":"https://chaphlagical.github.io/zh/about/"}]