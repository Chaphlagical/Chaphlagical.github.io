<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"chaphlagical.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":true,"version":"8.14.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":true,"storage":true,"lazyload":false,"nav":{"disqus":{"text":"Load Disqus","order":-1}}},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="TOG 2020 paper reading">
<meta property="og:type" content="blog">
<meta property="og:title" content="Adaptive Incident Radiance Field Sampling and Reconstruction Using Deep Reinforcement Learning">
<meta property="og:url" content="https://chaphlagical.github.io/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/index.html">
<meta property="og:site_name" content="Chaf&#39;s Blog">
<meta property="og:description" content="TOG 2020 paper reading">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://chaphlagical.github.io/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/image-20230114185443216.png">
<meta property="og:image" content="https://chaphlagical.github.io/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/image-20230114203256351.png">
<meta property="og:image" content="https://chaphlagical.github.io/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/image-20230114195859830.png">
<meta property="og:image" content="https://chaphlagical.github.io/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/image-20230114203015118.png">
<meta property="og:image" content="https://chaphlagical.github.io/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/image-20230114203043520.png">
<meta property="og:image" content="https://chaphlagical.github.io/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/image-20230114214044709.png">
<meta property="og:image" content="https://chaphlagical.github.io/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/image-20230114214441994.png">
<meta property="og:image" content="https://chaphlagical.github.io/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/image-20230114215842383.png">
<meta property="og:image" content="https://chaphlagical.github.io/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/image-20230114222118357.png">
<meta property="og:image" content="https://chaphlagical.github.io/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/image-20230114222235529.png">
<meta property="article:published_time" content="2023-01-25T00:04:00.000Z">
<meta property="article:modified_time" content="2023-01-14T14:37:59.600Z">
<meta property="article:author" content="Chaf Chen">
<meta property="article:tag" content="Rendering">
<meta property="article:tag" content="TOG 2020">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chaphlagical.github.io/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/image-20230114185443216.png">


<link rel="canonical" href="https://chaphlagical.github.io/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://chaphlagical.github.io/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/","path":"2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/","title":"Adaptive Incident Radiance Field Sampling and Reconstruction Using Deep Reinforcement Learning"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Adaptive Incident Radiance Field Sampling and Reconstruction Using Deep Reinforcement Learning | Chaf's Blog</title>
  






  <script async defer data-website-id="" src=""></script>

  <script defer data-domain="" src=""></script>

  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Chaf's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Here I share some fun stuff</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Contribution"><span class="nav-number">1.</span> <span class="nav-text">Contribution</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Related-Work"><span class="nav-number">2.</span> <span class="nav-text">Related Work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Representation-of-Incident-Radiance-Field"><span class="nav-number">3.</span> <span class="nav-text">Representation of Incident Radiance Field</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Radiance-Field-Reconstruction-Using-the-R-network"><span class="nav-number">4.</span> <span class="nav-text">Radiance Field Reconstruction Using the R-network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Filtering-4D-Radiance-Space"><span class="nav-number">5.</span> <span class="nav-text">Filtering 4D Radiance Space</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#R-Network"><span class="nav-number">5.1.</span> <span class="nav-text">R-Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Image-Direction-Network"><span class="nav-number">5.2.</span> <span class="nav-text">Image-Direction Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Experiments-on-Reconstruction-Networks"><span class="nav-number">5.3.</span> <span class="nav-text">Experiments on Reconstruction Networks</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DRL-based-Adaptive-Sampling"><span class="nav-number">6.</span> <span class="nav-text">DRL-based Adaptive Sampling</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Deep-Q-Learning"><span class="nav-number">6.1.</span> <span class="nav-text">Deep Q-Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reinforcement-Learning-Process"><span class="nav-number">6.2.</span> <span class="nav-text">Reinforcement Learning Process</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q-network-Structure"><span class="nav-number">6.3.</span> <span class="nav-text">Q-network Structure</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adaptive-Sampling-and-Rendering"><span class="nav-number">7.</span> <span class="nav-text">Adaptive Sampling and Rendering</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Adaptive-Sampling-Algorithm"><span class="nav-number">7.1.</span> <span class="nav-text">Adaptive Sampling Algorithm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reconstruction-and-Final-Rendering"><span class="nav-number">7.2.</span> <span class="nav-text">Reconstruction and Final Rendering</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Result"><span class="nav-number">8.</span> <span class="nav-text">Result</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Limitations"><span class="nav-number">9.</span> <span class="nav-text">Limitations</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Chaf Chen"
      src="/images/logo.jpg">
  <p class="site-author-name" itemprop="name">Chaf Chen</p>
  <div class="site-description" itemprop="description">USTC CG Student</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">16</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/chaphlagical" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;chaphlagical" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:chaf@mail.ustc.cn" title="E-Mail → mailto:chaf@mail.ustc.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          Links
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="http://gcl.ustc.edu.cn/" title="http:&#x2F;&#x2F;gcl.ustc.edu.cn&#x2F;" rel="noopener" target="_blank">GCL</a>
            </li>
        </ul>
      </div>
    </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://chaphlagical.github.io/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo.jpg">
      <meta itemprop="name" content="Chaf Chen">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chaf's Blog">
      <meta itemprop="description" content="USTC CG Student">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Adaptive Incident Radiance Field Sampling and Reconstruction Using Deep Reinforcement Learning | Chaf's Blog">
      <meta itemprop="description" content="TOG 2020 paper reading">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Adaptive Incident Radiance Field Sampling and Reconstruction Using Deep Reinforcement Learning
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-01-25 00:04:00" itemprop="dateCreated datePublished" datetime="2023-01-25T00:04:00+00:00">2023-01-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-01-14 14:37:59" itemprop="dateModified" datetime="2023-01-14T14:37:59+00:00">2023-01-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Reading/" itemprop="url" rel="index"><span itemprop="name">Paper Reading</span></a>
        </span>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

            <div class="post-description">TOG 2020 paper reading</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><p><img src="/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/image-20230114185443216.png"></p>
<p><img src="/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/image-20230114203256351.png"></p>
<ul>
<li>Addresses the light-field sampling and reconstruction problem with deep learning techniques and offline datasets.</li>
<li>Proposes a novel R-network that explores the image and direction spaces of the radiance field to effectively filter and reconstruct the incident radiance field.</li>
<li>Presents a novel RL-based Q-network to guide the adaptive rendering process.</li>
</ul>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul>
<li>Image-Space Methods</li>
<li>Light field Reconstruction Methods</li>
<li>Light field Adaptive Sampling Methods</li>
<li>Filtering using DNN</li>
<li>DRL</li>
</ul>
<h2 id="Representation-of-Incident-Radiance-Field"><a href="#Representation-of-Incident-Radiance-Field" class="headerlink" title="Representation of Incident Radiance Field"></a>Representation of Incident Radiance Field</h2><ul>
<li>4D incident radiance field<ul>
<li>Image spaces: the space of a pixel or a shading point</li>
<li>Direction spaces: the space of an incident hemisphere centered on the average normal of a group of shading points</li>
</ul>
</li>
<li>Radiance field blocks<ul>
<li>Partition the image space (pixels) into tiles</li>
<li>Partition the direction space into bins</li>
</ul>
</li>
<li>A radiance block $B^j$ is defined by a bounded domain of the direction space and can be expressed as:<br>  $$<br>  B^j\mathop&#x3D;^{\Delta}\{\theta,\phi;0\leq\theta_0^j\leq\theta\leq\theta_1^j&lt;\frac{\pi}{2},0\leq\phi_0^j\leq\phi\leq\phi_1^j&lt;2\pi\}<br>  $$<ul>
<li>$[\theta_0^j,\theta_1^j]$ and $[\phi_0^j,\phi_1^j]$ are the elevation and azimuth angle bounds of $B^j$</li>
<li>The j-th partition in the direction space at the particular pixel $\pmb x$ is $B_{\pmb x}^j$</li>
<li>The j-th partition in the direction space shared by all pixels in the tile is $B_T^j$</li>
</ul>
</li>
<li>Guided by the Q-network<ul>
<li>Adaptively partition the direction space into a radiance field hierarchy with nodes of various sizes</li>
<li>Recursively partitioning the azimuth angle $\theta$ and the cosine weighted zenith angle $\theta$ into half</li>
</ul>
</li>
<li>Reconstruct an incident radiance field per tile<ul>
<li>As the inputs of the networks</li>
<li>The hierarchy is built in the hemisphere of the local frame of a tile<ul>
<li>Defined from the average normal of pixels in the tile</li>
</ul>
</li>
</ul>
</li>
<li>Project the result to the individual frames of the pixels<ul>
<li>Special case: the hemisphere of an individual pixel has certain incident directions (i.e., uncovered domains) that are not covered by the hemisphere of the average local frame exists<ul>
<li>Lead to poor runtime performance</li>
<li>Assign a uniform PDF to each uncovered domain for unbiased sampling</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Radiance-Field-Reconstruction-Using-the-R-network"><a href="#Radiance-Field-Reconstruction-Using-the-R-network" class="headerlink" title="Radiance Field Reconstruction Using the R-network"></a>Radiance Field Reconstruction Using the R-network</h2><ul>
<li><p>Radiance field reconstruction $\mathcal N$ of an incident radiance block $B^j$ is</p>
<p>  $$<br>  \hat L_{in}^{B^j}(\pmb x)&#x3D;\mathcal N(\pmb X,\Xi;\pmb w)<br>  $$</p>
<ul>
<li>$\hat L_{in}^{B^j}(\pmb x)$: the output of the network<ul>
<li>The average incident radiance in $B^j$ at pixel $\pmb x$</li>
</ul>
</li>
<li>$\pmb X$: the incident radiance sample in the domain of $B^j$</li>
<li>$\Xi$: indicate auxiliary features<ul>
<li>e.g., the position, normal and depth</li>
</ul>
</li>
<li>$\pmb w$: the trainable weight and bias term of $\mathcal N$</li>
</ul>
</li>
</ul>
<h2 id="Filtering-4D-Radiance-Space"><a href="#Filtering-4D-Radiance-Space" class="headerlink" title="Filtering 4D Radiance Space"></a>Filtering 4D Radiance Space</h2><ul>
<li>Challenges<ul>
<li>The number of samples per radiance block is smaller than the number of samples per pixel because one pixel has more than one block</li>
<li>Due to the curse of dimensionality, performing convolutions in a 4D space requires higher menory, training time, and data</li>
</ul>
</li>
</ul>
<h3 id="R-Network"><a href="#R-Network" class="headerlink" title="R-Network"></a>R-Network</h3><ul>
<li><p>Differences compared to image-space filtering</p>
<ul>
<li>Samples are disperesd in many directions<ul>
<li>Inputs are sparse at individual radiance field blocks</li>
</ul>
</li>
<li>Direct convolution in the 4D light-field space requires high memory and computation</li>
</ul>
</li>
<li><p>Four different CNNs</p>
<ul>
<li>Image network<ul>
<li>Use image-space auxiliary features and performs image-space convolution</li>
</ul>
</li>
<li>Direction network<ul>
<li>Works with features and convolution in the direction space</li>
</ul>
</li>
<li>Image-direction network (final R-network)</li>
<li>Direction-image network</li>
</ul>
</li>
</ul>
<h3 id="Image-Direction-Network"><a href="#Image-Direction-Network" class="headerlink" title="Image-Direction Network"></a>Image-Direction Network</h3><p><img src="/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/image-20230114195859830.png"></p>
<ul>
<li><p>The image-direction network can be partitioned into image and direction parts</p>
<ul>
<li>$\pmb X^i_\Gamma$: feature map associated with directional block $i$ and pixel tile $\Gamma$</li>
<li>The image part takes some image-space auxiliary feature maps $\pmb G_\Gamma$  (i.e., surface normals, positions, and depth) and radiance feature maps $\pmb R^j_{\Gamma}$ (mean, variance and gradient of the radiance) as inputs</li>
<li>The output is the direction-space feature map $\pmb F_{\pmb d_\Gamma}^j$ <ul>
<li>Learn from the image part as input to simultaneously convolve the radiance predictions of all radiance field blocks</li>
</ul>
</li>
</ul>
</li>
<li><p>The geometrical features have a total of 26 channels as follows:</p>
<ul>
<li>Three channels for the average normal, one channel for the average variance in the normals, and six channels for the gradients of the average normal</li>
<li>Three channels for the average position, one channel for the average variance in the positions, and six channels for the gradients of the average position</li>
<li>One channel for the average depth, one channel for the variance in the depth, and two channels for the gradients of the average depth</li>
<li>Two channels for the gradients of the average radiance of all blocks</li>
</ul>
</li>
<li><p>The radiance features have a total of four channels, which comprise:</p>
<ul>
<li>Three channels for radiance</li>
<li>One channel for average variance in the radiance</li>
</ul>
</li>
</ul>
<h3 id="Experiments-on-Reconstruction-Networks"><a href="#Experiments-on-Reconstruction-Networks" class="headerlink" title="Experiments on Reconstruction Networks"></a>Experiments on Reconstruction Networks</h3><p><img src="/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/image-20230114203015118.png"></p>
<p><img src="/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/image-20230114203043520.png"></p>
<h2 id="DRL-based-Adaptive-Sampling"><a href="#DRL-based-Adaptive-Sampling" class="headerlink" title="DRL-based Adaptive Sampling"></a>DRL-based Adaptive Sampling</h2><ul>
<li><p>Sample distribution and radiance field resolution greatly influence the results</p>
<ul>
<li>A higher number of samples provides richer information for even using well-trained denoising CNNs</li>
<li>Adaptively refining the radiance field is a commonly used strategy to preserve lighting details with a limited budget</li>
</ul>
</li>
<li><p>Propose the use of the DRL-based Q-network to guide the sampling and refinement of the radiance field hierarchy</p>
<ul>
<li>Use DRL to train the network: attempt to cover all the possible radiance field hierarchies and sampling distributions to search for GT are impractical</li>
<li>Treat adaptive sampling as a dynamic process that iteratively takes action to refine radiance field blocks into smaller blocks or to increase the number of samples</li>
<li>The trained Q-network evaluates the value of each action at the runtime to guide the adaptive process</li>
</ul>
</li>
<li><p>Two factors are critical when building the hierarchy:</p>
<ol>
<li>The structure of the hierarchy, i.e., the method for discretizing the radiance field<ul>
<li>Noted in a previous adaptive method: a higher grid resolution can effectively capture high-frequency lighting features, but it comes with an overhead</li>
</ul>
</li>
<li>An adaptive sample distribution<ul>
<li>More samples (i.e., a greater sample density) are placed in those noisy areas (blocks or nodes) to reduce reconstruction errors</li>
</ul>
</li>
</ol>
</li>
</ul>
<h3 id="Deep-Q-Learning"><a href="#Deep-Q-Learning" class="headerlink" title="Deep Q-Learning"></a>Deep Q-Learning</h3><ul>
<li>Input states: the global radiance field information (e.g., geometry information, radiance samples and radiance field hierarchy)</li>
<li>Output: predicts the quality value (Q-value) of possible actions as the output to determine the next action</li>
<li>Action:<ol>
<li>Resample the block by doubling its sample density per block (the number of samples per block)<ul>
<li>Decrease the variance in the radiance feature, which suppresses noise</li>
</ul>
</li>
<li>Refine the radiance field block to 4x4 new blocks by equally partitioning each axis, while keeping the average number of samples per block by adding some new samples<ul>
<li>The goal of maintaining the sample density per block is to prevent the degeneration of the reconstruction quality due to the sparser samples</li>
<li>Increase the resolution of the grid, which can capture high-frequency details</li>
</ul>
</li>
</ol>
</li>
<li>The quality value $Q$ and reward of action $r$ are defined in each radiance field block. For radiance field block $B^j$ at a pixel, the quality value of taking action $a$ in a state $s^j$ is defined by Bellman equation:<br>  $$<br>  Q^j(s^j,a)&#x3D;r(s^j,a)+\gamma\max_{a’}Q^j(s^{‘j’},a’)<br>  $$<ul>
<li>$s^j$ and $s^{‘j}$ correspond to the states before and after action $a$ is taken</li>
<li>$a’$ is a possible next action</li>
<li>$r(s^j,a)$ denotes the reward of the action</li>
<li>$\gamma$ is a decay parameter between 0 and 1</li>
</ul>
</li>
<li>Estimate the Q-value of an action:<ul>
<li>Approximate equation as follows:<br>  $$<br>  Q^j(s^j,a)\approx r(s^j,a)+\gamma\max_{a’}r(s^{‘j},a’)<br>  $$<br>  Define the reward $r(s^j, a)$ as follows:<br>  $$<br>  r(s^j,a)&#x3D;E^j(s^j)-E^j(s’^j)<br>  $$<ul>
<li>$E^j(s^j)$: the reconstruction error of block $B^j$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Reinforcement-Learning-Process"><a href="#Reinforcement-Learning-Process" class="headerlink" title="Reinforcement Learning Process"></a>Reinforcement Learning Process</h3><p><img src="/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/image-20230114214044709.png"></p>
<h3 id="Q-network-Structure"><a href="#Q-network-Structure" class="headerlink" title="Q-network Structure"></a>Q-network Structure</h3><p><img src="/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/image-20230114214441994.png"></p>
<h2 id="Adaptive-Sampling-and-Rendering"><a href="#Adaptive-Sampling-and-Rendering" class="headerlink" title="Adaptive Sampling and Rendering"></a>Adaptive Sampling and Rendering</h2><p>The adaptive sampling and the rendering pipeline contain three steps given the trained networks:</p>
<ol>
<li>Use the trained Q-network to guide the process of adaptive sampling and refine the field blocks<ul>
<li>Result in a hierarchy of radiance field blocks</li>
</ul>
</li>
<li>Use the trained R-network to reconstruct the incoming radiances from the hierarchy</li>
<li>Apply the reconstruction result for the final rendering</li>
</ol>
<h3 id="Adaptive-Sampling-Algorithm"><a href="#Adaptive-Sampling-Algorithm" class="headerlink" title="Adaptive Sampling Algorithm"></a>Adaptive Sampling Algorithm</h3><p><img src="/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/image-20230114215842383.png"></p>
<h3 id="Reconstruction-and-Final-Rendering"><a href="#Reconstruction-and-Final-Rendering" class="headerlink" title="Reconstruction and Final Rendering"></a>Reconstruction and Final Rendering</h3><ul>
<li>Use the image-direction R-network to reconstruct the radiance field blocks of the hierarchy $H_T$<ul>
<li>Generate a fast preview: simply use the reconstructed incident radiance field to evaluate and integrate the product of the incident radiance and the BRDF</li>
<li>Render the unbiased image: treat the reconstructed radiance field and the BRDF as two PDFs to generate the sampling directions, combine those two samplers via MIS<ul>
<li>The BRDF samples can be analytically drawn from a cumulative distribution function</li>
<li>The reconstructed radiance field samples are generated by initially selecting a radiance field block from a discrete PDF and then proportionally sampling a point from the block according to the cosine weighting term</li>
</ul>
</li>
</ul>
</li>
<li>As multi-bounce vertices are sparse, they are not compatible with the input format of the networks<ul>
<li>Switch to standard multiple importance sampling</li>
<li>Adapt to other photon guided methods if the lighting is complex</li>
</ul>
</li>
</ul>
<h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><p><img src="/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/image-20230114222118357.png"></p>
<p><img src="/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/image-20230114222235529.png"></p>
<h2 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h2><ul>
<li>BRDF term is not considered</li>
<li>Focus on first-bounce radiance field reconstruction</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>Post author:  </strong>Chaf Chen
  </li>
  <li class="post-copyright-link">
      <strong>Post link: </strong>
      <a href="https://chaphlagical.github.io/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/" title="Adaptive Incident Radiance Field Sampling and Reconstruction Using Deep Reinforcement Learning">https://chaphlagical.github.io/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/Rendering/" rel="tag"># Rendering</a>
              <a href="/tags/TOG-2020/" rel="tag"># TOG 2020</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/01/06/paper_reading/Temporal_Coherence-Based_Distributed_Ray_Tracing_of_Massive_Scenes/" rel="prev" title="Temporal Coherence-based Distributed Ray Tracing of Massive Scenes">
                  <i class="fa fa-chevron-left"></i> Temporal Coherence-based Distributed Ray Tracing of Massive Scenes
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






    
  <div class="comments" id="disqus_thread">
    <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
  </div>
  
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chaf Chen</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.2.2/mermaid.min.js","integrity":"sha256-o5AUfgOkNFWuInA78GVCoVdGoN9eTaB3Hxu+ep7TTrY="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="disqus" type="application/json">{"enable":true,"shortname":"chaphlagical-github-io","count":true,"i18n":{"disqus":"disqus"}}</script>
<script src="/js/third-party/comments/disqus.js"></script>

</body>
</html>
