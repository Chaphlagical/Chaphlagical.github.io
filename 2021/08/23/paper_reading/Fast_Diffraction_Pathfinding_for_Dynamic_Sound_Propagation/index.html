<!DOCTYPE html><html lang="[&quot;en&quot;,&quot;zh-CN&quot;,&quot;default&quot;]" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>Fast Diffraction Pathfinding for Dynamic Sound Propagation | Chaf's Blog</title><meta name="author" content="Chaf Chen"><meta name="copyright" content="Chaf Chen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="Siggraph 2021 paper reading">
<meta property="og:type" content="article">
<meta property="og:title" content="Fast Diffraction Pathfinding for Dynamic Sound Propagation">
<meta property="og:url" content="https://chaphlagical.github.io/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/index.html">
<meta property="og:site_name" content="Chaf&#39;s Blog">
<meta property="og:description" content="Siggraph 2021 paper reading">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://chaphlagical.github.io/img/logo.jpg">
<meta property="article:published_time" content="2021-08-23T21:20:00.000Z">
<meta property="article:modified_time" content="2023-03-02T13:45:39.507Z">
<meta property="article:author" content="Chaf Chen">
<meta property="article:tag" content="Siggraph 2021">
<meta property="article:tag" content="Sound Rendering">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chaphlagical.github.io/img/logo.jpg"><link rel="shortcut icon" href="/img/logo.jpg"><link rel="canonical" href="https://chaphlagical.github.io/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"Traditional Chinese Activated Manually","cht_to_chs":"Simplified Chinese Activated Manually","day_to_night":"Dark Mode Activated Manually","night_to_day":"Light Mode Activated Manually","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Fast Diffraction Pathfinding for Dynamic Sound Propagation',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-03-02 13:45:39'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><script>const preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> { preloader.endLoading() })

if (false) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/logo.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">33</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="Chaf's Blog"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/logo.jpg"/><span class="site-name">Chaf's Blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Fast Diffraction Pathfinding for Dynamic Sound Propagation</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2021-08-23T21:20:00.000Z" title="Created 2021-08-23 21:20:00">2021-08-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-03-02T13:45:39.507Z" title="Updated 2023-03-02 13:45:39">2023-03-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper-Reading/">Paper Reading</a></span></div><div class="meta-secondline"></div></div></div><article class="post-content" id="article-container"><p><strong>Author</strong>: Carl Schissler, Gregor Mückl, Paul Calamia</p>
<p><strong>Institution</strong>: Facebook Reality Labs Research, USA</p>
<p><strong>Link</strong>: <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3450626.3459751">https://dl.acm.org/doi/10.1145/3450626.3459751</a></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>Diffraction is one the the most perceptually important yet difficult to simulate acoustic effects<ul>
<li>A phenomenon that allows sound to propagate around obstructions and corners</li>
</ul>
</li>
<li>A significant bottleneck in real-time simulation of diffraction:<ul>
<li>The enumeration of high-order diffraction propagation paths in scenes with complex geometry</li>
</ul>
</li>
<li>The paper present a dynamic geometric diffraction approach that consists of an extensive mesh preprocessing pipeline and complementary runtime algorithm<ul>
<li>Preprocessing module identifies a small subset of edges that are important for diffraction using a novel silhouette edge detection heuristic<ul>
<li>It also extends these edges with planar diffraction geometry and precomputes a graph data structure encoding the visibility between the edges</li>
</ul>
</li>
<li>The runtime module uses bidirectional path tracing against the diffraction geometry to probabilistically explore potential paths between sources and listeners, then evaluates the intensities for these paths using the Uniform Theory of Diffraction<ul>
<li>It uses the edge visibility graph and the A* pathfinding algorithm to robustly and efficiently find additional high-order diffraction paths</li>
</ul>
</li>
</ul>
</li>
<li>The paper demonstrate how this technique can simulate 10th-order diffraction up to 568 times faster than the previous state of the art, and can efficiently handle large scenes with both high geometric complexity and high numbers of sources</li>
</ul>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><ul>
<li>In order to generate a convincing simulation of reality, the senses must be provided with plausible recreations of the real world  <ul>
<li>For virtual reality (VR) and augmented reality (AR) applications, high quality audio is especially important to create immersion and a sense of presence</li>
<li>Audio sources must be rendered in a way that seems plausible to the user, i.e. where the percept matches the user’s expectation</li>
<li>Involving simulating how the sounds emitted by sources interact with the virtual environment through reverberation, reflections, and diffraction, among other acoustic phenomena</li>
</ul>
</li>
<li>One of the most difficult to simulate yet perceptually important acoustic effects in a geometric acoustics (GA) framework is diffraction<ul>
<li>Diffraction is a wave scattering phenomenon that occurs when sound interacts with a feature in the environment whose size is similar to the wavelength</li>
<li>With proper simulation of diffraction, sources that become occluded from view are still audible but become low-pass filtered</li>
<li>In a GA simulation that does not handle diffraction, occluded sources will be abruptly silenced, resulting in a jarring unnatural transition that has the potential to break the perceived plausibility of the auralization</li>
</ul>
</li>
<li>The paper present a new efficient approach for simulating diffraction within a real-time GA framework that allows for dynamic motion of rigid geometry and for high-order diffraction<ul>
<li>Focus on a particular subset of the diffraction problem: the simulation of direct diffraction only <ul>
<li>Direct diffraction is defined as diffraction that occurs directly between a source and listener with no reflections involved</li>
</ul>
</li>
<li>The approach is able to simulate perceptually-important features<ul>
<li>Particularly the smooth transition from unoccluded to occluded state</li>
<li>Ignoring more complex paths which can be difficult to identify but contribute less to the overall sound field</li>
</ul>
</li>
</ul>
</li>
<li>Main contributions:<ul>
<li>A mesh preprocessing approach that extracts a reduced subset of silhouette diffraction edges and augments the mesh with diffraction flag geometry</li>
<li>A runtime approach for efficiently finding high-order diffraction paths using stochastic bidirectional path tracing and a persistent cache of paths</li>
<li>A complementary approach that uses a precomputed edge-to-edge visibility graph and the A* algorithm to quickly find high-order diffraction paths</li>
</ul>
</li>
</ul>
<h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><h3 id="2-1-Sound-Propagation"><a href="#2-1-Sound-Propagation" class="headerlink" title="2.1. Sound Propagation"></a>2.1. Sound Propagation</h3><ul>
<li>Based on solving the acoustic wave equation (wave-based methods)<ul>
<li>Methods:<ul>
<li>Finite Difference Time Domain (DFTD)</li>
<li>Finite Element Method (FEM)</li>
<li>Boundary Element Method (BEM)</li>
</ul>
</li>
<li>Pros<ul>
<li>Accurate</li>
</ul>
</li>
<li>Cons<ul>
<li>Very computationally intensive</li>
<li>Limited to precomputation and static scenes</li>
</ul>
</li>
</ul>
</li>
<li>Geometric acoustics algorithm<ul>
<li>Pros<ul>
<li>Can simulate reflection, scattering, and reverberation efficiency</li>
</ul>
</li>
<li>Cons<ul>
<li>Do not handle wave phenomena like diffraction<ul>
<li>Make the high-frequency assumption that sound travels as a ray, not a wave</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2-2-Diffraction-for-Geometric-Acoustic"><a href="#2-2-Diffraction-for-Geometric-Acoustic" class="headerlink" title="2.2. Diffraction for Geometric Acoustic"></a>2.2. Diffraction for Geometric Acoustic</h3><ul>
<li>Diffraction models that are applicable to GA generally consider the case of diffraction over one or more edges of the scene geometry<ul>
<li>Diffraction order: the number of edges in a path</li>
</ul>
</li>
<li>Uniform Theory of Diffraction (UTD)<ul>
<li>Given a source, listener, and sequence of diffraction edges, the UTD can analytically compute an approximation for diffracted sound field</li>
<li>Fast to evaluate, therefore UTD is attractive for real-time simulation</li>
<li>Drawback: it assumes every edges to be infinitely long<ul>
<li>May cause implausible results</li>
</ul>
</li>
</ul>
</li>
<li>Biot-Tolstoy-Medwin  (BTM) diffraction method<ul>
<li>Doesn’t have UTD’s limitation</li>
<li>Require significantly more compute to evaluate</li>
</ul>
</li>
<li>Diffraction based on the uncertainty principle (UP)<ul>
<li>Use stochastic ray tracing in a Monte Carlo integrator to compute the diffracted sound field</li>
<li>It can be easily integrated into existing path tracing algorithm</li>
<li>It has slow convergence that makes it unsuitable for real-time application</li>
</ul>
</li>
<li>Other object-based diffraction approaches<ul>
<li>Used a hybrid of precomputed wave simulation and ray tracing to simulate sound scattering around objects</li>
<li>2D rasterization-based method for approximating occlusion that compares the diffracted path length to the straight-line distance between the source and listener</li>
<li>Uses a dense volumetric sampling of rays around existing direct and reflected propagation path segments to approximate the BTM magnitude response</li>
</ul>
</li>
<li>Main computational challenge<ul>
<li>For either UTD or BTM</li>
<li>Find sequences of edges that can form valid diffraction paths</li>
<li>High-order diffraction involves considering the interaction of every edge with every other edge recursively<ul>
<li>A naive approach is to recursively consider all pairs of edges, but this has complexity $O(N^d)$, where $N$ is the number of edges and $d$ is the maximum diffraction order</li>
</ul>
</li>
</ul>
</li>
<li>Optimization<ul>
<li>Frustum and beam tracing have been used to find diffraction paths more efficiently<ul>
<li>Frusta or beams are emitted from the source and propagated through the environment to find paths</li>
<li>Have difficulty scaling to complex geometries or to high diffraction order due to the large number of child beams and time spent on intersection testing</li>
</ul>
</li>
<li>Used ray tracing from sources to detect first-order diffraction edges, followed by a traversal of a precomputed edge-to-edge visibility graph to find all diffraction paths originating at those edges<ul>
<li>Reduce the number of edges considered and allowed computation of diffraction up to order 3 or 4 in real time</li>
<li>But retains exponential algorithmic complexity</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2-3-Mesh-Simplification-for-Acoustics"><a href="#2-3-Mesh-Simplification-for-Acoustics" class="headerlink" title="2.3. Mesh Simplification for Acoustics"></a>2.3. Mesh Simplification for Acoustics</h3><ul>
<li>Compared to graphics rendering, room acoustic simulation is quite tolerant to aggressive geometry simplification, provided that properties of the environment such as volume and surface area are preserved<ul>
<li>Due to the long wavelengths of low-frequency sound, low spatial resolution of spatial audio as well as the diffuse nature of late reverberation</li>
<li>Simplification also tends to increase the size of planar surfaces relative to the wavelength, which may improve accuracy for GA</li>
</ul>
</li>
<li>Methods<ul>
<li>Extract significant faces using a regular grid subdivision of the scene followed by clustering and bounding box fitting to approximate the input geometry<ul>
<li>Drastically reduce the number of faces in a mesh but it does not preserve details, topology, or scene volume</li>
</ul>
</li>
<li>Use a remeshing approach to first voxelize the scene and then extract an isosurface<ul>
<li>This isosurface extraction was followed by coplanar face merging to reduce the number of faces, and post-processing to patch cracks in the mesh surface</li>
</ul>
</li>
<li>Use a remeshing approach to simplify the mesh, except that the edge collapse algorithm was used instead of coplanar face merging<ul>
<li>Generated different geometry for each simulation wavelength using proportionally-sized voxel grids, and applied a parallel edge merging step to reduce the number of edges considered for diffraction</li>
</ul>
</li>
<li>Performed frequency-dependent simplification and used meshes with varying level of detail to speed up real-time simulation<ul>
<li>Using time-dependent geometry, where lower levels of detail would be used for higher-order reflections</li>
<li>To reduce the number of edges considered for diffraction, compared the angle between adjacent faces to a threshold as a way to select significant diffraction edges</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="3-Overview"><a href="#3-Overview" class="headerlink" title="3. Overview"></a>3. Overview</h2><ul>
<li>Preprocessing stage<ul>
<li>Apply a series of operations to first simplify input meshes, then identify important silhouette diffraction edges using a ray-based heuristic</li>
<li>Augment the edges with additional diffraction geometry and also precompute a visibility graph between the edges that is used to accelerate the runtime exploration of high-order diffraction paths</li>
</ul>
</li>
<li>Runtime stage<ul>
<li>Use bidirectional stochastic ray tracing to explore possible diffraction paths, then validate those paths using robust visibility tests<ul>
<li>The resulting paths are stored in a persistent cache to add temporal stability over successive simulation updates</li>
</ul>
</li>
<li>Utilize the precomputed edge visibility graph and the A* pathfinding algorithm to efficiently and robustly find additional high-order diffraction paths</li>
<li>The output of the runtime simulation module is a collection of frequency-dependent room impulse response parameters<ul>
<li>These parameters are the input to the audio rendering module which uses standard signal processing techniques to auralize the impulse response parameters</li>
</ul>
</li>
<li>The final output audio is spatialized using the listener’s head-related transfer function and then reproduced over headphones</li>
</ul>
</li>
</ul>
<h2 id="4-Diffraction-Mesh-Preprocessing"><a href="#4-Diffraction-Mesh-Preprocessing" class="headerlink" title="4. Diffraction Mesh Preprocessing"></a>4. Diffraction Mesh Preprocessing</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/image-20210821161415010.png" alt="image-20210821161415010"></p>
<ul>
<li>Input: Raw triangle mesh with acoustic materials assigned to each triangle</li>
<li>Stages:<ol>
<li>Apply standard mesh simplification algorithms to reduce the input mesh complexity to a given error tolerance</li>
<li>Identify a subset of the edges of the mesh that are relevant for diffraction using a novel silhouette edge identification method and apply additional post-processing to simplify and cluster the selected edges<ul>
<li>Output: A collection of mesh boundaries<ul>
<li>Sequences of edges that make up the same logical diffraction edge</li>
</ul>
</li>
<li>An important part: decouple the diffraction edges from the underlying surface geometry to reduce the number of edges considered for diffraction at runtime</li>
</ul>
</li>
<li>Augment the simplified mesh with additional diffraction geometry (flags) that bisect the outside angle of each boundary<ul>
<li>These flags are used at runtime to detect when a ray passes nearby a diffraction edge, similar to the Uncertainty Principle method</li>
</ul>
</li>
<li>Precomputing a graph of edge-to-edge visibility that can be used to accelerate pathfinding during the runtime algorithm</li>
</ol>
</li>
<li>In the case of moving geometry, apply this pipeline separately to each rigid part of the scene</li>
</ul>
<h3 id="4-1-Mesh-Simplification"><a href="#4-1-Mesh-Simplification" class="headerlink" title="4.1. Mesh Simplification"></a>4.1. Mesh Simplification</h3><ul>
<li>Apply standard mesh simplification approaches to reduce the overall geometric complexity</li>
<li>Benefits<ul>
<li>By reducing the number of triangles, the number of edges that must be considered in the other preprocessing stages is also reduced</li>
<li>After simplification some geometric connectivity problems may be corrected</li>
<li>Improves the consistency of the local mesh curvature and this helps with correct identification of diffraction edges</li>
</ul>
</li>
<li>Make use of vertex welding and edge collapse operations<ul>
<li>Forgo the use of voxelization and marching cubes due to the artifacts that can be introduced when using large voxels, as well as their tendency to actually increase the number of diffraction edges by beveling sharp corners after surface reconstruction</li>
</ul>
</li>
<li>Vertex welding is applied by greedily clustering each vertex with its neighbors within a certain tolerance distance $\epsilon_{weld}&#x3D;0.001\mathrm m$<ul>
<li>Use a spatial hashing approach to implement this with $O(N)$ time complexity</li>
</ul>
</li>
<li>Edge collapse algorithm is similar to the standard approach, but with a few extensions<ul>
<li>Limit the maximum amount of error that can be introduced in triangles normals to $\epsilon_m&#x3D;10°$, while the original algorithm only prevents flipping of triangles ($\epsilon_n&#x3D;90°$)<ul>
<li>Help to preserve the overall shape of the mesh better, particularly at the silhouette edges and mesh corners which are important for diffraction</li>
</ul>
</li>
<li>Prevent simplification across acoustic material boundaries</li>
</ul>
</li>
</ul>
<h3 id="4-2-Diffraction-Edge-Extraction"><a href="#4-2-Diffraction-Edge-Extraction" class="headerlink" title="4.2. Diffraction Edge Extraction"></a>4.2. Diffraction Edge Extraction</h3><h4 id="4-2-1-Initial-Edge-Selection"><a href="#4-2-1-Initial-Edge-Selection" class="headerlink" title="4.2.1. Initial Edge Selection"></a>4.2.1. Initial Edge Selection</h4><ul>
<li>Determine which edges in a mesh are relevant for diffraction<ul>
<li>Too few edges are selected -&gt; Cause some diffraction paths to be missed -&gt; Abrupt occlusion</li>
<li>Use only the edges that can produce significant diffraction in order to get the best performance</li>
<li>Identify those edges is a non-trivial task</li>
</ul>
</li>
<li>Edges that are between two faces with similar normals are unlikely to produce any significant diffraction<ul>
<li>Previous approach: Dihedral angle to classify edges as diffracting<ul>
<li>The angle between adjacent face normals $\theta_s$, is compared to a threshold angle $\epsilon_\theta$, to determine if the shared edge is a diffraction edge</li>
<li>If $\theta_s&gt;\epsilon_\theta$, that edge is classified as a diffraction edge</li>
<li>This can greatly overestimate the number of diffraction edges for highly-tessellated curved surfaces because it uses only local information<ul>
<li>This is especially problematic for highly tessellated meshes or curved surfaces that have $\theta_s$ close to 0</li>
<li>In such meshes, the value of $\epsilon_\theta$ must be close to 0 to find all relevant edges, which cause many extraneous edges to also be selected -&gt; poor runtime performance</li>
</ul>
</li>
<li>It’s difficult to find a value $\epsilon_\theta$ that works robustly for all inputs</li>
</ul>
</li>
<li>A novel diffraction edge identification method based on face normal clustering<ul>
<li>Adapt Felzenszwalb graph segmentation algorithm<ul>
<li>Face adjacency graph</li>
</ul>
</li>
<li>Cluster adjacent faces that have similar surface normals</li>
<li>The boundaries between the face clusters are then used as the initial set of diffraction edges</li>
<li>Compared to existing approaches based on local curvature, this approach works well on meshes with any level of tessellation</li>
</ul>
</li>
</ul>
</li>
<li>The novel diffraction edge identification method<ol>
<li>Computing the weight for each edge between adjacent faces in the face adjacency graph<ul>
<li>Propose using the cosine of the angle between each pair of adjacent face normals, $w(f_i,f_j)&#x3D;\cos(\theta_s)&#x3D;\vec n_i\cdot\vec n_j$</li>
</ul>
</li>
<li>These weights are sorted in decreasing order, such that face pairs that have more similar normals come first<ul>
<li>Each face in the mesh is initially assigned to its own unique cluster, where each cluster maintains information about the distribution of surface normals for faces that belong to the cluster</li>
<li>Represent the normal distribution using a cone where the axial direction $\vec a$ approximates the average normal and the opening angle $\theta_{\vec n}$ approximates the spread of normals within the cluster</li>
<li>The algorithm proceeds by inspecting each face pair in order of decreasing weight and evaluating whether or not the clusters that the faces belong to can be merged</li>
</ul>
</li>
<li>To determine if merging two clusters is possible:</li>
</ol>
<ul>
<li>Compute the merged normal cone for the two clusters, i.e. the smallest cone that contains both merged cones<ul>
<li>Given two cones $i$ and $j$, this can be efficiently approximated by first computing the vector $\vec x_i$, on the boundary of cone $i$ that has the greatest angle with the axis of cone $j$​, and vice versa to yield $\vec x_j$</li>
<li>The average of the two extreme<br>vectors is then used as the merged cone axis and the angle between them is used as the opening angle of the cone $\theta_{\vec n}$<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/image-20210822093047703.png" alt="image-20210822093047703.png"></li>
<li>The clusters are then merged if $\theta_\vec n$ is less than merging threshold for either cluster<ul>
<li><p>The merging threshold is maintained separately for each cluster and is initially set to $\tau&#x3D;\epsilon_\theta$ at the beginning of the algorithm, where $\epsilon_\theta$ is the minimum dihedral angle to consider for diffraction</p>
</li>
<li><p>After two clusters are merged, the resulting cluster’s threshold is increased according to the following relation:</p>
<p>  $$<br>  \tau(F_i\cup F_j)&#x3D;\theta_{\vec n}+\frac{k_{\epsilon_\theta}}{|F_i|+|F_j|}<br>  $$</p>
<p>  where $|F_i|$ and $|F_j|$ represent the number of faces in the constituent clusters<br>  where $k&#x3D;4$ is a parameter that controls the scale of the clusters</p>
</li>
<li><p>This has the effect of requiring stronger evidence for a boundary between small clusters</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ol start="4">
<li>Merge clusters smaller than a certain threshold with adjacent larger clusters<ul>
<li>This is necessary in the case of noisy mesh data such as that from 3D reconstructions where there may be occasional small clusters that are not included in the cluster for a large flat wall</li>
<li>For each cluster that is considered too small, we merge it into the neighboring cluster that has the most similar average surface normal</li>
</ul>
</li>
<li>Once the face clusters are computed, extract the boundaries between the clusters as the initial set of diffraction edges<ul>
<li>Each boundary is a set of edges that have adjacent faces belonging to the same 2 clusters</li>
<li>These boundaries are then provided to the next stage of the preprocessing pipeline</li>
</ul>
</li>
</ol>
</li>
</ul>
<h4 id="4-2-2-Curved-Boundary-Splitting"><a href="#4-2-2-Curved-Boundary-Splitting" class="headerlink" title="4.2.2. Curved Boundary Splitting"></a>4.2.2. Curved Boundary Splitting</h4><ul>
<li><p>The face clusters in the previous step can have any shape</p>
<ul>
<li>There is no restriction on the collinearity of the edges that make up a cluster boundary</li>
<li>In conflict with the final goal of turning each boundary into a single straight diffraction edge</li>
<li>Propose a simple approach for splitting mesh boundaries into collinear segments</li>
</ul>
</li>
<li><p>Curved Boundary Splitting</p>
<ul>
<li>Calculate a bounding cylinder of the vertices<ul>
<li>Axis<ul>
<li>The dominant direction of the boundary</li>
<li>Defined by the two vertices in the boundary that are farthest apart</li>
</ul>
</li>
<li>Radius<ul>
<li>A measure of how collinear the vertices are</li>
<li>Given by the maximum distance of a boundary vertex from the axis line segment</li>
</ul>
</li>
<li>The cylinder is used to determine whether or not a boundary should be split into more than one boundary</li>
</ul>
</li>
<li>A boundary should be split if the aspect ratio of its bounding cylinder $\Big(\frac{2r}{h}\Big)$ is more than a certain threshold, e.g. 0.025<ul>
<li>The splitting point is chosen to be the boundary vertex that is farthest from the cylinder’s axis</li>
<li>The edges that are on either side of the split are placed into one of the two resulting boundaries<ul>
<li>These new boundaries are then recursively split until their aspect ratio falls below the threshold</li>
</ul>
</li>
</ul>
</li>
<li>The output of this stage is a collection of mesh boundaries that are known to be approximately straight.</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/image-20210822151840621.png" alt="image-20210822151840621.png"></p>
</li>
</ul>
<h4 id="4-2-3-Silhouette-Edges"><a href="#4-2-3-Silhouette-Edges" class="headerlink" title="4.2.3. Silhouette Edges"></a>4.2.3. Silhouette Edges</h4><ul>
<li><p>Only consider the diffraction that occurs in the shadow region of an edge</p>
<ul>
<li>The only edges that can produce diffraction are silhouette edges</li>
<li>i.e. Those edges that can cast a “shadow” when illuminated from a point in the conservative shadow region<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/image-20210822152155452.png" alt="image-20210822152155452.png"><ul>
<li>The edge axis is perpendicular to the image</li>
<li>The edge is shared by faces $f_0$ and $f_1$ that have face normals $\vec n_0$ and $\vec n_1$</li>
<li>The exterior angle between $f_0$ and $f_1$ is bisected by edge normal $\vec n_e$ and a planar diffraction flag that extend a distance $d_{flag}$ from the edge</li>
<li>Conservative shadow regions: defined by the planes of $f_0$ and $f_1$</li>
<li>Shadow regions: bounded by the horizon plane and plane of face $f_{shadow}$<ul>
<li>The horizon plane, with normal vector $\vec h$: defined by the edge vertices and the reference point $\vec p_{ref}$, which corresponds to a source, listener, or point on a previous diffraction edge, calculated as $\vec h&#x3D;(\vec v_1-\vec v_0)\times (\vec p_{ref}-\vec v_0)$</li>
</ul>
</li>
<li>$\epsilon_h$ and $\epsilon_f$ extend the shadow region on the horizon and face sides</li>
<li>Enable smoother transitions between direct and diffracted state</li>
</ul>
</li>
</ul>
</li>
<li><p>A novel approach to reliably identify these silhouette edges</p>
<ul>
<li>Utilize global information about the structure of the mesh acquired through stochastic ray tracing from points on an edge to determine whether or not a given edge is a silhouette</li>
<li>Based on the observation that in order for an edge to contribute to diffraction, it must be able to cast a shadow, and that a source or listener with non-zero size must be able to go into the conservative shadow region on both sides of the edge</li>
<li>Main idea: if there are other parts of the mesh that completely obstruct one or both sides of a given edge such that no sound source or listener can form a shadowed path over the edge, that edge cannot produce any diffraction paths</li>
</ul>
</li>
<li><p>Algorithm</p>
<ul>
<li><p>The information can be determined approximately by stochastic ray tracing in the conservative shadow regions (CSR) of each edge</p>
<ul>
<li>Emit rays that randomly sample the CSR from uniformly-sampled random points on the edge<ul>
<li>The number of rays traced for an edge, $N_{samp}$, is determined by its length and the angular size of the CSR:<br>$$<br>N_{samp}&#x3D;\min\Bigg(N_{samp}^{max}, \dfrac{|\vec v_1-\vec v_0|_2}{h_d}\dfrac{\theta_s}{h_\theta} \Bigg)<br>$$<ul>
<li>$\theta_s&#x3D;\cos^{-1}(\vec n_0\cdot \vec n_1)$ is the angular size of the CSR</li>
<li>$h_d&#x3D;0.1\mathrm m$ is the distance sampling resolution</li>
<li>$h_\theta&#x3D;5°$ is the angular sampling resolution</li>
<li>In practice, $N_{samp}$ is limited to a reasonable maximum value, e.g. $10^4$<ul>
<li>To prevent spending too much time on very long edges</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>To sample the outgoing ray direction, use a uniform spherical distribution that has been modified to generate rays in a wedge<br>shape<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/image-20210822160819792.png" alt="image-20210822160819792.png"><ul>
<li><p>The outgoing ray direction in the local tangent space is given by:<br>$$<br>\vec r_d&#x3D;\Big(u_0, \sqrt{1-u_0^2}\sin u_1, \sqrt{1-u_0^2}\cos u_1 \Big)<br>$$</p>
<ul>
<li>$u_0$: a uniform random variable in the range $[0,\theta_s)$</li>
<li>$u_1$: a uniform random variable in the range $[-\sin\theta_r, \sin \theta_r]$<ul>
<li>$\theta_r&#x3D;30°$ controls the amount of spreading of the rays in the direction of the edge axis</li>
<li>$\theta_r&#x3D;0$ would generate rays that are always perpendicular to the edge</li>
</ul>
</li>
</ul>
</li>
<li><p>Once the local ray direction is generated, it is rotated to mesh space by applying the orthonormal rotation matrix $\pmb R_i&#x3D;\begin{bmatrix}\frac{\vec v_1-\vec v_0}{|\vec v_1-\vec v_0|_2},\vec n_i,\vec t_i\end{bmatrix}$, where $i$ is the face index</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Classify an edge as silhouette if both sides of the CSR have at least $N_{valid}&#x3D;1$ rays that don’t hit anything within a certain distance, $\tilde d_s$</p>
<ul>
<li><p>Use this information as a proxy for whether or not a source or listener can be occluded by the edge</p>
</li>
<li><p>The distance $\tilde d_s$ is proportional to the diameter of a source or listener, $d_s$</p>
<ul>
<li>As a source or listener grows bigger, an edge must protrude further from the nearby geometry to produce any diffraction</li>
<li>$d_s$ is a parameter of our algorithm that controls how aggressive the silhouette edge detection is<ul>
<li>Here use $d_s&#x3D;0.25\mathrm m$, which roughly corresponds to the size of a human head</li>
</ul>
</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/image-20210822162719475.png" alt="image-20210822162719475.png"></p>
<ul>
<li>Yellow circle: a sound source or listener with diameter $d_s$</li>
<li>Red rays: represent random rays generated by $\vec r_d&#x3D;\Big(u_0, \sqrt{1-u_0^2}\sin u_1, \sqrt{1-u_0^2}\cos u_1 \Big)$ </li>
<li>In this figure:<ol>
<li>Edge (a) is not silhouette edge because the circle cannot be placed where it is completely occluded by the edge and all of the rays hit<br>another face before traveling distance $\tilde d_s$</li>
<li>Edge (b) is classified as a silhouette edge because at least $N_{valid}$ rays were able to travel for a least distance $\tilde d_s$ and because the circle can be occluded by the edge</li>
</ol>
</li>
</ul>
</li>
<li><p>In order for the approach to work correctly, the value of $\tilde d_s$ must increase for rays that are closer to the CSR boundary</p>
<ul>
<li><p>If $\tilde d_s$ did not increase for rays near the CSR boundary, Edge (a) would be erroneously classified as a silhouette edge</p>
</li>
<li><p>The value of $\tilde d_s$ for each ray using the following relation:<br>$$<br>\tilde d_s&#x3D;\dfrac{d_s}{\max(\cos(u_0),\epsilon)}<br>$$<br>This causes the threshold distance to increase substantially for rays that have large $u_0$</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Summary of silhouette detection algorithm</p>
<ul>
<li>Inspects every input edge and traces rays to determine if that edge is a silhouette</li>
<li>If at least $N_{valid}$ rays on both sides of an edge are able to travel a distance of at least $\tilde d_s$​ before hitting other geometry, then that edge is classified  as a silhouette</li>
</ul>
</li>
</ul>
<h4 id="4-3-Diffraction-Geometry-Construction"><a href="#4-3-Diffraction-Geometry-Construction" class="headerlink" title="4.3. Diffraction Geometry Construction"></a>4.3. Diffraction Geometry Construction</h4><ul>
<li><p>Inspired by the Uncertainty Principle (UP) diffraction approach</p>
<ul>
<li>Augmenting the main geometry with diffraction flags - quadrilaterals that bisect the outside angle of diffraction edges and protrude a distance proportional to the wavelength of the lowest frequency band, e.g. $d_{flag}&#x3D;6\lambda$</li>
</ul>
</li>
<li><p>Construct a similar set of diffraction flags but do not require any particular flag length, $d_{flag}$</p>
<ul>
<li>The accuracy of the diffraction approach does not depend on the flag length due to the use of the analytical UTD diffraction model</li>
<li>Changing the flag length changes the diffracted sound intensity for UP because in that model the flag is an integral domain</li>
<li>In this paper’s approach, the length of the diffraction flags controls how likely it is for a ray to intersect a flag and find diffraction paths over the associated edge<ul>
<li>Use $d_{flag}&#x3D;1.0\mathrm m$ as a reasonable tradeoff between finding enough diffraction paths and spending too much time on ray-versus-flag intersection tests for rays as they traverse the scene</li>
</ul>
</li>
</ul>
</li>
<li><p>Algorithm</p>
<ul>
<li><p>Convert the input mesh boundaries, each made up of one or more edges, into singular diffraction edges that act as proxies for the underlying surface geometry</p>
<ul>
<li>Decouple the surface geometry representation from the edges used to compute diffraction effects</li>
<li>Each roughly collinear mesh boundary is approximated with a single straight edge<ul>
<li>Apply the approach discussed in previous section <em>Curved Boundary Splitting</em> a second time to compute the best-fitting proxy edge for a mesh boundary</li>
</ul>
</li>
<li>Calculate the local geometric information needed for diffraction, namely the adjacent face normals of the proxy edge<ul>
<li>Compute the area-weighted average of the face normals on each side of the boundary after assigning each adjacent face to one side or another based on the similarity of its normal vector to the faces processed so far</li>
</ul>
</li>
</ul>
</li>
<li><p>Determine where to place the two far vertices for each flag</p>
<ul>
<li><p>The simplest approach</p>
<ul>
<li><p>Place the far vertices at distance $d_{flag}$ form each edge vertex in the direction of the edge normal, e.g. $\vec v_i+\vec n_e d_{flag}$</p>
<ul>
<li>Work well in many cases, but can fail with certain meshes</li>
<li>Consider the diffraction edges at the top ring of a tessellated cylinder, like follow figure:</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/image-20210822201205087.png" alt="image-20210822201205087.png"></p>
<ul>
<li>Placing the far vertices along the edge normal produces many gaps in the ring of flags that may reduce the effectiveness of the runtime diffraction algorithm</li>
</ul>
</li>
<li><p>Using the vertex normals rather than the edge normal to determine the far vertex locations, e.g. $\vec v_i+\vec n_{v_i}d_{flag}$</p>
<ul>
<li>Exception: if both vertex normals point toward the center of the edge, we use the edge normal instead</li>
<li>Use vertex normals on the convex parts of the mesh and edge normals on the concave parts</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Support intersecting rays against either the surface mesh or the flags</p>
<ul>
<li>Put the additional flag geometry in a separate mesh and acceleration structure with the same transformation as the surface mesh</li>
<li>A bitmask is then used by the ray tracer to select what type(s) of geometry each ray should intersect with<ul>
<li>Flags should not interfere with next event estimation in the path tracer or line-of-sight checks in the runtime diffraction algorithm</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="4-4-Diffraction-Graph"><a href="#4-4-Diffraction-Graph" class="headerlink" title="4.4. Diffraction Graph"></a>4.4. Diffraction Graph</h3><ul>
<li>Build a separate directed edge-to-edge visibility graph between the final set of diffraction edges for each rigid mesh in the scene<ul>
<li>Used in the runtime graph traversal algorithm to speed up the search for diffraction paths </li>
<li>The data structure is a flat array of edge neighbor indices</li>
<li>Generate the graph in a different way that scales better to complex scenes with many edges<ul>
<li>In practice most edges can only diffract with a few neighbors</li>
</ul>
</li>
<li>Handles approximate partial visibility and has $O(N)$ time complexity</li>
<li>Utilize the diffraction flag geometry along with stochastic ray tracing to determine whether or not edges are mutually visible</li>
<li>For each edge $e_i$ in the mesh, the paper emit random rays in the conservative shadow regions according to the same distribution used to generate silhouette rays (See Section 4.2.3), but with $\theta_r&#x3D;60°$<ul>
<li>These rays are then intersected with the surface mesh to find the ray endpoint at distance $d_{max}$</li>
<li>The same ray is intersected with the diffraction flags to find all hits along the ray up to distance $d_{max}$<ul>
<li>For each flag intersection, we check to see if the associated edge, $e_j$, is in the CSR of the edge $e_i$ that emitted the rays</li>
<li>This condition is met when the signed distance of an edge endpoint to the face planes of the other edge is more than $\epsilon_f$ for one plane and less than $\epsilon_f$ for the other<ul>
<li>Additional tolerance $\epsilon_f$ that prevents edge pairs that share a face plane from being discarded</li>
</ul>
</li>
</ul>
</li>
<li>It this succeeds, a directed link is added to the graph from edge $e_i$ to edge $e_j$</li>
</ul>
</li>
</ul>
</li>
<li>Another improvement<ul>
<li>Make to the graph data structure is to partition the links originating from a given edge into two sets corresponding to the two sides (i.e. CSR) of the edge that generated the connections<ul>
<li>By partitioning the outgoing links in this way, the graph search algorithm can be sped up by about a factor of 2</li>
</ul>
</li>
<li>Example: if the current position of the diffraction graph search is on one side of the edge, it only has to explore neighboring edges that were visible to the far side of the edge because the other edges would not be able to form valid diffraction paths</li>
</ul>
</li>
</ul>
<h2 id="5-Diffraction-Runtime"><a href="#5-Diffraction-Runtime" class="headerlink" title="5. Diffraction Runtime"></a>5. Diffraction Runtime</h2><ul>
<li><p>Problem</p>
<ul>
<li>Finding direct diffraction paths between every source and listener in the scene each time the simulation is updated</li>
</ul>
</li>
<li><p>Approach</p>
<ul>
<li>Based on the idea of intersecting rays with additional diffraction flag geometry, originally proposed for the UP diffraction method</li>
<li>In contrast to the UP approach, the paper don’t rely on stochastic ray tracing to directly calculate the diffraction path intensity<ul>
<li>Use a UP-like approach to find sequences of diffraction edges in a random ray traversal, but instead use the UTD diffraction model to analytically compute the path intensity</li>
</ul>
</li>
<li>Maintain a persistent cache of these paths over the course of the simulation to improve the temporal coherence</li>
<li>Propose a graph traversal algorithm to find high-order paths more quickly</li>
</ul>
</li>
<li><p>Advantages</p>
<ol>
<li>Like UP, its time complexity does not scale exponentially with the maximum diffraction order and it can be easily integrated into existing acoustic path tracers<ul>
<li>Enables very high-order diffraction to be calculated with good performance, even in scenes with high geometric detail</li>
</ul>
</li>
<li>Unlike UP, the degree of convergence of the results does not depend on the number of rays traced</li>
<li>Efficiently handle diffraction between multiple dynamic objects</li>
</ol>
</li>
</ul>
<h3 id="5-1-Ray-Tracing"><a href="#5-1-Ray-Tracing" class="headerlink" title="5.1. Ray Tracing"></a>5.1. Ray Tracing</h3><ul>
<li><p>Core of runtime system: Bidirectional path tracer (BDPT) with multiple importance sampling</p>
<ul>
<li>Use this to compute early reflections and to build an energy-decay histogram for the late reverb from which frequency-dependent reverberation times can be determined</li>
<li>Diffraction approach is integrated within the path tracer and is similarly bidirectional<ul>
<li>It can find diffraction paths starting from either the listener or a source</li>
<li>This bidirectionality improves the likelihood of finding paths in certain geometric configurations where either source or listener is highly occluded</li>
</ul>
</li>
</ul>
</li>
<li><p>In the path tracer, consider diffraction only for subpaths originating at a source or listener that have not yet intersected any surfaces</p>
<ul>
<li>This is consistent with our restriction to only direct diffraction</li>
<li>For these subpaths, intersect the constituent rays with both surface geometry and diffraction flag geometry to find the nearest intersection<ul>
<li>If the intersection is with a surface mesh, we reflect or transmit the ray according to the surface material and disable intersections with further diffraction flags</li>
<li>Otherwise, the ray hit a diffraction flag and remains a candidate for more diffraction events</li>
</ul>
</li>
</ul>
</li>
<li><p>Since flags can stick through geometry, we trace an additional ray from the ray vs. flag intersection point toward its projection on the edge to verify that the edge is visible</p>
<ul>
<li>If so, try to find paths to sources or listeners in the scene that are in the shadow region of the edge</li>
<li>For each of these possible paths, we evaluate whether or not diffraction over the edge is valid, given the sequence of previous edges in the subpath  </li>
<li>If a precomputed diffraction graph is available for the intersected mesh, we can also perform a deterministic graph search to find additional high-order diffraction paths (Diffraction graph traversal)<ul>
<li>Analogous to deterministic next event estimation in a path tracer</li>
<li>If the edge is not in a valid configuration to produce diffraction, the ray continues past the flag in its current direction without modification</li>
</ul>
</li>
</ul>
</li>
<li><p>After any paths have been found for the current edge, we modify the outgoing ray direction to explore the scene further</p>
<ul>
<li>One possible ray distribution is the diffraction probability density function (DAPDF) proposed for the UP diffraction model<ul>
<li>However, found in practice, a simple lambertian distribution on the opposite side of the flag empirically finds more diffraction paths and is faster to sample</li>
</ul>
</li>
<li>Once the ray is redirected, we modify the ray’s frequency-dependent energy according to the DAPDF<ul>
<li>Ensure that the outgoing ray has the correct diffracted energy for its direction, and also that further reflections of that subpath are influenced by the diffraction that occurred earlier in the path</li>
</ul>
</li>
</ul>
</li>
<li><p>Ray tracing repeats until a surface mesh is intersected or a maximum number of diffractions occur, at which point the further rays for the subpath are handled using standard BDPT</p>
</li>
</ul>
<h3 id="5-2-Path-Validation"><a href="#5-2-Path-Validation" class="headerlink" title="5.2. Path Validation"></a>5.2. Path Validation</h3><h4 id="5-2-1-Shadow-Test"><a href="#5-2-1-Shadow-Test" class="headerlink" title="5.2.1. Shadow Test"></a>5.2.1. Shadow Test</h4><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/image-20210822152155452.png" alt="image-20210822152155452.png"></p>
<ul>
<li>Each diffraction edge in a subpath must intersect the shadow region of the previous edge, if one exists<ul>
<li>The shadow region is defined as the intersection of the two half-spaces corresponding to the shadow face plane and the shadow horizon plane</li>
<li>The shadow horizon plane is defined by the diffraction edge vertices $\vec v_0$, $\vec v_1$ and the reference point $\vec p_{ref}$, which for the first edge is the source or listener position</li>
<li>For diffraction beyond 1, $\vec p_{ref}$ is the point on the previous edge that creates the largest (i.e. closest to conservative) shadow region<ul>
<li>This can be determined by clipping the previous edge’s line segment with the face planes of the current edge, so as to limit the previous edge segment to only the part in the current edge’s CSR on the non-shadowed side<ul>
<li>If the previous edge is completely outside of this region, diffraction cannot occur between the edges</li>
</ul>
</li>
</ul>
</li>
<li>The clipped endpoint that creates  the shadow region with greatest angle is chosen as $\vec p_{ref}$ and the horizon plane normal $\vec h$ is calculated as $\vec h&#x3D;(\vec v_1-\vec v_0)\times(\vec p_{ref}-\vec v_0)$</li>
<li>Use a few dot products to check if the current edge intersects the shadow region for the previous edge</li>
<li>Once a potentially valid subpath is found, we can then check for connections to sources or listeners that are in the shadow region of the last edge using a similar shadow test</li>
</ul>
</li>
</ul>
<h4 id="5-2-2-Shadow-Test-Tolerances"><a href="#5-2-2-Shadow-Test-Tolerances" class="headerlink" title="5.2.2. Shadow Test Tolerances"></a>5.2.2. Shadow Test Tolerances</h4><ul>
<li><p>Allow a tolerance of $\epsilon_h&#x3D;1.0\mathrm m$ for the horizon plane and $\epsilon_f&#x3D;0.1\mathrm m$ for the shadow face plane</p>
</li>
<li><p>The tolerance $\epsilon_f$ allows our approach to find diffraction paths between coplanar edges without numerical issues</p>
<ul>
<li>It also avoids problems where the diffraction wedge geometry doesn’t correspond exactly to the surface mesh<ul>
<li>For instance, if the averaged face normals for a proxy edge are slightly wrong, the shadow test might reject otherwise valid diffraction paths. Introducing a face plane tolerance helps to avoid these geometric issues.</li>
</ul>
</li>
</ul>
</li>
<li><p>The large horizon plane tolerance $\epsilon_h$ is to anticipate diffraction paths before they are needed and enable smooth transitions between direct and diffracted sound</p>
<ul>
<li>This is important when a source or listener moves from the region where direct sound is valid into the shadow region of an edge</li>
<li>It’s possible that a ray may not immediately hit the flag for the edge<ul>
<li>Due to the random nature of the rays</li>
<li>Resulting in a temporary gap in the audio until the diffraction path is found</li>
<li>This phenomenon is more problematic with high-order diffraction because those paths are much less likely to be explored by random ray traversal</li>
</ul>
</li>
<li>By anticipating diffraction paths that may soon become valid, those paths are more likely to be in the path cache when they are actually needed (i.e. when the direct sound becomes occluded)</li>
<li>In the case where direct sound is unoccluded, these anticipated paths are not used for auralization</li>
</ul>
</li>
</ul>
<h4 id="5-2-3-Visibility-Test"><a href="#5-2-3-Visibility-Test" class="headerlink" title="5.2.3. Visibility Test"></a>5.2.3. Visibility Test</h4><ul>
<li><p>For each source or listener in the shadow region, check to see if that source or listener can form a valid path back to the listener or source that emitted the subpath</p>
<ul>
<li><p>Compute the apex points (points where diffraction occurs) on each edge using the Newton’s method approach</p>
<ul>
<li>An important detail: clamp the points to be on the edge’s line segment<ul>
<li>This is needed for robust diffraction around curved surfaces with many small edges</li>
<li>In such cases, the apex point often is not between the edges’ endpoints, and rejecting these paths would make the diffraction significantly less robust</li>
</ul>
</li>
</ul>
</li>
<li><p>Once the apex points are determined, then trace a series of rays between the source, apex point(s), and listener to determine if the path is blocked by other geometry</p>
<ul>
<li><p>Bias each apex point a variable distance $\beta$ out from the edge along the edge normal $\vec n_e$ to prevent self-intersection of rays with neighboring faces, and also to implement a robust soft visibility test</p>
</li>
<li><p>Main idea of soft visibility test</p>
<ul>
<li>If all rays in the path are unoccluded for some $\beta \in [\beta_\min,\beta_\max]$, then that path is considered valid</li>
</ul>
<ol>
<li>Set $\beta&#x3D;\beta_\min$ and then trace rays between all of the points along the path</li>
<li>If any rays are blocked, we then geometrically increase $\beta$ by a factor of 2 and trace more rays between the new biased apex points</li>
<li>Repeat this until $\beta\geq \beta_\max$. If no $\beta$ passed the visibility test, then the diffraction path is discarded</li>
</ol>
<ul>
<li>Use $\beta_\min&#x3D;0.01\mathrm m$ and $\beta_\max&#x3D;1.0\mathrm m$</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/image-20210823195554438.png" alt="image-20210823195554438.png"></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="5-3-Diffraction-Path-Cache"><a href="#5-3-Diffraction-Path-Cache" class="headerlink" title="5.3. Diffraction Path Cache"></a>5.3. Diffraction Path Cache</h3><ul>
<li><p>Purpose: reduce unnatural variation in the sound</p>
<ul>
<li>The diffraction paths that are found on each simulation update may be different because different random rays are traced<ul>
<li>Lead to audible artifacts in real-time applications where the number of rays is small</li>
</ul>
</li>
<li>Leverage the idea of a persistent cache of paths and adapt it to diffraction</li>
</ul>
</li>
<li><p>The cache contains diffraction edge sequences from previous time steps that are known to be valid</p>
<ul>
<li>The cache entries are stored in a hash map data structure accessed by an integer key that is generated from a hash of the source index, listener index, and edge indices for a path</li>
<li>At the beginning of each time step, the cache entries are revalidated using the approach from visibility test and newly invalid entries are discarded</li>
<li>During ray tracing, as new valid paths are found, they are inserted into the cache</li>
<li>Insert a special invalid entry in the cache to indicate that that edge sequence shouldn’t be checked again this frame<ul>
<li>For explored paths that are known to be invalid</li>
</ul>
</li>
<li>Use the cache to avoid checking the same edge sequences for diffraction more than once on each frame</li>
<li>At the end of each time step, inspect the contents of the cache and pick the loudest single diffraction path for each source&#x2F;listener pair<ul>
<li>The intensity and direction for this path is then used for the final audio rendering whenever the direct sound is occluded</li>
<li>i.e. the same interpolated delay line tap is used for direct sound and diffraction to ensure smoothness</li>
</ul>
</li>
</ul>
</li>
<li><p>In scenes with many edges, the number of paths that are in the cache can increase substantially</p>
<ul>
<li>If the cache is too large, it can slow down revalidation of the cached paths on the next simulation update<ul>
<li>The cache also prioritizes the valid paths in the cache based on the intensity of the loudest frequency band</li>
</ul>
</li>
<li>Use an additional min-heap data structure to dynamically rank the paths as they are found and discard all except the top $N$<ul>
<li>$N$ is chosen to be proportional to the number of sources in the scene. e.g. $N&#x3D;20$</li>
</ul>
</li>
<li>If a new path is found and it is quieter than the Nth quietest path, we don’t add that path to the valid set (though we still mark that path as explored on this frame)</li>
<li>This effectively enforces a maximum size for the set of valid paths in the cache globally for all sources&#x2F;listeners</li>
<li>While not directly perceptually motivated, this scheme produces an approximate kind of perceptual prioritization, where if the scene is complex, the quietest paths will  be masked by the louder ones</li>
</ul>
</li>
<li><p>Main reason to restrict the rendered output to just one path is because it helps to overcome deficiencies with the UTD diffraction model</p>
<ul>
<li>UTD assumes every edge is infinitely long and that the adjacent faces are also infinite<ul>
<li>Cause UTD to produce a total diffracted sound field that is much too loud in some geometric configurations</li>
</ul>
</li>
<li>Since UTD considers each path to be over an infinite edge, the sum of diffraction contributions is not physically correct or plausible</li>
<li>By picking the single loudest (usually shortest) path, we get a diffracted sound field that is much closer to correct in these situations.</li>
</ul>
</li>
</ul>
<h3 id="5-4-Diffraction-Graph-Traversal"><a href="#5-4-Diffraction-Graph-Traversal" class="headerlink" title="5.4. Diffraction Graph Traversal"></a>5.4. Diffraction Graph Traversal</h3><ul>
<li><p>Use precomputed edge-to-edge visibility graph</p>
<ul>
<li>Improve robustness and performance</li>
<li>Increase the likelihood that we find valid high-order diffraction paths between sources and listeners that are separated by complex geometry, rather than relying on the random traversal of diffracted rays to find high-order paths</li>
<li>Apply the A* algorithm from the agent navigation field to find the shortest diffraction path through the graph starting from the intersected flag</li>
<li>While this only finds one path through the graph for each query, it tends to be a prominent path because of distance attenuation</li>
</ul>
</li>
<li><p>The graph traversal begins whenever a diffraction flag with the correct orientation is intersected by a BDPT subpath</p>
<ul>
<li>Do a separate traversal for each source or listener in the scene that was not able to form a valid diffraction path directly over the edge, i.e. we only perform the graph traversal when a lower-order path was not found using visibility test</li>
<li>At this point, we transform $\vec p_{ref}$ and the goal source or listener into the mesh’s local space so that the search can operate locally to avoid transforming vertices and normals</li>
<li>The search starts at the graph node corresponding to the intersected flag</li>
<li>From there, we investigate only the neighboring nodes that are on the opposite side of the flag from $\vec p_{ref}$</li>
</ul>
</li>
<li><p>For each neighbor, compute the estimated distance from the neighboring edge to the goal source or listener  </p>
<ul>
<li>This is the A* heuristic that is used to rank potential paths through the graph</li>
<li>The choice of heuristic influences which paths are prioritized<ul>
<li>Using the point on the neighboring edge that is closest to the line between the source and listener</li>
</ul>
</li>
</ul>
</li>
<li><p>Once the distance from the closest point on the neighbor to the goal is determined, it is added to the shortest distance through the graph from the starting edge to the current edge to yield the total estimated distance for the neighbor</p>
<ul>
<li>Discard any neighbors that are in the A* closed set and which have distance estimates greater than best path through the graph to that node, if the neighbor was previously visited</li>
<li>If a neighbor is not in the A* open set or has a distance estimate lower than the best so far, we then check the edge further to see if it is in a proper geometric configuration for diffraction according to shadow test</li>
</ul>
</li>
<li><p>Once all neighbors are either discarded or put into the A* heap, we check the top of the heap (i.e. the node with smallest distance estimate) to see if a valid diffraction path is formed from the starting node to the top node</p>
<ul>
<li>First check to make sure the goal point is inside the shadow region of the final edge</li>
<li>If this succeeds, the edge sequence for the shortest path is reconstructed and transformed into world space for the final path validation and visibility testing<ul>
<li>If the path is valid, that path is inserted into the cache and the graph search terminates</li>
<li>Otherwise, the neighbors of that node are investigated recursively</li>
</ul>
</li>
</ul>
</li>
<li><p>This process repeats until a path is found or until a maximum number of nodes has been visited</p>
<ul>
<li>If no valid path through the graph exists, which is sometimes the case with diffraction through complex environments, A* degenerates to Djikstra’s algorithm and explores the entire graph</li>
<li>By limiting the number of nodes that are visited, we can avoid spending a lot of time searching the extraneous parts of the graph when no path actually exists<ul>
<li>Suggest using a limit that is 2 - 3 times larger than the maximum diffraction order</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="6-Implementation"><a href="#6-Implementation" class="headerlink" title="6. Implementation"></a>6. Implementation</h2><h2 id="7-Result"><a href="#7-Result" class="headerlink" title="7. Result"></a>7. Result</h2><h3 id="7-1-Scene"><a href="#7-1-Scene" class="headerlink" title="7.1. Scene"></a>7.1. Scene</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/image-20210823215019823.png" alt="image-20210823215019823.png"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/image-20210823215048917.png" alt="image-20210823215048917.png"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/image-20210823215109007.png" alt="image-20210823215109007.png"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/image-20210823215127730.png" alt="image-20210823215127730.png"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/image-20210823215200235.png" alt="image-20210823215200235.png"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/image-20210823215222200.png" alt="image-20210823215222200.png"></p>
<h3 id="7-2-Preprocessing"><a href="#7-2-Preprocessing" class="headerlink" title="7.2. Preprocessing"></a>7.2. Preprocessing</h3><ul>
<li>The time taken by each section of our preprocessing pipeline</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/image-20210823215334108.png" alt="image-20210823215334108.png"></p>
<h3 id="7-3-Runtime"><a href="#7-3-Runtime" class="headerlink" title="7.3. Runtime"></a>7.3. Runtime</h3><ul>
<li>The runtime performance with respect to the maximum diffraction order</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/image-20210823215444442.png" alt="image-20210823215444442.png"></p>
<ul>
<li><p>The runtime performance with respect to the varying diffraction flag lengths  </p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/image-20210823221752265.png" alt="image-20210823221752265.png"></p>
</li>
<li><p>The number of paths found for varying diffraction flag lengths</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/image-20210823221837251.png" alt="image-20210823221837251.png"></p>
</li>
</ul>
<h3 id="7-4-Validation"><a href="#7-4-Validation" class="headerlink" title="7.4. Validation"></a>7.4. Validation</h3><ul>
<li>Paper’s approach vs. Offline FDTD simulation</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/image-20210823215852679.png" alt="image-20210823215852679.png"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/image-20210824085947838.png" alt="image-20210824085947838.png"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/image-20210824090007192.png" alt="image-20210824090007192.png"></p>
<ul>
<li>Two different configuration of paper’s approach vs. Offline FDTD simulation</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/image-20210823220010421.png" alt="image-20210823220010421.png"></p>
<h2 id="8-Conclusions"><a href="#8-Conclusions" class="headerlink" title="8. Conclusions"></a>8. Conclusions</h2><ul>
<li><p>Presented a complete approach for simulating approximate acoustic diffraction for real-time AR and VR applications </p>
<ul>
<li>Uses a novel mesh preprocessing pipeline to identify a reduced set of diffraction edges as well as construct diffraction flag geometry and edge visibility graphs</li>
<li>Traces rays against the diffraction flags to probabilistically explore possible diffraction paths, then computes the path intensities using the UTD diffraction model</li>
<li>Utilize a precomputed edge visibility graph and the A* algorithm to greatly speed up the exploration of high-order diffraction paths</li>
</ul>
</li>
<li><p>This diffraction technique is between 2.7 and 586 times faster than the previous state of the art in real-time high order diffraction, depending on the<br>scene, and is able to scale efficiently and robustly to large scenes with high geometric detail</p>
</li>
<li><p>Evaluated its objective accuracy by comparing to an offline FDTD wave simulation</p>
</li>
<li><p>Limitations</p>
<ol>
<li>Only consider direct diffraction between a source or listener, i.e. diffraction paths consisting of only edge diffraction with no reflections<ul>
<li>In theory, combinations of reflection and diffraction are compatible with paper’s approach, but would require changes to how paths are stored and accessed in the diffraction path cache, and also changes to how the path intensity is evaluated</li>
<li>The generation of unique cache identifiers for diffuse reflections may prove more difficult than for diffraction edges or specular reflections</li>
</ul>
</li>
<li>Has all of the limitations of UTD such as inaccuracy with small edges<ul>
<li>Other more-accurate diffraction models like BTM could be used in place of UTD to ameliorate some of these issues</li>
</ul>
</li>
<li>The diffraction graph traversal algorithm only finds a single path per edge, though this nevertheless produces plausible results</li>
<li>Since the graphs for each mesh in the scene are disjoint, the graph search can only find diffraction paths around individual objects</li>
</ol>
</li>
<li><p>Future work</p>
<ul>
<li>Apply this diffraction method to mobile class devices where compute is extremely limited</li>
<li>Explore possibilities for leveraging more precomputation to further reduce the runtime overhead of diffraction</li>
</ul>
</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://chaphlagical.github.io">Chaf Chen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://chaphlagical.github.io/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/">https://chaphlagical.github.io/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Siggraph-2021/">Siggraph 2021</a><a class="post-meta__tags" href="/tags/Sound-Rendering/">Sound Rendering</a></div><div class="post_share"><div class="social-share" data-image="/img/logo.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/08/25/paper_reading/Neural_Light_Transport_for_Relighting_and_View_Synthesis/" title="Neural Light Transport for Relighting and View Synthesis"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Neural Light Transport for Relighting and View Synthesis</div></div></a></div><div class="next-post pull-right"><a href="/2021/08/18/paper_reading/Temporally_Adaptive_Shading_Reuse_for_Real_time_Rendering_and_Virtual_Reality/" title="Temporally Adaptive Shading Reuse for Real-time Rendering and Virtual Reality"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/18/paper_reading/Temporally_Adaptive_Shading_Reuse_for_Real_time_Rendering_and_Virtual_Reality/image-20210817150114663.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Temporally Adaptive Shading Reuse for Real-time Rendering and Virtual Reality</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2021/08/29/paper_reading/Low_Cost_SPAD_Sensing_for_Non_Line_Of_Sight_Tracking_Material/" title="Low-Cost SPAD Sensing for Non-Line-Of-Sight Tracking, Material Classification and Depth Imaging"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-29</div><div class="title">Low-Cost SPAD Sensing for Non-Line-Of-Sight Tracking, Material Classification and Depth Imaging</div></div></a></div><div><a href="/2021/08/25/paper_reading/Neural_Light_Transport_for_Relighting_and_View_Synthesis/" title="Neural Light Transport for Relighting and View Synthesis"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-25</div><div class="title">Neural Light Transport for Relighting and View Synthesis</div></div></a></div><div><a href="/2021/08/15/paper_reading/GPU_Accelerated_Path_Tracing_of_Massive_Scenes/" title="GPU Accelerated Path Tracing of Massive Scenes"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-15</div><div class="title">GPU Accelerated Path Tracing of Massive Scenes</div></div></a></div><div><a href="/2021/08/27/paper_reading/Real_Time_Global_Illumination_Decomposition_of_Videos/" title="Real-Time Global Illumination Decomposition of Videos"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-27</div><div class="title">Real-Time Global Illumination Decomposition of Videos</div></div></a></div><div><a href="/2021/08/18/paper_reading/Temporally_Adaptive_Shading_Reuse_for_Real_time_Rendering_and_Virtual_Reality/" title="Temporally Adaptive Shading Reuse for Real-time Rendering and Virtual Reality"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/18/paper_reading/Temporally_Adaptive_Shading_Reuse_for_Real_time_Rendering_and_Virtual_Reality/image-20210817150114663.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-18</div><div class="title">Temporally Adaptive Shading Reuse for Real-time Rendering and Virtual Reality</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/logo.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Chaf Chen</div><div class="author-info__description">USTC CG Student</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">33</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Chaphlagical"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Chaphlagical" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:mail@ustc.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Looking for a Ph.D position!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Introduction"><span class="toc-number">2.</span> <span class="toc-text">1. Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Background"><span class="toc-number">3.</span> <span class="toc-text">2. Background</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Sound-Propagation"><span class="toc-number">3.1.</span> <span class="toc-text">2.1. Sound Propagation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Diffraction-for-Geometric-Acoustic"><span class="toc-number">3.2.</span> <span class="toc-text">2.2. Diffraction for Geometric Acoustic</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Mesh-Simplification-for-Acoustics"><span class="toc-number">3.3.</span> <span class="toc-text">2.3. Mesh Simplification for Acoustics</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Overview"><span class="toc-number">4.</span> <span class="toc-text">3. Overview</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Diffraction-Mesh-Preprocessing"><span class="toc-number">5.</span> <span class="toc-text">4. Diffraction Mesh Preprocessing</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Mesh-Simplification"><span class="toc-number">5.1.</span> <span class="toc-text">4.1. Mesh Simplification</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Diffraction-Edge-Extraction"><span class="toc-number">5.2.</span> <span class="toc-text">4.2. Diffraction Edge Extraction</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-1-Initial-Edge-Selection"><span class="toc-number">5.2.1.</span> <span class="toc-text">4.2.1. Initial Edge Selection</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-2-Curved-Boundary-Splitting"><span class="toc-number">5.2.2.</span> <span class="toc-text">4.2.2. Curved Boundary Splitting</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-3-Silhouette-Edges"><span class="toc-number">5.2.3.</span> <span class="toc-text">4.2.3. Silhouette Edges</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-Diffraction-Geometry-Construction"><span class="toc-number">5.2.4.</span> <span class="toc-text">4.3. Diffraction Geometry Construction</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-Diffraction-Graph"><span class="toc-number">5.3.</span> <span class="toc-text">4.4. Diffraction Graph</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Diffraction-Runtime"><span class="toc-number">6.</span> <span class="toc-text">5. Diffraction Runtime</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-Ray-Tracing"><span class="toc-number">6.1.</span> <span class="toc-text">5.1. Ray Tracing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-Path-Validation"><span class="toc-number">6.2.</span> <span class="toc-text">5.2. Path Validation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-1-Shadow-Test"><span class="toc-number">6.2.1.</span> <span class="toc-text">5.2.1. Shadow Test</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-2-Shadow-Test-Tolerances"><span class="toc-number">6.2.2.</span> <span class="toc-text">5.2.2. Shadow Test Tolerances</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-3-Visibility-Test"><span class="toc-number">6.2.3.</span> <span class="toc-text">5.2.3. Visibility Test</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-Diffraction-Path-Cache"><span class="toc-number">6.3.</span> <span class="toc-text">5.3. Diffraction Path Cache</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-Diffraction-Graph-Traversal"><span class="toc-number">6.4.</span> <span class="toc-text">5.4. Diffraction Graph Traversal</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Implementation"><span class="toc-number">7.</span> <span class="toc-text">6. Implementation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Result"><span class="toc-number">8.</span> <span class="toc-text">7. Result</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-Scene"><span class="toc-number">8.1.</span> <span class="toc-text">7.1. Scene</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-Preprocessing"><span class="toc-number">8.2.</span> <span class="toc-text">7.2. Preprocessing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-Runtime"><span class="toc-number">8.3.</span> <span class="toc-text">7.3. Runtime</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-4-Validation"><span class="toc-number">8.4.</span> <span class="toc-text">7.4. Validation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-Conclusions"><span class="toc-number">9.</span> <span class="toc-text">8. Conclusions</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/" title="Adaptive Incident Radiance Field Sampling and Reconstruction Using Deep Reinforcement Learning">Adaptive Incident Radiance Field Sampling and Reconstruction Using Deep Reinforcement Learning</a><time datetime="2023-01-25T00:04:00.000Z" title="Created 2023-01-25 00:04:00">2023-01-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/01/06/paper_reading/Temporal_Coherence-Based_Distributed_Ray_Tracing_of_Massive_Scenes/" title="Temporal Coherence-based Distributed Ray Tracing of Massive Scenes">Temporal Coherence-based Distributed Ray Tracing of Massive Scenes</a><time datetime="2023-01-06T22:13:11.000Z" title="Created 2023-01-06 22:13:11">2023-01-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/29/gaming/spiderman_miles/" title="Marvel's Spider-Man Miles Morales"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2022/12/29/gaming/spiderman_miles/image-20221229175057590.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Marvel's Spider-Man Miles Morales"/></a><div class="content"><a class="title" href="/2022/12/29/gaming/spiderman_miles/" title="Marvel's Spider-Man Miles Morales">Marvel's Spider-Man Miles Morales</a><time datetime="2022-12-29T21:13:11.000Z" title="Created 2022-12-29 21:13:11">2022-12-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/12/17/paper_reading/Vectorization_for_Fast_Analytic_and_Differentiable_Visibility/" title="Vectorization for Fast, Analytic, and Differentiable Visibility">Vectorization for Fast, Analytic, and Differentiable Visibility</a><time datetime="2022-12-17T21:13:11.000Z" title="Created 2022-12-17 21:13:11">2022-12-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/12/14/note/edge_sampling/" title="Physics Based Differentiable Rendering: Edge Sampling">Physics Based Differentiable Rendering: Edge Sampling</a><time datetime="2022-12-14T00:00:00.000Z" title="Created 2022-12-14 00:00:00">2022-12-14</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Chaf Chen</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script><script>function loadDisqus () {
  var disqus_config = function () {
    this.page.url = 'https://chaphlagical.github.io/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/'
    this.page.identifier = '/2021/08/23/paper_reading/Fast_Diffraction_Pathfinding_for_Dynamic_Sound_Propagation/'
    this.page.title = 'Fast Diffraction Pathfinding for Dynamic Sound Propagation'
  };

  window.disqusReset = () => {
    DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  if (window.DISQUS) disqusReset()
  else {
    (function() { 
      var d = document, s = d.createElement('script');
      s.src = 'https://chaphlagical-github-io.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  }

  document.getElementById('darkmode').addEventListener('click', () => {
    setTimeout(() => window.disqusReset(), 200)
  })
}

if ('Disqus' === 'Disqus' || !false) {
  if (false) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
  else loadDisqus()
} else {
  function loadOtherComment () {
    loadDisqus()
  }
}
</script></div></div></body></html>