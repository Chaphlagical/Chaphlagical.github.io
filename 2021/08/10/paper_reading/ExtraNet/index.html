<!DOCTYPE html><html lang="[&quot;en&quot;,&quot;zh-CN&quot;,&quot;default&quot;]" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>ExtraNet: Real-time Extrapolated Rendering for Low-latency Temporal Supersampling | Chaf's Blog</title><meta name="author" content="Chaf Chen"><meta name="copyright" content="Chaf Chen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="Siggraph Asia 2021 paper reading">
<meta property="og:type" content="article">
<meta property="og:title" content="ExtraNet: Real-time Extrapolated Rendering for Low-latency Temporal Supersampling">
<meta property="og:url" content="https://chaphlagical.github.io/2021/08/10/paper_reading/ExtraNet/index.html">
<meta property="og:site_name" content="Chaf&#39;s Blog">
<meta property="og:description" content="Siggraph Asia 2021 paper reading">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://chaphlagical.github.io/img/logo.jpg">
<meta property="article:published_time" content="2021-08-10T21:13:11.000Z">
<meta property="article:modified_time" content="2023-03-02T12:40:59.566Z">
<meta property="article:author" content="Chaf Chen">
<meta property="article:tag" content="Rendering">
<meta property="article:tag" content="Neural Network">
<meta property="article:tag" content="Siggraph Asia 2021">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chaphlagical.github.io/img/logo.jpg"><link rel="shortcut icon" href="/img/logo.jpg"><link rel="canonical" href="https://chaphlagical.github.io/2021/08/10/paper_reading/ExtraNet/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"Traditional Chinese Activated Manually","cht_to_chs":"Simplified Chinese Activated Manually","day_to_night":"Dark Mode Activated Manually","night_to_day":"Light Mode Activated Manually","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'ExtraNet: Real-time Extrapolated Rendering for Low-latency Temporal Supersampling',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-03-02 12:40:59'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><script>const preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> { preloader.endLoading() })

if (false) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/logo.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">16</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">3</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="Chaf's Blog"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/logo.jpg"/><span class="site-name">Chaf's Blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">ExtraNet: Real-time Extrapolated Rendering for Low-latency Temporal Supersampling</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2021-08-10T21:13:11.000Z" title="Created 2021-08-10 21:13:11">2021-08-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-03-02T12:40:59.566Z" title="Updated 2023-03-02 12:40:59">2023-03-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper-Reading/">Paper Reading</a></span></div><div class="meta-secondline"></div></div></div><article class="post-content" id="article-container"><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><strong>Real-time Rendering</strong></p>
<ul>
<li>Frame rate</li>
<li>Latency</li>
</ul>
<p><strong>Spatial Super Sampling</strong>: DLSS</p>
<ul>
<li>Decreasing the rendering time of each frame by rendering at a lower resolution</li>
</ul>
<p><strong>Temporal Super Sampling</strong></p>
<ul>
<li>Producing more frames on the fly</li>
<li>Problems:<ul>
<li>It‚Äôs own computational cost</li>
<li>The latency introduced by interpolating frames from the future</li>
</ul>
</li>
</ul>
<p><strong>ExtraNet</strong></p>
<ul>
<li><p>An efficient neural network that predicts accurate shading results on an extrapolated frame, to minimize both the performance overhead and the latency</p>
</li>
<li><p>Rendered auxiliary geometry buffers of the extrapolated frame &amp; Temporally reliable motion vectors</p>
</li>
<li><p>Train to perform two tasks:</p>
<ol>
<li>Irradiance in-painting for regions that cannot find historical correspondences</li>
<li>Accurate ghosting-free shading prediction for regions where temporal information is available</li>
</ol>
</li>
<li><p>A robust hole-marking strategy to automate the classification of these tasks  </p>
</li>
<li><p>The data generation from a series of high-quality production-ready scenes</p>
</li>
</ul>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p><strong>Some techniques to increase the rendering performance</strong></p>
<p>Leveraging the temporal coherency between frames: Reduces the sampling rate at each individual frame, and rely on temporal reprojections to combine samples taken over multiple frames to reconstruct a high quality image (Still performing spatial upsampling &#x2F; reconstruction, with a virtually higher sampling rate acquired temporally to speed up the generation of the current frame of interest)</p>
<ul>
<li>TAA (Temporal Anti-Aliasing)</li>
<li>TAAU (Temporal Anti-Aliasing Upsample)</li>
<li>DLSS (Deep Learning Super Sampling)</li>
<li>Ray Tracing Denoising</li>
</ul>
<p><strong>Limitation of current work</strong></p>
<ul>
<li>Frame Interpolation: only when the next frame has been fully rendered, their algorithms can start to work to interpolate one or more frames in between</li>
<li>Performance: generating a new frame is already more expensive than a full rendering of it  </li>
<li>Situation: more information can be used in rendered scene, like G-buffers</li>
</ul>
<p><strong>Contribution</strong></p>
<ul>
<li>A new temporally interleaved real-time rendering and extrapolation architecture that can nearly double the frame rate without introducing additional latency.  </li>
<li>A novel neural network architecture that leverages G-buffers and temporal motion (computed both from consecutive frames and temporally reliable motion vectors) to perform the extrapolation as a shading prediction task.</li>
<li>A lightweight design that incurs low performance overhead (around 8 milliseconds per frame at 720P) but results in high quality and smoothly extrapolated frames.</li>
</ul>
<h2 id="2-Related-work-and-background"><a href="#2-Related-work-and-background" class="headerlink" title="2. Related work and background"></a>2. Related work and background</h2><h3 id="2-1-Temporal-Reconstruction"><a href="#2-1-Temporal-Reconstruction" class="headerlink" title="2.1. Temporal Reconstruction"></a>2.1. Temporal Reconstruction</h3><ul>
<li>Rely on temporal motion vectors</li>
<li>Temporal reconstruction methods reduce the samples taken at each individual frame, and reproject samples over the course of multiple frames using backward motion vectors to reconstruct a high quality image</li>
<li>In their paper<ul>
<li>Leverage temporal motion vectors to reproject shading of the previous rendered frames to the extrapolated frames</li>
<li>But they do not have any samples in the extrapolated frames, unlike temporal reconstruction</li>
</ul>
</li>
</ul>
<p><strong>Motion Vectors</strong></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/10/paper_reading/ExtraNet/image-20210810160748362.png" alt="image-20210810160748362.png"><br>$$<br>\pmb p_0^{t}&#x3D;\pmb P^t\pmb M^t\pmb V^t \pmb s_0<br>$$</p>
<p>$$<br>\pmb p_0^{t-1}&#x3D;\pmb P^{t-1}\pmb M^{t-1}\pmb V^{t-1}\pmb s_0<br>$$</p>
<p>Store screen space motion vectors &#x3D;&gt; Velocity Buffer</p>
<p>vertex shader:</p>
<figure class="highlight glsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">uniform</span> <span class="type">mat4</span> uModelViewProjectionMat;</span><br><span class="line"><span class="keyword">uniform</span> <span class="type">mat4</span> uPrevModelViewProjectionMat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">smooth</span> <span class="keyword">out</span> <span class="type">vec4</span> vPosition;</span><br><span class="line"><span class="keyword">smooth</span> <span class="keyword">out</span> <span class="type">vec4</span> vPrevPosition;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> main(<span class="type">void</span>) &#123;</span><br><span class="line">	vPosition = uModelViewProjectionMat * <span class="built_in">gl_Vertex</span>;</span><br><span class="line">	vPrevPosition = uPrevModelViewProjectionMat * <span class="built_in">gl_Vertex</span>;</span><br><span class="line"></span><br><span class="line">	<span class="built_in">gl_Position</span> = vPosition;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>fragment shader:</p>
<figure class="highlight glsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">smooth</span> <span class="keyword">in</span> <span class="type">vec4</span> vPosition;</span><br><span class="line"><span class="keyword">smooth</span> <span class="keyword">in</span> <span class="type">vec4</span> vPrevPosition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">out</span> <span class="type">vec2</span> oVelocity;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> main(<span class="type">void</span>) &#123;</span><br><span class="line">	<span class="type">vec2</span> a = (vPosition.xy / vPosition.w) * <span class="number">0.5</span> + <span class="number">0.5</span>;</span><br><span class="line">	<span class="type">vec2</span> b = (vPrevPosition.xy / vPrevPosition.w) * <span class="number">0.5</span> + <span class="number">0.5</span>;</span><br><span class="line">	oVelocity = a - b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-2-Texture-In-painting"><a href="#2-2-Texture-In-painting" class="headerlink" title="2.2. Texture In-painting"></a>2.2. Texture In-painting</h3><ul>
<li><p>Using backward motion vectors to reproject shading from previous frame is that disoccluded regions will not contain any valid information in the previous frame, resulting in ‚Äúholes‚Äù in the reprojected image</p>
</li>
<li><p>Some work:</p>
<ul>
<li>Convolution</li>
<li>Deep learning</li>
<li>Resort to a two-stage prediction strategy to separately predict missing textures in a step-by-step manner</li>
</ul>
</li>
<li><p>Limitation</p>
<ul>
<li>Not designed specifically for real-time rendering  </li>
<li>Perform the in-painting task completely on single images without temporal information and G-buffers  </li>
<li>The running time of some methods are in the order of seconds, which are not suitable in our real-time extrapolation task</li>
</ul>
</li>
</ul>
<h3 id="2-3-Image-Warping"><a href="#2-3-Image-Warping" class="headerlink" title="2.3. Image Warping"></a>2.3. Image Warping</h3><ul>
<li>Realized by forward scattering or backward gathering, according to their data access patterns  </li>
<li>Compared with forward mapping, backward mapping is more appealing in real-time rendering due to its high efficiency  </li>
<li>Although visually plausible contents can be recovered at hole regions after image warping, shadow and highlight movements are almost untouched in these traditional methods, especially those used in the context of frame extrapolation</li>
</ul>
<h3 id="2-4-Video-and-Rendering-Interpolation"><a href="#2-4-Video-and-Rendering-Interpolation" class="headerlink" title="2.4. Video and Rendering Interpolation"></a>2.4. Video and Rendering Interpolation</h3><ul>
<li>Interpolation based frame rate upsampling is widely used for temporally upsampling videos, using optical flow of the last couple of frames</li>
<li>Optical flow is an image based algorithm that often produces inaccurate motion, which will lead to artifacts in the interpolation results  </li>
<li>Uses accurate motion vectors available from the rendering engine, usually rely on image warping techniques to reuse shading information across frames  </li>
<li>Deep learning based video interpolation methods are still time-consuming, thus are not suitable for real-time rendering</li>
</ul>
<h2 id="3-Motivation-and-Challenge"><a href="#3-Motivation-and-Challenge" class="headerlink" title="3. Motivation and Challenge"></a>3. Motivation and Challenge</h2><h3 id="3-1-Latency"><a href="#3-1-Latency" class="headerlink" title="3.1. Latency"></a>3.1. Latency</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/10/paper_reading/ExtraNet/image-20210810165027122.png" alt="image-20210810165027122.png"></p>
<ul>
<li>Interpolation &#x3D;&gt; Produce significant lantency</li>
<li>Extrapolation &#x3D;&gt; doesn‚Äôt introduce any additional latency</li>
</ul>
<h3 id="3-2-Challenge"><a href="#3-2-Challenge" class="headerlink" title="3.2. Challenge"></a>3.2. Challenge</h3><p><strong>Disocclusion</strong></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/10/paper_reading/ExtraNet/image-20210810170445162.png" alt="image-20210810170445162"></p>
<ul>
<li>Backward motion vectors may not always exist  <ul>
<li>Traditional motion vectors propose to use the same pixel values from previous frames, resulting in ghosting artifacts in the disoccluded regions  </li>
<li>Occlusion motion vectors alleviates this problem by looking for a nearby similar region in the current frame, then look for the corresponding region from previous frames</li>
</ul>
</li>
<li>Leverage an in-painting network to reshade the occluded regions  using occlusion motion vector</li>
</ul>
<p><strong>Dynamic changes in shading</strong></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/10/paper_reading/ExtraNet/image-20210810171446843.png" alt="image-20210810171446843.png"></p>
<ul>
<li>Dynamic changes in shading over frames, such as moving shadows, parallax motion from reflections and the colors on rotating objects, may change drastically and are thus unknown in the current frame </li>
<li>Always keep the most recently rendered 3 frames for the network to learn to extrapolate the changes</li>
</ul>
<p><strong>Other challenges</strong></p>
<ul>
<li>Extremely low tolerance towards errors and artifacts</li>
<li>The inference of the neural network must be (ideally much) faster than the actual rendering process</li>
</ul>
<h2 id="4-ExtraNet-for-frame-extrapolation"><a href="#4-ExtraNet-for-frame-extrapolation" class="headerlink" title="4. ExtraNet for frame extrapolation"></a>4. ExtraNet for frame extrapolation</h2><p>Design and train a deep neural network, namely ExtraNet, to predict a new frame from some historical frames and the G-buffers of the extrapolated frame.</p>
<ol>
<li>Considering that disoccluded regions of the extrapolated frame lack shading information but reliable and cheap G-buffers are available, we construct  an In-painting Network to reshade the disoccluded regions with the help of occlusion motion vectors and those G-buffers</li>
<li>In contrast to the general image in-painting task, regions that are not disoccluded may also contain invalid pixels attributing to dynamic shading changes in the scene. To tackle this issue, we design a History Encoder, an additional convolutional neural network, to extract necessary information from historical frames</li>
<li>To meet the requirement of high inference speed which is critical to real-time rendering, adopting lightweight gated convolutions in the In-painting Network.</li>
</ol>
<h3 id="4-1-Problem-Formulation"><a href="#4-1-Problem-Formulation" class="headerlink" title="4.1. Problem Formulation"></a>4.1. Problem Formulation</h3><ul>
<li>Frame $i$: Denoting the current frame rendered by the graphics engine</li>
<li>Frame $i+0.5$: Next frame the network will predict</li>
<li>Two historical frames including frame $i-1$ and $i-2$ and the corresponding G-buffers</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/10/paper_reading/ExtraNet/image-20210810200628067.png" alt="image-20210810200628067"></p>
<p>Pipeline:</p>
<ol>
<li>Demodulation: Dividing by the albedo to acquire texture-free irradiance  </li>
<li>Warp the demodulated frame using both traditional backward motion vectors and occlusion motion vectors</li>
<li>The warping operation probably incurs holes in the frame. Using In-painting network to fill the frame</li>
<li>Modulation: multiplying by the albedo to re-acquire textured shading  </li>
<li>Apply regular post-processing(tone mapping ,TAA‚Ä¶)</li>
</ol>
<h3 id="4-2-Motion-Vectors-and-Image-Warping"><a href="#4-2-Motion-Vectors-and-Image-Warping" class="headerlink" title="4.2. Motion Vectors and Image Warping"></a>4.2. Motion Vectors and Image Warping</h3><p>Instead of computing a zero motion vector in disoccluded regions as the traditional backward motion vector does, occlusion motion vector computes the motion vector in disoccluded regions as the motion vector of the foreground in the previous frame</p>
<h3 id="4-3-Network-Architecture"><a href="#4-3-Network-Architecture" class="headerlink" title="4.3. Network Architecture"></a>4.3. Network Architecture</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/10/paper_reading/ExtraNet/image-20210810202820213.png" alt="image-20210810202820213"></p>
<ul>
<li><p>Similar structure with U-NET</p>
</li>
<li><p>Adopt gated convolutions<br>$$<br>\pmb M&#x3D;\mathrm{Conv}(\pmb W_m, \pmb X)<br>$$</p>
<p>$$<br>\pmb F&#x3D;\mathrm{Conv}(\pmb W_f,\pmb X)<br>$$</p>
<p>$$<br>\pmb O&#x3D;\sigma(\pmb M)\odot \pmb F<br>$$</p>
<ul>
<li>$\pmb X$: Input feature map</li>
<li>$\pmb W_m$ and $\pmb W_f$: Two trainable filters</li>
<li>$\odot$ : Element-wise multiplication  </li>
<li>$\sigma(\cdot)$: Sigmoid activation</li>
</ul>
</li>
<li><p>Gated convolutions increase the inference time</p>
<ul>
<li>Resort to a light-weight variant of gated convolution by making $\pmb M$ a single-channel mask</li>
<li>Not use any gated convolution in the upsampling stage<ul>
<li>Assume all holes have already been filled in the downsampling stage</li>
</ul>
</li>
</ul>
</li>
<li><p>Use a residual learning strategy in our pipeline by adding the predicted image of our network with the input image warped by traditional backward motion vectors</p>
<ul>
<li>This stabilizes the training process</li>
</ul>
</li>
</ul>
<h3 id="4-4-History-Encoder"><a href="#4-4-History-Encoder" class="headerlink" title="4.4. History Encoder"></a>4.4. History Encoder</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/10/paper_reading/ExtraNet/image-20210810204738275.png" alt="image-20210810204738275.png"></p>
<ul>
<li><p>Input: Warped images of the current frame $i$ and its two past frames including frame $i-1$ and frame $i-2$</p>
<ul>
<li>Warp frames $i-1$ and $i-2$: accumulate the motion vectors</li>
<li>Invalid pixels should be marked for these frames</li>
</ul>
</li>
<li><p>Structure: Nine $3\times 3$ convolution layers</p>
<ul>
<li>Down sample the input</li>
<li>Shared by different historical frames</li>
</ul>
</li>
<li><p>Output: Implicitly encode the high-level representations of shadows and shadings in previous frames, are concatenated together and fed into our In-painting Network  </p>
</li>
<li><p>Optical flow can be explicitly predicted from several historical frames</p>
<ul>
<li>But correct forward optical flow is difficult to acquire in the frame extrapolation pipeline, since only historical frames are available  </li>
<li>Use backward optical flow as an approximation  </li>
<li>Not using it</li>
</ul>
</li>
</ul>
<h3 id="4-5-Loss-Function"><a href="#4-5-Loss-Function" class="headerlink" title="4.5. Loss Function"></a>4.5. Loss Function</h3><p><strong>Penalizes pixel-wise error between the predicted frame $\pmb P$ and ground-truth frame $\pmb T$</strong></p>
<p>$$<br>\mathcal L_{l_1}&#x3D;\frac{1}{n}\sum_i |\pmb P_i - \pmb T_i|<br>$$</p>
<p><strong>hole-augmented  loss: penalize more in the hole regions marked beforehand</strong></p>
<p>$$<br>\mathcal L_{\mathrm {hole}}&#x3D;\frac{1}{n}\sum_i |\pmb P_i-\pmb T_i|\odot(1-\pmb m)<br>$$</p>
<ul>
<li>$\pmb m$ is the binary mask fed into network</li>
</ul>
<p><strong>The shading-augmented loss: focuses on handling potential shading changes in the predicted frames</strong>  </p>
<p>$$<br>\mathcal L_{\mathrm{shade}}&#x3D;\frac{1}{k}\sum_{i\in \Phi_{\mathrm{top}-k}}|\pmb P_i-\pmb T_i|<br>$$</p>
<ul>
<li>Shading changes: stem from moving shadows due to dynamic lights and specular reflections</li>
<li>Considering that shadows and reflections tend to generate large pixel variation among neighboring frames, we select $ùëò$ pixels with top $ùëò$ largest errors from the predicted frame and mark these pixels as potential shading change regions  </li>
<li>$\Phi_{\mathrm{top}-k}$ includes the indices of $k$ pixels with top $k$ largest errors<ul>
<li>Currently, $k$ set to 10% of total pixels number</li>
</ul>
</li>
</ul>
<p><strong>Final loss function</strong><br>$$<br>\mathcal L&#x3D;\mathcal L_{l_1}+\lambda_{\mathrm{hole}}\mathcal L_{\mathrm{hole}}+\lambda_{\mathrm{shade}}\mathcal L_{\mathrm{shade}}<br>$$</p>
<ul>
<li>$\lambda_{\mathrm{hole}}$ and $\lambda_{\mathrm{shade}}$are weights to balance the influence of the losses, here set to 1</li>
</ul>
<h3 id="4-6-Training-detail"><a href="#4-6-Training-detail" class="headerlink" title="4.6. Training detail"></a>4.6. Training detail</h3><ul>
<li>PyTorch</li>
<li>Mini-batch SGD and Adam optimizer</li>
<li>mini-batch size as 8, $\beta_1 &#x3D; 0.9$ and $\beta_2&#x3D;0.999$ in Adam optimizer</li>
<li>default initialization</li>
<li>applied HDR image: logarithm transformation $y&#x3D;\log(1+x)$ before feeding images into network</li>
</ul>
<h2 id="5-Dataset"><a href="#5-Dataset" class="headerlink" title="5. Dataset"></a>5. Dataset</h2><h3 id="5-1-Scenes-and-Buffers"><a href="#5-1-Scenes-and-Buffers" class="headerlink" title="5.1. Scenes and Buffers"></a>5.1. Scenes and Buffers</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/10/paper_reading/ExtraNet/image-20210810220452315.png" alt="image-20210810220452315.png"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/10/paper_reading/ExtraNet/image-20210810220314215.png" alt="image-20210810220314215.png"></p>
<p>Each dumped frame comprises 10 buffers which can be divided into three categories:  </p>
<ol>
<li>Actual shading frame before tone mapping (PreTonemapHDRColor) and albedo (Base color) for demodulation  </li>
<li>G-buffers used in our network, including scene depth ($ùëë$, 1channel), world normal ($\pmb n_ùë§$, 3 channels), roughness (1 channel), and metallic (1 channel)  </li>
<li>Other auxiliary buffers which are necessary for marking holes (invalid pixels) in the warped frame. They are motion vector, world position ($\pmb p_ùë§$), NoV ($\pmb n_ùë§\cdot \pmb v$, the dot product of world normal $\pmb n_ùë§$ and view vector $\pmb v$), and customized stencil ($ùë†$)</li>
</ol>
<h3 id="5-2-Marking-Holes"><a href="#5-2-Marking-Holes" class="headerlink" title="5.2. Marking Holes"></a>5.2. Marking Holes</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/10/paper_reading/ExtraNet/image-20210810220301362.png" alt="image-20210810220301362.png"></p>
<p>When reusing previous frames to generate a new frame, some pixels in the warped frame will be invalid, due to camera sliding and object moving. These pixels should be marked as holes before feeding into the network  </p>
<ol>
<li><p>For occlusion caused by object moving, used custom stencil to indicate the sharp changes along the edge of the dynamic object. When custom stencil value is different between the current frame ($ùë†$) and the warped frame ($ùë†<em>ùë§$), these pixels will be marked as invalid:<br>$$<br>\Phi</em>{\mathrm{stencil}}&#x3D;{s_i-s_{w,i}\neq 0}<br>$$<br>where $ùëñ$ is the pixel index and $\Phi_{\mathrm{stencil}}$ is the set including pixels that are counted as invalid according to stencil value  </p>
</li>
<li><p>Self occlusions for some dynamic objects when they are moving or the camera is sliding. Resort to world normal since world normal will probably change in these regions. Calculate the cosine value between current frame‚Äôs world normal ($\pmb n_ùë§$) and warped frame‚Äôs world normal ($\pmb n_{ùë§ùë§}$), i.e., $\pmb n_ùë§\cdot \pmb n_{ùë§ùë§}$. If this value is large than a predetermined threshold $ùëá<em>ùëõ$, the corresponding pixel indexed by $ùëñ$ in the warped frame is counted as invalid:<br>$$<br>\Phi</em>{wn}&#x3D;{\pmb n_{w,i}\cdot \pmb n_{ww,i}&gt;T_n}<br>$$<br>where $\pmb \Phi _{wn}$ contains invalid pixels marked by differences in world normal  </p>
</li>
<li><p>Invalid pixels due to camera movement. These pixels can be selected out by world position, considering that static objects‚Äô world positions keep unchanged in a 3D scene. For a given pixel, let $\pmb p_ùë§$ and $\pmb p_{ùë§ùë§}$ be the world position of current frame and warped frame, respectively. We calculate their distance by $|\pmb p_ùë§ - \pmb p_{ùë§ùë§}|$. If this distance is larger than a threshold $ùëá<em>ùëë$ (which is computed by NoV and depth values), we mark this pixel as invalid. Hence, the invalid pixels in this set $\Phi</em>{wp}$ are<br>$$<br>\Phi_{wp}&#x3D;{|\pmb p_{w,i}-\pmb p_{ww,i}|&gt;T_d}<br>$$</p>
</li>
</ol>
<p>Finally,<br>$$<br>\Phi_{\mathcal{comb}}&#x3D;\Phi_{\mathrm{stencil}}\cup\Phi_{wn}\cup \Phi_{wp}<br>$$</p>
<h2 id="6-Result-and-Comparisons"><a href="#6-Result-and-Comparisons" class="headerlink" title="6. Result and Comparisons"></a>6. Result and Comparisons</h2><p><strong>Training setups and time</strong></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/10/paper_reading/ExtraNet/image-20210812091839252.png" alt="image-20210812091839252.png"></p>
<h3 id="6-1-Comparisons-against-Frame-Interpolation-Methods"><a href="#6-1-Comparisons-against-Frame-Interpolation-Methods" class="headerlink" title="6.1. Comparisons against Frame Interpolation Methods"></a>6.1. Comparisons against Frame Interpolation Methods</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/10/paper_reading/ExtraNet/image-20210812091915623.png" alt="image-20210812091915623"></p>
<h2 id="6-2-Comparisons-against-Frame-Extrapolation-Methods"><a href="#6-2-Comparisons-against-Frame-Extrapolation-Methods" class="headerlink" title="6.2. Comparisons against Frame Extrapolation Methods"></a>6.2. Comparisons against Frame Extrapolation Methods</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/10/paper_reading/ExtraNet/image-20210812092044870.png" alt="image-20210812092044870"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/10/paper_reading/ExtraNet/image-20210812092056289.png" alt="image-20210812092056289.png"></p>
<h3 id="6-3-Comparisons-against-ASW-Technology"><a href="#6-3-Comparisons-against-ASW-Technology" class="headerlink" title="6.3. Comparisons against ASW Technology"></a>6.3. Comparisons against ASW Technology</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/10/paper_reading/ExtraNet/image-20210812092149063.png" alt="image-20210812092149063"></p>
<h3 id="6-4-Analysis-of-Runtime-Performance-and-Latency"><a href="#6-4-Analysis-of-Runtime-Performance-and-Latency" class="headerlink" title="6.4. Analysis of Runtime Performance and Latency"></a>6.4. Analysis of Runtime Performance and Latency</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/10/paper_reading/ExtraNet/image-20210812092258548.png" alt="image-20210812092258548.png"></p>
<h3 id="6-5-Ablation-Study"><a href="#6-5-Ablation-Study" class="headerlink" title="6.5. Ablation Study"></a>6.5. Ablation Study</h3><p><strong>Validation of History Encoder</strong></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/10/paper_reading/ExtraNet/image-20210812092412313.png" alt="image-20210812092412313.png"></p>
<p><strong>Validation of occlusion motion vectors</strong></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/10/paper_reading/ExtraNet/image-20210812092433191.png" alt="image-20210812092433191.png"></p>
<p><strong>Validation of the shading-augmented loss</strong></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/10/paper_reading/ExtraNet/image-20210812092455410.png" alt="image-20210812092455410.png"></p>
<p><strong>Failure cases</strong></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2021/08/10/paper_reading/ExtraNet/image-20210812092521428.png" alt="image-20210812092521428.png"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://chaphlagical.github.io">Chaf Chen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://chaphlagical.github.io/2021/08/10/paper_reading/ExtraNet/">https://chaphlagical.github.io/2021/08/10/paper_reading/ExtraNet/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Rendering/">Rendering</a><a class="post-meta__tags" href="/tags/Neural-Network/">Neural Network</a><a class="post-meta__tags" href="/tags/Siggraph-Asia-2021/">Siggraph Asia 2021</a></div><div class="post_share"><div class="social-share" data-image="/img/logo.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-full"><a href="/2021/08/15/paper_reading/GPU_Accelerated_Path_Tracing_of_Massive_Scenes/" title="GPU Accelerated Path Tracing of Massive Scenes"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">GPU Accelerated Path Tracing of Massive Scenes</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2021/08/25/paper_reading/Neural_Light_Transport_for_Relighting_and_View_Synthesis/" title="Neural Light Transport for Relighting and View Synthesis"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-25</div><div class="title">Neural Light Transport for Relighting and View Synthesis</div></div></a></div><div><a href="/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/" title="Adaptive Incident Radiance Field Sampling and Reconstruction Using Deep Reinforcement Learning"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-25</div><div class="title">Adaptive Incident Radiance Field Sampling and Reconstruction Using Deep Reinforcement Learning</div></div></a></div><div><a href="/2021/08/29/paper_reading/Low_Cost_SPAD_Sensing_for_Non_Line_Of_Sight_Tracking_Material/" title="Low-Cost SPAD Sensing for Non-Line-Of-Sight Tracking, Material Classification and Depth Imaging"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-29</div><div class="title">Low-Cost SPAD Sensing for Non-Line-Of-Sight Tracking, Material Classification and Depth Imaging</div></div></a></div><div><a href="/2023/01/06/paper_reading/Temporal_Coherence-Based_Distributed_Ray_Tracing_of_Massive_Scenes/" title="Temporal Coherence-based Distributed Ray Tracing of Massive Scenes"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-06</div><div class="title">Temporal Coherence-based Distributed Ray Tracing of Massive Scenes</div></div></a></div><div><a href="/2021/08/15/paper_reading/GPU_Accelerated_Path_Tracing_of_Massive_Scenes/" title="GPU Accelerated Path Tracing of Massive Scenes"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-15</div><div class="title">GPU Accelerated Path Tracing of Massive Scenes</div></div></a></div><div><a href="/2022/12/17/paper_reading/Vectorization_for_Fast_Analytic_and_Differentiable_Visibility/" title="Vectorization for Fast, Analytic, and Differentiable Visibility"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-17</div><div class="title">Vectorization for Fast, Analytic, and Differentiable Visibility</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/logo.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Chaf Chen</div><div class="author-info__description">USTC CG Student</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">16</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Chaphlagical"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Chaphlagical" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:mail@ustc.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Looking for a Ph.D position!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Introduction"><span class="toc-number">2.</span> <span class="toc-text">1. Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Related-work-and-background"><span class="toc-number">3.</span> <span class="toc-text">2. Related work and background</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Temporal-Reconstruction"><span class="toc-number">3.1.</span> <span class="toc-text">2.1. Temporal Reconstruction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Texture-In-painting"><span class="toc-number">3.2.</span> <span class="toc-text">2.2. Texture In-painting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Image-Warping"><span class="toc-number">3.3.</span> <span class="toc-text">2.3. Image Warping</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-Video-and-Rendering-Interpolation"><span class="toc-number">3.4.</span> <span class="toc-text">2.4. Video and Rendering Interpolation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Motivation-and-Challenge"><span class="toc-number">4.</span> <span class="toc-text">3. Motivation and Challenge</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Latency"><span class="toc-number">4.1.</span> <span class="toc-text">3.1. Latency</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Challenge"><span class="toc-number">4.2.</span> <span class="toc-text">3.2. Challenge</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-ExtraNet-for-frame-extrapolation"><span class="toc-number">5.</span> <span class="toc-text">4. ExtraNet for frame extrapolation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Problem-Formulation"><span class="toc-number">5.1.</span> <span class="toc-text">4.1. Problem Formulation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Motion-Vectors-and-Image-Warping"><span class="toc-number">5.2.</span> <span class="toc-text">4.2. Motion Vectors and Image Warping</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-Network-Architecture"><span class="toc-number">5.3.</span> <span class="toc-text">4.3. Network Architecture</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-History-Encoder"><span class="toc-number">5.4.</span> <span class="toc-text">4.4. History Encoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-Loss-Function"><span class="toc-number">5.5.</span> <span class="toc-text">4.5. Loss Function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-Training-detail"><span class="toc-number">5.6.</span> <span class="toc-text">4.6. Training detail</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Dataset"><span class="toc-number">6.</span> <span class="toc-text">5. Dataset</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-Scenes-and-Buffers"><span class="toc-number">6.1.</span> <span class="toc-text">5.1. Scenes and Buffers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-Marking-Holes"><span class="toc-number">6.2.</span> <span class="toc-text">5.2. Marking Holes</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Result-and-Comparisons"><span class="toc-number">7.</span> <span class="toc-text">6. Result and Comparisons</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-Comparisons-against-Frame-Interpolation-Methods"><span class="toc-number">7.1.</span> <span class="toc-text">6.1. Comparisons against Frame Interpolation Methods</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-Comparisons-against-Frame-Extrapolation-Methods"><span class="toc-number">8.</span> <span class="toc-text">6.2. Comparisons against Frame Extrapolation Methods</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-Comparisons-against-ASW-Technology"><span class="toc-number">8.1.</span> <span class="toc-text">6.3. Comparisons against ASW Technology</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-Analysis-of-Runtime-Performance-and-Latency"><span class="toc-number">8.2.</span> <span class="toc-text">6.4. Analysis of Runtime Performance and Latency</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-5-Ablation-Study"><span class="toc-number">8.3.</span> <span class="toc-text">6.5. Ablation Study</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/01/25/paper_reading/Adaptive_Incident_Radiance_Field_Sampling_and_Reconstruction_Using_Deep_Reinforcement_Learning/" title="Adaptive Incident Radiance Field Sampling and Reconstruction Using Deep Reinforcement Learning">Adaptive Incident Radiance Field Sampling and Reconstruction Using Deep Reinforcement Learning</a><time datetime="2023-01-25T00:04:00.000Z" title="Created 2023-01-25 00:04:00">2023-01-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/01/06/paper_reading/Temporal_Coherence-Based_Distributed_Ray_Tracing_of_Massive_Scenes/" title="Temporal Coherence-based Distributed Ray Tracing of Massive Scenes">Temporal Coherence-based Distributed Ray Tracing of Massive Scenes</a><time datetime="2023-01-06T22:13:11.000Z" title="Created 2023-01-06 22:13:11">2023-01-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/12/29/gaming/spiderman_miles/" title="Marvel's Spider-Man Miles Morales">Marvel's Spider-Man Miles Morales</a><time datetime="2022-12-29T21:13:11.000Z" title="Created 2022-12-29 21:13:11">2022-12-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/12/17/paper_reading/Vectorization_for_Fast_Analytic_and_Differentiable_Visibility/" title="Vectorization for Fast, Analytic, and Differentiable Visibility">Vectorization for Fast, Analytic, and Differentiable Visibility</a><time datetime="2022-12-17T21:13:11.000Z" title="Created 2022-12-17 21:13:11">2022-12-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/12/14/note/edge_sampling/" title="Physics Based Differentiable Rendering: Edge Sampling">Physics Based Differentiable Rendering: Edge Sampling</a><time datetime="2022-12-14T00:00:00.000Z" title="Created 2022-12-14 00:00:00">2022-12-14</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Chaf Chen</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script><script>function loadDisqus () {
  var disqus_config = function () {
    this.page.url = 'https://chaphlagical.github.io/2021/08/10/paper_reading/ExtraNet/'
    this.page.identifier = '/2021/08/10/paper_reading/ExtraNet/'
    this.page.title = 'ExtraNet: Real-time Extrapolated Rendering for Low-latency Temporal Supersampling'
  };

  window.disqusReset = () => {
    DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  if (window.DISQUS) disqusReset()
  else {
    (function() { 
      var d = document, s = d.createElement('script');
      s.src = 'https://chaphlagical-github-io.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  }

  document.getElementById('darkmode').addEventListener('click', () => {
    setTimeout(() => window.disqusReset(), 200)
  })
}

if ('Disqus' === 'Disqus' || !false) {
  if (false) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
  else loadDisqus()
} else {
  function loadOtherComment () {
    loadDisqus()
  }
}
</script></div></div></body></html>