---
title: 数据拟合
tags: 计算机图形学
article_header:
  type: cover
  image:
    src: /assets/images/embed.jpg
---

<!--more-->

## 一、拟合问题

* 输入：一些观察的数据点
* 输出：反映这些数据规律的函数$y=f(x)$

<img src="/assets/images/数据拟合.assets/image-20200228225559096.png" alt="image-20200228225559096" style="zoom:50%;" />

### (一) 插值 Interpolation

<img src="/assets/images/数据拟合.assets/image-20200228225637939.png" alt="image-20200228225637939" style="zoom:50%;" />

### (二) 逼近 Approximation

<img src="/assets/images/数据拟合.assets/image-20200228225704481.png" alt="image-20200228225704481" style="zoom:50%;" />

## 二、拟合问题的解法

### (一) 完备函数空间

* 用若干简单函数(“基函数”)线性组合成复杂函数
* 线性组合张成一个函数空间

<img src="/assets/images/数据拟合.assets/image-20200228225832774.png" alt="image-20200228225832774" style="zoom:67%;" />

### (二) 万能逼近定理：多项式空间是稠密的

**Weierstrass逼近定理**

对$[a,b]$上的任意连续函数$g$，及任意给定的$\varepsilon>0$，则必存在$n$次代数多项式$f(x)=\sum\limits^n_{k=0}w_kx^k$使得
$$
\mathop\min_{x\in[a,b]}|f(x)-g(x)|<\varepsilon
$$

### (三) 最小二乘拟合

* 选择一个变换函数空间

	* 线性函数空间$\mathrm{span}\{B_0(x),B_1(x),\cdots,B_n(x) \}$
		* 比如：多项式函数，三角函数，RBF函数
	* 函数表达为系数参数$W=\{w_0,w_1,\cdots,w_n\}$

	$$
	y=f(x)=\sum\limits^n_{i=0}w_iB_i(x)
	$$

* 最小二乘拟合：极小化数据误差
	$$
	\min\limits_W \|Y-XW\|^2
	$$
	

### (四) 其他常用基函数

#### 1、Radial Basis Function

<img src="/assets/images/数据拟合.assets/image-20200228232834006.png" alt="image-20200228232834006" style="zoom:50%;" />

<img src="/assets/images/数据拟合.assets/image-20200228232842559.png" alt="image-20200228232842559" style="zoom:50%;" />

#### 2、Lagrange插值函数

插值$n+1$个点、次数不超过$n$的多项式是存在而且是唯一的

* ($n+1$个变量，$n+1$个方程)

$$
p_k(x)=\prod_{i\in B_k}\frac{x-x_i}{x_k-x_i}
$$

插值函数的自由度=未知量个数-已知量个数

## 三、过拟合

* 误差为0，但拟合的函数无使用价值

<img src="/assets/images/数据拟合.assets/image-20200228233636743.png" alt="image-20200228233636743" style="zoom:50%;" />

**避免过拟合的常用方法：**

* 数据去噪
	* 剔除训练样本中噪声
* 数据增广
	* 增加样本数，或者增加样本的代表性和多样性
* 模型简化
	* 预测模型过于复杂，拟合了训练样本中的噪声
	* 选用更简单的模型，或者对模型进行裁剪
* 正则约束
	* 适当的正则项，比如方差正则项、稀疏正则项

### (一) 岭回归正则项

* 选择一个函数空间
	* 基函数的线性表达$W=(w_0,w_1,\cdots,w_n)$

$$
y=f(x)=\sum\limits^n_{i=0}w_iB_i(x)
$$

* 最小二乘拟合

$$
\min_W \|Y-XW\|^2
$$

* Ridge regression岭回归
	$$
	\min_W\|Y-XW\|^2+\mu\|W\|^2_2
	$$

### (二) 稀疏学习：稀疏正则化

* 冗余基函数（过完备）
* 通过优化来选择合适的基函数
	* 系数向量的$L_0$模（ 非0元素个数）尽量小
	* 挑选（“学习”）出合适的基函数

$$
\min_\alpha\|Y-XW\|^2+\mu\|W\|_0\\
\min_\alpha\|Y-XW\|^2,\ \ s.t.\|W\|_0\leq \beta
$$

**压缩感知**

<img src="/assets/images/数据拟合.assets/image-20200228234358445.png" alt="image-20200228234358445" style="zoom:67%;" />

* 已知$y$和$\Phi$，有无穷多解$x$

* 对于稀疏信号$x$，可通过优化能完全重建$x$

	* 在一定条件下

	$$
	\min\|x\|_0\\
	s.t.\ \Phi x=y
	$$

**稀疏学习的不同形式**

<img src="/assets/images/数据拟合.assets/image-20200228235553737.png" alt="image-20200228235553737" style="zoom:50%;" />

## 四、神经网络角度

### (一) 将函数看成网络

#### 1、$R^1\rightarrow R^1$

<img src="/assets/images/数据拟合.assets/image-20200228235723319.png" alt="image-20200228235723319" style="zoom:50%;" />

#### 2、$R^m\rightarrow R^n$

<img src="/assets/images/数据拟合.assets/image-20200229000221578.png" alt="image-20200229000221578" style="zoom: 50%;" />

### (二)万有逼近定理

**定义1：**称函数$\varphi(x)$为压缩函数，如果$\varphi(x)$单调不减，且满足
$$
\lim_{x\rightarrow -\inf} \varphi(x)=0,\lim_{x\rightarrow +\inf} \varphi(x)=1
$$
**定理1：**若$\varphi$是压缩函数，则$S(\varphi)$在$C^m$中一致稠密，在$M^m$中按如下距离$\rho_\mu$下稠密：
$$
\rho_\mu=\inf\{\varepsilon>0:\mu\{x:|f(x)-g(x) |>\varepsilon\}<\varepsilon\}
$$
**定理2：**若$\varphi$是连续有界的非常值函数，则$S(\varphi)$在$C^m$中稠密

### (三) 神经元模型

<img src="/assets/images/数据拟合.assets/image-20200229000819141.png" alt="image-20200229000819141" style="zoom:50%;" />

### (四) 神经网络模型

#### 1、单隐层神经网络

一个复杂的函数由很多小函数组合而成


$$
y(x)=\sum\limits^n_{k=1}w_k^1H_k(x)
$$


<img src="/assets/images/数据拟合.assets/image-20200229001051885.png" alt="image-20200229001051885" style="zoom:50%;" />

#### 2、多隐层神经网络：多重复合函数

<img src="/assets/images/数据拟合.assets/image-20200229001130240.png" alt="image-20200229001130240" style="zoom:50%;" />

#### 3、激活函数

<img src="/assets/images/数据拟合.assets/image-20200229001150877.png" alt="image-20200229001150877" style="zoom:50%;" />